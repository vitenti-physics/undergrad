[
  {
    "objectID": "courses/introduction-to-physics/physics-math.html",
    "href": "courses/introduction-to-physics/physics-math.html",
    "title": "A Física como Linguagem Matemática",
    "section": "",
    "text": "Podemos imaginar o espaço como o palco onde todos os fenômenos físicos se desenrolam. Mas surge uma pergunta fundamental: o que exatamente significa dizer a posição de algo?\nQuando falamos em posição, estamos descrevendo uma forma de relacionar dois pontos: o ponto que queremos localizar e um ponto de referência fixo, chamado origem. Para estabelecer essa relação, usamos coordenadas – números que indicam “quão longe” e “em que direção” estamos em relação à origem.\nNo caso mais simples, consideramos o espaço como uma linha reta e medimos distâncias diretas entre os pontos. Cada posição pode então ser associada a um único número que representa a distância até a origem (número cuja natureza matemática ainda vamos explorar).\n\n\n\nExplique com suas próprias palavras o que significa a posição de um objeto em relação a um ponto de referência. Por que a escolha da origem é importante?\nQual é a diferença entre descrever a posição de um ponto em uma linha reta e em um plano ou espaço tridimensional?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#o-espaço-onde-a-física-acontece",
    "href": "courses/introduction-to-physics/physics-math.html#o-espaço-onde-a-física-acontece",
    "title": "A Física como Linguagem Matemática",
    "section": "",
    "text": "Podemos imaginar o espaço como o palco onde todos os fenômenos físicos se desenrolam. Mas surge uma pergunta fundamental: o que exatamente significa dizer a posição de algo?\nQuando falamos em posição, estamos descrevendo uma forma de relacionar dois pontos: o ponto que queremos localizar e um ponto de referência fixo, chamado origem. Para estabelecer essa relação, usamos coordenadas – números que indicam “quão longe” e “em que direção” estamos em relação à origem.\nNo caso mais simples, consideramos o espaço como uma linha reta e medimos distâncias diretas entre os pontos. Cada posição pode então ser associada a um único número que representa a distância até a origem (número cuja natureza matemática ainda vamos explorar).\n\n\n\nExplique com suas próprias palavras o que significa a posição de um objeto em relação a um ponto de referência. Por que a escolha da origem é importante?\nQual é a diferença entre descrever a posição de um ponto em uma linha reta e em um plano ou espaço tridimensional?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#como-medimos-distâncias",
    "href": "courses/introduction-to-physics/physics-math.html#como-medimos-distâncias",
    "title": "A Física como Linguagem Matemática",
    "section": "Como medimos distâncias",
    "text": "Como medimos distâncias\nPara atribuir um número a uma distância, precisamos de um padrão: podemos imaginar um segmento-padrão, nossa unidade de comprimento, e contar quantos desses segmentos “cabem” entre dois pontos. É esse processo que define o número usado para representar a distância.\nSe quisermos medir com mais precisão, basta escolher um segmento-padrão menor, permitindo contar frações menores do caminho entre os pontos.\n\nPergunta fundamental\nSerá que existe uma unidade mínima, indivisível, que sirva para medir tudo? Se essa unidade existisse, nosso modelo do espaço seria discreto, formado por “blocos” indivisíveis, em vez de um continuum.\n\n\nExercícios\n\nExplique por que é necessário definir uma unidade de comprimento antes de medir distâncias.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#pitágoras-contra-o-discreto",
    "href": "courses/introduction-to-physics/physics-math.html#pitágoras-contra-o-discreto",
    "title": "A Física como Linguagem Matemática",
    "section": "Pitágoras contra o discreto",
    "text": "Pitágoras contra o discreto\nPensemos no Teorema de Pitágoras: \\[\nc^2 = a^2 + b^2\n\\]\nSe tomarmos \\(a = b = 1\\) (em unidades do segmento-padrão), então: \\[\nc = \\sqrt{2}\n\\]\nO problema é que:\n\\[\n\\sqrt{2} \\notin \\mathbb{Q}\n\\]\nou seja, não pode ser escrito como uma fração \\(\\frac{p}{q}\\) com \\(p, q \\in \\mathbb{Z}\\) e \\(q \\neq 0\\).\nIsso implica que, se existisse uma unidade mínima fixa, não conseguiríamos manter o Teorema de Pitágoras exatamente válido.\nConclusão provisória: ou o espaço não é discreto, ou o teorema é apenas uma aproximação de uma estrutura mais fundamental.\n\nExercícios\n\nExplique por que \\(\\sqrt{2}\\) não pode ser expresso como uma fração \\(\\frac{p}{q}\\) e como isso se relaciona com a ideia de um espaço discreto.\nDiscuta o que a impossibilidade de representar exatamente certas distâncias com um segmento-padrão sugere sobre a natureza do espaço.\nUsando (\\(a = b = 1\\)), calcule (c) aproximando-o por frações simples, como \\(c \\approx \\frac{7}{5}\\) ou \\(c \\approx \\frac{10}{7}\\). Qual é o erro relativo em cada aproximação?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#hipótese-da-física-clássica",
    "href": "courses/introduction-to-physics/physics-math.html#hipótese-da-física-clássica",
    "title": "A Física como Linguagem Matemática",
    "section": "Hipótese da física clássica",
    "text": "Hipótese da física clássica\nNa física clássica, adotamos a hipótese de que o espaço é contínuo. Isso significa que, em princípio, podemos medir distâncias arbitrariamente pequenas, sem um limite mínimo definido.\nEmbora não saibamos exatamente por que isso ocorre, modelamos as posições assumindo implicitamente que qualquer distância, por menor que seja, pode ser medida. Vale ressaltar que não há uma prova empírica definitiva de que o espaço é contínuo. Essa continuidade é uma das hipóteses fundamentais que adotamos, e dela derivam diversas consequências importantes para outras teorias, como a teoria de campos, a mecânica quântica e a relatividade.\nDessa forma, cada posição pode ser representada por um número real, que pode ser racional ou irracional.\n\nExercícios\n\nExplique o que significa assumir que o espaço é contínuo na física clássica.\nPor que números reais são usados para representar posições no espaço contínuo?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#conjuntos-numéricos",
    "href": "courses/introduction-to-physics/physics-math.html#conjuntos-numéricos",
    "title": "A Física como Linguagem Matemática",
    "section": "Conjuntos numéricos",
    "text": "Conjuntos numéricos\nPara organizar as ideias, temos os seguintes conjuntos numéricos:\n\n\\(\\mathbb{N}\\): números naturais: \\(0, 1, 2, 3, \\dots\\)\n\n\\(\\mathbb{Z}\\): números inteiros: \\(\\dots, -2, -1, 0, 1, 2, \\dots\\)\n\n\\(\\mathbb{Q}\\): números racionais: frações da forma \\(\\frac{p}{q}\\), com \\(p, q \\in \\mathbb{Z}\\) e \\(q \\neq 0\\)\n\nNúmeros irracionais: não podem ser escritos como fração (exemplos: \\(\\sqrt{2}\\), \\(\\pi\\), \\(e\\), …)\n\n\\(\\mathbb{R}\\): números reais: a união dos racionais e irracionais, ou seja, \\(\\mathbb{Q} \\cup\\) (irracionais)\n\n\nExercícios\n\nDê exemplos de números que pertencem a cada um dos conjuntos (), (), () e irracionais.\nExplique a relação entre os conjuntos \\(\\mathbb{Q}\\) e \\(\\mathbb{R}\\).\nClassifique os seguintes números nos conjuntos corretos: \\(0, -3, \\frac{5}{2}, \\sqrt{2}, \\pi\\).",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#o-paradoxo-de-zenão-e-as-séries-infinitas",
    "href": "courses/introduction-to-physics/physics-math.html#o-paradoxo-de-zenão-e-as-séries-infinitas",
    "title": "A Física como Linguagem Matemática",
    "section": "O paradoxo de Zenão e as séries infinitas",
    "text": "O paradoxo de Zenão e as séries infinitas\nImagine que queremos andar de um ponto A até um ponto B separados por \\(1\\) metro.\n\n\\(1^\\circ\\) passo: metade do caminho, ou seja, \\(\\frac{1}{2}\\) m.\n\\(2^\\circ\\) passo: metade do que resta, \\(\\frac{1}{4}\\) m.\n\\(3^\\circ\\) passo: metade novamente, \\(\\frac{1}{8}\\) m… e assim por diante.\n\nPodemos expressar essa sequência usando o símbolo de somatório: \\[\nd = \\sum_{n=1}^\\infty \\frac{1}{2^n}.\n\\]\n\nResolvendo a soma\nComeçamos com: \\[\nd = \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8} + \\dots\n\\]\nMultiplicando ambos os lados por \\(\\frac{1}{2}\\): \\[\n\\frac{d}{2} = \\frac{1}{4} + \\frac{1}{8} + \\dots\n\\]\nSubtraindo as duas expressões: \\[\nd - \\frac{d}{2} = \\frac{1}{2},\n\\] \\[\n\\frac{d}{2} = \\frac{1}{2},\n\\] \\[\nd = 1.\n\\]\nOu seja, somando infinitos pedaços cada vez menores, obtemos um comprimento finito.\nMais do que isso: além do Teorema de Pitágoras, esse resultado mostra que as distâncias podem ser compostas por somas infinitas de números racionais. Isso reforça que nosso modelo de espaço contínuo precisa ser representado por números reais, que incluem tanto os racionais quanto os irracionais.\n\n\nConclusão\nO modelo contínuo do espaço nos leva naturalmente ao uso dos números reais \\(\\mathbb{R}\\), que incluem tanto racionais quanto irracionais, e nos permite trabalhar com distâncias expressas como somas infinitas.\nNa física, essa suposição de continuidade está tão enraizada que aparece mesmo nos problemas mais simples, mas também levanta questões profundas: será que, em escalas muito pequenas, o espaço é realmente contínuo?\n\n\nExercícios\n\nExplique com suas próprias palavras o paradoxo de Zenão e como a soma infinita resolve a aparente contradição.\nCalcule a soma da série \\(\\sum_{n=1}^{\\infty} \\frac{1}{3^n}\\) e interprete o resultado no contexto de uma distância percorrida em passos que diminuem a cada movimento.\nPor que a existência de somas infinitas de números racionais reforça a necessidade de usar números reais para modelar o espaço contínuo?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#dimensões-do-espaço",
    "href": "courses/introduction-to-physics/physics-math.html#dimensões-do-espaço",
    "title": "A Física como Linguagem Matemática",
    "section": "Dimensões do espaço",
    "text": "Dimensões do espaço\nQuando pensamos no “tamanho” do espaço, a primeira coisa que vem à mente é quantos pontos ele contém. Por exemplo, o conjunto dos números inteiros \\(\\mathbb{Z}\\) é infinito, mas é um infinito contável: podemos listar seus elementos um a um, ou seja, existe uma forma de associar cada inteiro a um número natural de forma unívoca.\nJá o conjunto dos números reais \\(\\mathbb{R}\\) é infinito incontável: há muito mais números reais do que inteiros, a ponto de não ser possível associar cada real a um inteiro de forma unívoca.\nAssim, nosso espaço, modelado pelos números reais, inclui uma infinidade incontável de pontos.\nMas o número de pontos não é a única forma de medir o “tamanho” do espaço. Existe um conceito diferente e fundamental: a dimensão.\nNossa experiência mostra que, para localizar um ponto no espaço, usamos três números, as coordenadas \\(x\\), \\(y\\) e \\(z\\), que indicam a posição em três direções mutuamente perpendiculares. Escolhemos uma origem e três direções que formam ângulos retos (\\(90^\\circ\\)) entre si, e medimos distâncias ao longo dessas direções usando números reais, como discutimos antes. Ou seja, primeiro andamos em uma direção \\(x\\), depois em outra \\(y\\) e, por fim, em outra \\(z\\), até chegar ao ponto desejado. O interessante é que sempre conseguimos alcançar qualquer ponto usando esse procedimento, e a ordem dos passos não altera o resultado.\nEsses três números são suficientes para identificar qualquer ponto do espaço físico que conhecemos.\n\nExercícios\n\nExplique a diferença entre infinito contável e infinito incontável. Dê exemplos de cada caso.\nPor que usamos três coordenadas para localizar um ponto no espaço físico?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#produto-cartesiano",
    "href": "courses/introduction-to-physics/physics-math.html#produto-cartesiano",
    "title": "A Física como Linguagem Matemática",
    "section": "Produto cartesiano",
    "text": "Produto cartesiano\nPara descrever matematicamente esse conjunto de pontos, usamos o conceito de produto cartesiano.\nComeçamos com: \\[\n\\mathbb{R} \\times \\mathbb{R} \\equiv \\{(x, y) \\mid x, y \\in \\mathbb{R}\\},\n\\] ou seja, o conjunto de pares ordenados: elementos formados por dois números reais em uma ordem definida (importante porque \\((x,y)\\) geralmente não é o mesmo que \\((y,x)\\)).\nEsse novo conjunto é chamado de \\(\\mathbb{R}^2\\).\nAgora, aplicando novamente o produto cartesiano, construímos o conjunto tridimensional de pontos do espaço: \\[\n\\mathbb{R}^2 \\times \\mathbb{R} \\equiv \\{((x, y), z) \\mid (x, y) \\in \\mathbb{R}^2, z \\in \\mathbb{R}\\}.\n\\] Porém, o elemento \\(((x, y), z)\\) é matematicamente equivalente ao triplo ordenado \\((x, y, z)\\), e por isso escrevemos: \\[\n\\mathbb{R}^3 \\equiv \\{(x, y, z) \\mid x, y, z \\in \\mathbb{R}\\}.\n\\]\nEssa construção não é apenas formal, ela reflete a nossa própria forma de localizar pontos no espaço.\n\nExercícios\n\nExplique o que é o produto cartesiano e como ele nos ajuda a construir (^2) e (^3).\nListe cinco elementos quaisquer de (^2) e cinco elementos de (^3).\nPor que a ordem dos elementos em um par ou triplo ordenado é importante?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#operações-no-produto-cartesiano",
    "href": "courses/introduction-to-physics/physics-math.html#operações-no-produto-cartesiano",
    "title": "A Física como Linguagem Matemática",
    "section": "Operações no produto cartesiano",
    "text": "Operações no produto cartesiano\nAlém disso, podemos definir operações nesse conjunto usando as operações já conhecidas dos números reais. Por exemplo, ao dizer que andamos primeiro na direção \\(x\\) e depois na direção \\(y\\), estamos compondo os pontos \\((x, 0)\\) e \\((0, y)\\) para obter o ponto \\((x, y)\\).\nEssa composição é chamada de adição e é definida como: \\[\n(x, y) + (x', y') = (x + x', y + y'),\n\\] ou seja, somamos as componentes separadamente, estendendo o conceito de soma dos números reais para os pares ordenados e, por analogia, para trios ordenados também.\nNote que, como a ordem da soma das componentes não altera o resultado, o mesmo acontece com os pares ou trios ordenados, isso reflete a nossa experiência física e a propriedade comutativa da adição.\nOutra operação importante é a multiplicação por um número real (escalar). Se multiplicarmos as coordenadas do ponto por um número real \\(k\\), obtemos: \\[\nk (x, y) = (kx, ky),\n\\] ou seja, multiplicamos cada componente separadamente, estendendo o conceito de produto dos números reais para os pares ordenados. Analogamente, para o espaço tridimensional: \\[\nk (x, y, z) = (kx, ky, kz).\n\\] Essa operação permite ampliar ou reduzir as distâncias, mantendo a direção das coordenadas proporcional.\n\nExercícios\n\nExplique como a adição de pares ou trios ordenados se relaciona com a soma de números reais.\nCalcule \\((2, -1) + (3, 4)\\) e \\(2 \\cdot (1, -3, 2)\\).\nPor que a multiplicação por um escalar mantém a direção do ponto, mas altera sua distância à origem?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#espaço-vetorial",
    "href": "courses/introduction-to-physics/physics-math.html#espaço-vetorial",
    "title": "A Física como Linguagem Matemática",
    "section": "Espaço Vetorial",
    "text": "Espaço Vetorial\nAs operações definidas acima, a adição de pares ordenados e a multiplicação por escalares, são exemplos de como podemos construir uma estrutura matemática mais geral, chamada espaço vetorial.\nUm espaço vetorial é um conjunto de objetos (que chamaremos de vetores) sobre os quais estão definidas duas operações:\n\nAdição de vetores: a partir de dois vetores, obtemos um terceiro vetor do mesmo conjunto.\nNo caso do plano cartesiano \\(\\mathbb{R}^2\\), essa operação é: \\[\n(x, y) + (x', y') = (x + x', y + y').\n\\]\nMultiplicação por escalar: a partir de um vetor e de um número real, obtemos um novo vetor do mesmo conjunto.\nPor exemplo: \\[\nk(x, y) = (kx, ky), \\quad k \\in \\mathbb{R}.\n\\]\n\nEssas duas operações devem satisfazer certas propriedades chamadas axiomas de espaço vetorial, como:\n\nAssociatividade da adição: \\((u + v) + w = u + (v + w)\\).\n\nComutatividade da adição: \\(u + v = v + u\\).\n\nElemento neutro: existe um vetor \\(0\\) tal que \\(u + 0 = u\\).\n\nElemento oposto: para todo vetor \\(u\\), existe \\(-u\\) tal que \\(u + (-u) = 0\\).\n\nDistributividade: \\(k(u + v) = ku + kv\\) e \\((k + m)u = ku + mu\\).\n\nCompatibilidade: \\(k(mu) = (km)u\\).\n\nUnidade: \\(1 \\cdot u = u\\).\n\nNo caso do plano \\(\\mathbb{R}^2\\) e do espaço tridimensional \\(\\mathbb{R}^3\\), todas essas propriedades são satisfeitas. Portanto, \\((\\mathbb{R}^2, +, \\cdot)\\) e \\((\\mathbb{R}^3, +, \\cdot)\\) são exemplos de espaços vetoriais sobre o conjunto dos números reais.\nEssa estrutura será a base para o estudo da Geometria Analítica e da Álgebra Linear, permitindo tratar vetores não apenas como “setas no espaço”, mas como objetos matemáticos com propriedades bem definidas.\n\nIndependência Linear\nNo espaço vetorial, uma questão importante é entender quando um vetor pode ser obtido a partir de outros.\nPara isso, introduzimos a ideia de combinação linear.\nDizemos que um vetor \\(v\\) é uma combinação linear de \\(v_1\\) e \\(v_2\\) se existem números reais \\(a\\) e \\(b\\) tais que \\[\nv = a v_1 + b v_2.\n\\]\nUm caso especial é quando a combinação linear resulta no vetor nulo: \\[\na v_1 + b v_2 = 0.\n\\] O problema é determinar para quais valores de \\(a\\) e \\(b\\) essa igualdade é possível.\n\nSe a única solução é \\(a = b = 0\\), então \\(v_1\\) e \\(v_2\\) são chamados linearmente independentes (L.I.).\n\nSe existir uma solução diferente de \\(a = b = 0\\), então \\(v_1\\) e \\(v_2\\) são linearmente dependentes (L.D.).\n\n\n\nQuantos vetores linearmente independentes cabem em \\(\\mathbb{R}^2\\)?\nNo plano \\(\\mathbb{R}^2\\), no máximo dois vetores podem ser linearmente independentes.\nIsso significa que qualquer conjunto com três ou mais vetores em \\(\\mathbb{R}^2\\) será necessariamente linearmente dependente.\n\n\nExemplo: os vetores \\((1,1)\\), \\((1,-1)\\) e \\((x,y)\\)\nVamos analisar o conjunto \\[\n\\{ (1,1), (1,-1), (x,y) \\}.\n\\]\nSe esses três vetores fossem linearmente independentes, a única solução da equação \\[\na(1,1) + b(1,-1) + c(x,y) = (0,0)\n\\] seria \\(a = b = c = 0\\).\nExpandindo: \\[\na(1,1) + b(1,-1) + c(x,y) = (a+b+cx,\\, a-b+cy).\n\\]\nPara que o resultado seja \\((0,0)\\), temos o sistema: \\[\n\\begin{cases}\na + b + cx = 0 \\\\\na - b + cy = 0\n\\end{cases}\n\\]\nEsse sistema tem sempre soluções não triviais (com \\(a, b, c\\) não todos nulos).\nPortanto, os três vetores são linearmente dependentes para qualquer valor de \\(x\\) e \\(y\\).\nAssim, concluímos que em \\(\\mathbb{R}^2\\) só é possível ter até dois vetores linearmente independentes, o que está em acordo com a dimensão do espaço.\n\n\nDimensão\nAté aqui vimos que em \\(\\mathbb{R}^2\\) só podemos ter no máximo dois vetores linearmente independentes.\nEsse número máximo de vetores linearmente independentes em um espaço vetorial é chamado de sua dimensão.\nDe forma geral:\n\nA dimensão de um espaço vetorial é o número de vetores em uma base, isto é, o número máximo de vetores linearmente independentes que podem existir nesse espaço.\n\nPor exemplo:\n\nEm \\(\\mathbb{R}^1\\), apenas um vetor pode ser linearmente independente, logo \\(\\dim(\\mathbb{R}^1) = 1\\).\n\nEm \\(\\mathbb{R}^2\\), até dois vetores podem ser linearmente independentes, logo \\(\\dim(\\mathbb{R}^2) = 2\\).\n\nEm \\(\\mathbb{R}^3\\), até três vetores podem ser linearmente independentes, logo \\(\\dim(\\mathbb{R}^3) = 3\\).\n\nAssim, o conceito de dimensão nos permite medir, de forma matemática, quantas direções independentes existem em um espaço vetorial.\nEsse resultado conecta-se ao nosso objetivo inicial: compreender e medir o espaço onde a física acontece.\nVimos que, embora cada conjunto numérico (como os reais) contenha um número incontável de pontos, podemos distinguir diferentes espaços, reta, plano e espaço tridimensional, pelo número de direções independentes que eles possuem.\nA dimensão é, portanto, a medida fundamental que caracteriza a estrutura do espaço.\n\n\nExercícios\n\nDefina em suas palavras o que é um espaço vetorial e quais operações devem estar definidas nele.\nVerifique se os vetores \\((1,0)\\) e \\((0,1)\\) em \\(\\mathbb{R}^2\\) são linearmente independentes.\nExplique por que três vetores quaisquer em \\(\\mathbb{R}^2\\) são necessariamente linearmente dependentes.\nDetermine a dimensão de \\(\\mathbb{R}^3\\) e dê um exemplo de uma base para esse espaço vetorial.\nRelacione o conceito de dimensão de um espaço vetorial com a ideia de direções independentes no espaço físico.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/physics-math.html#funções-contínuas-definição-de-limite",
    "href": "courses/introduction-to-physics/physics-math.html#funções-contínuas-definição-de-limite",
    "title": "A Física como Linguagem Matemática",
    "section": "Funções Contínuas: Definição de Limite",
    "text": "Funções Contínuas: Definição de Limite\nPara compreender o que significa uma função ser contínua, precisamos primeiro introduzir o conceito de limite.\nA definição rigorosa de limite usa a linguagem de \\(\\varepsilon\\) (épsilon) e \\(\\delta\\) (delta), baseada no conceito de módulo:\n\\[\n|x| =\n\\begin{cases}\nx, & \\text{se } x \\geq 0, \\\\\n-x, & \\text{se } x &lt; 0.\n\\end{cases}\n\\]\nO módulo mede a distância de \\(x\\) até a origem.\nAssim, \\(|x - p|\\) representa a distância entre \\(x\\) e \\(p\\). Esse é um exemplo de métrica: uma forma de medir distâncias.\n\nDefinição (Limite por \\(\\varepsilon\\) e \\(\\delta\\))\nDizemos que o limite de \\(f(x)\\) quando \\(x\\) tende a \\(p\\) é \\(L\\), e escrevemos \\[\n\\lim_{x \\to p} f(x) = L,\n\\] se, para todo \\(\\varepsilon &gt; 0\\), existe um \\(\\delta &gt; 0\\) tal que \\[\n0 &lt; |x - p| &lt; \\delta \\quad \\Rightarrow \\quad |f(x) - L| &lt; \\varepsilon.\n\\]\n\n\nComo “ler” a definição\nEssa definição pode parecer compacta demais em sua forma simbólica. Para entendê-la melhor, podemos transformá-la em uma receita passo a passo:\n\nPara todo \\(\\varepsilon &gt; 0\\):\nEscolha um valor positivo e não nulo para \\(\\varepsilon\\). Ele representa a margem de erro que aceitamos para \\(f(x)\\) em relação a \\(L\\).\nExiste um \\(\\delta &gt; 0\\): Dado esse \\(\\varepsilon\\), conseguimos encontrar um \\(\\delta\\) positivo. Esse \\(\\delta\\) será a margem de proximidade de \\(x\\) em relação a \\(p\\).\nSe \\(x \\in \\mathbb{R}\\) e \\(0 &lt; |x - p| &lt; \\delta\\):\nSignifica que escolhemos valores de \\(x\\) suficientemente próximos de \\(p\\), mas diferentes de \\(p\\).\nEntão \\(|f(x) - L| &lt; \\varepsilon\\):\nNessas condições, o valor de \\(f(x)\\) ficará dentro da margem de erro \\(\\varepsilon\\) em torno de \\(L\\).\n\nEssa estrutura mostra que o limite é, essencialmente, um jogo de aproximações:\n\no \\(\\varepsilon\\) fixa o “quanto” queremos que \\(f(x)\\) fique perto de \\(L\\),\n\no \\(\\delta\\) nos garante o “quão perto” \\(x\\) deve estar de \\(p\\) para que isso aconteça.\n\nEsse raciocínio será a base para a definição formal de continuidade.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Função exemplo\ndef f(x):\n    return np.sin(x)\n\n\n# Ponto onde queremos o limite\np = np.pi / 4\nL = np.sin(p)\n\n# Valores de epsilon e delta\neps = 0.2\ndelta = 0.4\n\n# Intervalo para plot\nx_vals = np.linspace(p - 1.5, p + 1.5, 400)\ny_vals = f(x_vals)\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Gráfico da função\nax.plot(x_vals, y_vals, label=r\"$f(x) = \\sin(x)$\")\n\n# Linha horizontal em L\nax.axhline(L, color=\"black\", linestyle=\"--\")\nax.text(p + 1.1, L + 0.02, r\"$L$\", fontsize=12)\n\n# Faixa |f(x) - L| &lt; eps\nax.fill_between(\n    x_vals, L - eps, L + eps, color=\"gray\", alpha=0.3, label=r\"$|f(x)-L| &lt; \\varepsilon$\"\n)\n# Linha vertical em p\nax.axvline(p, color=\"black\", linestyle=\"--\")\nax.text(p, -1.1, r\"$p$\", fontsize=12, ha=\"center\")\n\n# Faixa |x - p| &lt; delta\nax.axvspan(p - delta, p + delta, color=\"orange\", alpha=0.2, label=r\"$|x-p| &lt; \\delta$\")\n# Destacar o ponto (p, L)\nax.plot(p, L, \"ro\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(\"$f(x)$\")\nax.legend(loc=\"best\")\nax.set_title(r\"Definição de limite por $\\epsilon$ e $\\delta$\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Representação gráfica da definição de limite \\(\\lim_{x \\to p} f(x) = L\\).\n\n\n\n\n\n\n\nLimite visto como conjuntos\nNa definição de limite usamos a desigualdade \\(|f(x)-L|&lt;\\varepsilon\\).\nMas esse conjunto de valores de \\(f(x)\\) nada mais é que o intervalo aberto \\[\n(L-\\varepsilon,\\, L+\\varepsilon),\n\\] isto é, todos os pontos de \\(y\\) que ficam dentro de uma “faixa” em torno de \\(L\\).\nEssa faixa é aberta porque não inclui as bordas \\(L-\\varepsilon\\) e \\(L+\\varepsilon\\).\n\nO que acontece no domínio?\nSe queremos que \\(f(x)\\) caia dentro dessa faixa, precisamos escolher valores de \\(x\\) que levem a \\(f(x)\\) até lá.\nOu seja, olhamos para o conjunto \\[\nU_\\varepsilon = \\{x \\in \\mathbb{R}: |f(x)-L|&lt;\\varepsilon\\}.\n\\]\nEsse é o conjunto dos \\(x\\) que produzem valores de \\(f(x)\\) dentro da faixa em \\(y\\).\n\n\nOnde entram \\(x_1\\) e \\(x_2\\)?\nAs equações \\[\nf(x_1) = L - \\varepsilon, \\quad f(x_2) = L + \\varepsilon\n\\] são as condições que “marcam as bordas” do intervalo em \\(y\\).\nOs pontos \\(x_1\\) e \\(x_2\\) no domínio são justamente aqueles que fazem a função chegar exatamente nas bordas \\(L-\\varepsilon\\) e \\(L+\\varepsilon\\).\nAssim, o intervalo \\((x_1, x_2)\\) no eixo-\\(x\\) é a região que leva a imagem da função para dentro do intervalo \\((L-\\varepsilon, L+\\varepsilon)\\) no eixo-\\(y\\).\n\n\nComo isso ajuda a escolher \\(\\delta\\)?\nAo fixar \\(\\varepsilon\\), olhamos para a faixa aberta em \\(y\\): \\[\n(L-\\varepsilon,\\, L+\\varepsilon).\n\\] No domínio, aparecem dois pontos importantes:\n\\[\nf(x_1)=L-\\varepsilon, \\quad f(x_2)=L+\\varepsilon.\n\\]\nEsses pontos \\(x_1\\) e \\(x_2\\) marcam as “bordas” de onde \\(f(x)\\) ainda cai dentro da faixa em \\(y\\).\nSe o ponto \\(p\\) está entre \\(x_1\\) e \\(x_2\\), então qualquer \\(x\\) dentro do intervalo \\((x_1,\\,x_2)\\) satisfaz \\(|f(x)-L|&lt;\\varepsilon\\).\nPara garantir isso em termos de \\(\\delta\\), basta escolher uma distância que mantenha todos os \\(x\\) próximos de \\(p\\) dentro desse intervalo.\nUma forma prática é tomar \\[\n\\delta = \\min\\{|x_1-p|,\\, |x_2-p|\\}.\n\\]\nCom essa escolha, o intervalo \\((p-\\delta,\\, p+\\delta)\\) fica inteiramente contido em \\((x_1,\\,x_2)\\), o que garante: \\[\n0&lt;|x-p|&lt;\\delta \\quad \\Rightarrow \\quad |f(x)-L|&lt;\\varepsilon.\n\\]\n\n\nIntuição\n\nO \\(\\varepsilon\\) abre um “tubo horizontal” em torno de \\(L\\).\n\nOs pontos \\(x_1\\) e \\(x_2\\) marcam onde a curva da função toca as bordas desse tubo.\n\nEscolher \\(\\delta = \\min(|x_1-p|,|x_2-p|)\\) garante que, ao nos aproximarmos de \\(p\\), permanecemos dentro desse tubo.\n\nAssim, conseguimos traduzir a tolerância em \\(y\\) (dada por \\(\\varepsilon\\)) em uma tolerância correspondente no \\(x\\) (dada por \\(\\delta\\)).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Função exemplo\ndef f(x):\n    return np.sin(x)\n\n\n# Ponto do limite\np = np.pi / 4\nL = np.sin(p)\n\n# Valores\neps = 0.2\n\n# Encontrar x1 e x2 numericamente tais que f(x) = L ± eps\n# Usando busca simples em uma malha fina em torno de p\nx_vals = np.linspace(p - 2, p + 2, 2000)\ny_vals = f(x_vals)\n\n# Condições\nmask_lower = np.isclose(y_vals, L - eps, atol=0.01)\nmask_upper = np.isclose(y_vals, L + eps, atol=0.01)\n\n# Aproximar x1 e x2 como os primeiros pontos que satisfazem a condição\nx1_candidates = x_vals[mask_lower]\nx2_candidates = x_vals[mask_upper]\n\nx1 = x1_candidates[0] if len(x1_candidates) &gt; 0 else p - 0.5\nx2 = x2_candidates[-1] if len(x2_candidates) &gt; 0 else p + 0.5\n\n# Escolha de delta\ndelta = min(abs(x1 - p), abs(x2 - p))\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot função\nax.plot(x_vals, y_vals, label=r\"$f(x) = \\sin(x)$\")\n\n# Linhas horizontais L±eps\nax.axhline(L + eps, color=\"gray\", linestyle=\"--\")\nax.axhline(L - eps, color=\"gray\", linestyle=\"--\")\nax.fill_between(\n    x_vals, L - eps, L + eps, color=\"gray\", alpha=0.2, label=r\"$|f(x)-L|&lt;\\varepsilon$\"\n)\n\n# Linha vertical em p\nax.axvline(p, color=\"black\", linestyle=\"--\")\nax.text(p, -1.2, r\"$p$\", ha=\"center\")\n\n# Marcar x1, x2\nax.axvline(x1, color=\"blue\", linestyle=\"--\")\nax.axvline(x2, color=\"blue\", linestyle=\"--\")\nax.text(x1, -1.0, r\"$x_1$\", ha=\"left\", color=\"blue\")\nax.text(x2, -1.0, r\"$x_2$\", ha=\"left\", color=\"blue\")\n\n# Região delta em torno de p\nax.axvspan(p - delta, p + delta, color=\"orange\", alpha=0.3, label=r\"$|x-p|&lt;\\delta$\")\n\n# Ponto limite\nax.plot(p, L, \"ro\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(\"$f(x)$\")\nax.legend(loc=\"best\")\nax.set_title(r\"Escolha de $\\delta$ a partir de $x_1$ e $x_2$\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Representação gráfica da definição de limite \\(\\lim_{x \\to p} f(x) = L\\).\n\n\n\n\n\n\n\n\nExemplo com Descontinuidade\nQuando uma função apresenta uma descontinuidade em um ponto \\(p\\), a definição de limite \\(\\lim_{x \\to p} f(x) = L\\) falha. Vejamos um exemplo concreto:\nConsidere a função definida por partes: \\[\nf(x) =\n\\begin{cases}\nx & \\text{se } x &lt; 1 \\\\\n2x^2 & \\text{se } x \\geq 1\n\\end{cases}\n\\]\nNo ponto \\(x = 1\\), suponha que tentamos provar que \\(\\lim_{x \\to 1} f(x) = 2\\). Escolhemos \\(\\varepsilon = 0.1\\).\nQueremos encontrar \\(\\delta &gt; 0\\) tal que o conjunto: \\[\n\\{x \\in \\mathbb{R} : 0 &lt; |x - 1| &lt; \\delta\\}\n\\] esteja contido no conjunto: \\[\n\\{x \\in \\mathbb{R} : |f(x) - 2| &lt; 0.1\\}\n\\]\nAnálise do conjunto onde \\(|f(x) - 2| &lt; 0.1\\):\nPara \\(x \\geq 1\\) (usando \\(f(x) = 2x^2\\)): \\[\n|2x^2 - 2| &lt; 0.1 \\Rightarrow 1.9 &lt; 2x^2 &lt; 2.1 \\Rightarrow x \\in (\\sqrt{0.95}, \\sqrt{1.05}) \\approx (0.975, 1.025)\n\\]\nPara \\(x &lt; 1\\) (usando \\(f(x) = x\\)): \\[\n|x - 2| &lt; 0.1 \\Rightarrow x \\in (1.9, 2.1)\n\\]\nPortanto, o conjunto total onde \\(|f(x) - 2| &lt; 0.1\\) é aproximadamente: \\[\n(0.975, 1.025) \\cup (1.9, 2.1)\n\\]\nO problema: Qualquer intervalo \\((1-\\delta, 1+\\delta)\\) com \\(\\delta &gt; 0\\) conterá pontos que não pertencem a este conjunto. Especificamente, conterá pontos em \\((1-\\delta, 1)\\) onde \\(f(x) = x \\approx 1\\), e portanto \\(|f(x) - 2| \\approx 1 &gt; 0.1\\).\nA única maneira de evitar isso seria tomar \\(\\delta = 0\\), mas isso viola a definição de limite que exige \\(\\delta &gt; 0\\). Esta impossibilidade de encontrar um \\(\\delta &gt; 0\\) tal que a vizinhança perfurada de \\(1\\) esteja contida no conjunto onde \\(|f(x) - 2| &lt; \\varepsilon\\) caracteriza a não existência do limite.\n\n\nCode\n# Código corrigido para plotar a função por partes (legendas em texto simples)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef f_piece(x):\n    return np.where(x &lt; 1, x, 2 * x**2)\n\n\n# Domínios separados para desenhar as duas partes claramente\nx_left = np.linspace(0.5, 0.999, 400)\nx_right = np.linspace(1, 1.5, 400)\n\ny_left = f_piece(x_left)\ny_right = f_piece(x_right)\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot das duas parcelas (sem especificar cores)\nax.plot(x_left, y_left, label=r\"$f(x)=x, x&lt;1$\")\nax.plot(x_right, y_right, label=r\"$f(x)=2x^2, x&gt;=1$\")\n\n# Marcar limites laterais em x=1\nx0 = 1.0\nleft_limit = 1.0  # lim x-&gt;1^- f(x) = 1\nright_limit = 2.0  # lim x-&gt;1^+ f(x) = 2\nvalue_at_1 = 2.0  # f(1) = 2*1^2 = 2\n\n# Limite pela esquerda: ponto aberto em (1,1)\nax.plot(\n    x0,\n    left_limit,\n    marker=\"o\",\n    markersize=8,\n    markerfacecolor=\"none\",\n    markeredgewidth=1.2,\n    label=r\"$\\lim_{x-&gt;1^-} f(x) = 1$ (aberto)\",\n)\n\n# Limite pela direita: ponto aberto em (1,2)\nax.plot(\n    x0,\n    right_limit,\n    marker=\"o\",\n    markersize=8,\n    markerfacecolor=\"none\",\n    markeredgewidth=1.2,\n    label=r\"$\\lim_{x-&gt;1^+}f(x) = 2$ (aberto)\",\n)\n\n# Valor da função em x=1: ponto preenchido\nax.plot(x0, value_at_1, marker=\"o\", markersize=8, label=\"f(1)=2 (valor)\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(\"$f(x)$\")\nax.set_title(\"Função por partes com descontinuidade em x=1\")\nax.legend(loc=\"upper left\")\nax.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Representação gráfica da definição de limite \\(\\lim_{x \\to p} f(x) = L\\).\n\n\n\n\n\n\n\nExercícios\n\nExplique em palavras simples o que significa o limite \\(\\lim_{x \\to p} f(x) = L\\) usando a ideia de \\(\\varepsilon\\) e \\(\\delta\\).\nPara \\(f(x) = 3x + 1\\), calcule \\(\\delta\\) correspondente a \\(\\varepsilon = 0.1\\) quando \\(x \\to 2\\).\nPor que a escolha de \\(\\delta = \\min(|x_1 - p|, |x_2 - p|)\\) garante que todos os valores de \\(x\\) dentro de \\((p-\\delta, p+\\delta)\\) levam \\(f(x)\\) para dentro da faixa \\(|f(x)-L|&lt;\\varepsilon\\)?\nConsidere a função por partes \\[\nf(x) =\n\\begin{cases}\nx & \\text{se } x &lt; 1 \\\\\n2x^2 & \\text{se } x \\ge 1\n\\end{cases}\n\\] e \\(\\varepsilon = 0.1\\). Mostre que não existe \\(\\delta &gt; 0\\) que satisfaça a definição de limite em \\(x=1\\).\nExplique como a descontinuidade em \\(x=1\\) impede a existência do limite \\(\\lim_{x \\to 1} f(x)\\).",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Physics and Mathematics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/heisenberg-uncertainty.html",
    "href": "courses/introduction-to-physics/heisenberg-uncertainty.html",
    "title": "Introdução ao Princípio da Incerteza de Heisenberg",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Heisenberg Uncertainty Principle"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/heisenberg-uncertainty.html#lei-de-newton-e-momento",
    "href": "courses/introduction-to-physics/heisenberg-uncertainty.html#lei-de-newton-e-momento",
    "title": "Introdução ao Princípio da Incerteza de Heisenberg",
    "section": "Lei de Newton e momento",
    "text": "Lei de Newton e momento\nA mecânica clássica descreve o movimento de uma partícula pela segunda lei de Newton: \\[\nm \\ddot{x}(t) = F(x,t),\n\\] onde \\(\\ddot{x} = \\dd^2x/\\dd t^2\\) é a aceleração, \\(m\\) a massa, e \\(F\\) a força resultante.\nDefinimos o momento linear como \\[\np(t) \\equiv m \\dot{x}(t).\n\\] Derivando em relação ao tempo: \\[\n\\dot{p}(t) = m \\ddot{x}(t) = F(x,t).\n\\] Portanto, \\(m\\ddot{x}=F\\) é equivalente a \\(\\dot{p}=F\\).\n\nSistema de primeira ordem\nPodemos reescrever a equação de segunda ordem em \\(x\\) como um sistema de primeira ordem para as variáveis \\(x(t)\\) e \\(p(t)\\): \\[\n\\dot{x}(t) = \\frac{p(t)}{m}, \\qquad \\dot{p}(t) = F(x,t).\n\\]\n\n\nExpansão temporal para \\(\\delta t\\)\nPara um pequeno intervalo \\(\\delta t\\), expandindo por Taylor de primeira ordem: \\[\nx(t+\\delta t) \\approx x(t) + \\frac{p(t)}{m} , \\delta t,\n\\] \\[\np(t+\\delta t) \\approx p(t) + F(x,t),\\delta t.\n\\]\n\n\nForça conservativa e potencial\nSe a força for conservativa, existe uma função potencial \\(V(x)\\) tal que \\[\nF(x) = -\\frac{\\dd V}{\\dd x}.\n\\]\n\n\nFormulação Hamiltoniana\nDefinimos o Hamiltoniano \\[\nH(x,p) = \\frac{p^2}{2m} + V(x).\n\\] As equações de movimento são obtidas pelas derivadas parciais: \\[\n\\dot{x} = \\frac{\\partial H}{\\partial p} = \\frac{p}{m}, \\qquad\n\\dot{p} = -\\frac{\\partial H}{\\partial x} = -\\frac{\\dd V}{\\dd x}.\n\\] Note que \\(\\tfrac{p^2}{2m}\\) depende apenas de \\(p\\), e \\(V(x)\\) depende apenas de \\(x\\), o que simplifica os cálculos.\n\n\nEstrutura matricial\nPodemos condensar o sistema na forma matricial: \\[\n\\frac{\\dd}{\\dd t} \\begin{pmatrix} x \\\\ p \\end{pmatrix} =\n\\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}\n\\begin{pmatrix} \\frac{\\partial H}{\\partial x} \\\\ \\frac{\\partial H}{\\partial p} \\end{pmatrix}.\n\\] A matriz \\[\nJ = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}\n\\] é chamada de matriz simplética e codifica a estrutura da mecânica Hamiltoniana.\n\n\nExemplo: Oscilador harmônico\nPara \\(V(x) = \\tfrac{1}{2}kx^2\\), temos: \\[\nH(x,p) = \\frac{p^2}{2m} + \\frac{1}{2}kx^2,\n\\] \\[\n\\dot{x} = \\frac{p}{m}, \\qquad \\dot{p} = -kx.\n\\] Combinando: \\(m\\ddot{x} = -kx\\), que é a equação do oscilador harmônico.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# parâmetros\nm = 2.0\nk = 0.34\nomega = np.sqrt(k / m)\n\n# tempo\nt = np.linspace(0, 2 * np.pi / omega, 400)\n\n# condições iniciais\nx0 = 1.0\np0 = 0.0\n\n# soluções analíticas\nx = x0 * np.cos(omega * t) + (p0 / (m * omega)) * np.sin(omega * t)\np = p0 * np.cos(omega * t) - m * omega * x0 * np.sin(omega * t)\n\n# gráfico no espaço de fases\nplt.figure(figsize=(6, 6))\nplt.plot(x, p, lw=2)\n\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$p$\")\nplt.title(\"Espaço de fases do oscilador harmônico\")\nplt.grid(alpha=0.3)\nplt.axis(\"equal\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Evolução de uma trajetória no espaço de fases \\((x,p)\\) para o oscilador harmônico.\n\n\n\n\n\n\n\nExercícios\n\nMostre que a segunda lei de Newton \\(m \\ddot{x} = F(x,t)\\) é equivalente à equação \\(\\dot{p} = F(x,t)\\) usando a definição de momento linear \\(p = m \\dot{x}\\).\nReescreva a equação \\(m \\ddot{x} = F(x,t)\\) como um sistema de primeira ordem em termos de \\(x(t)\\) e \\(p(t)\\) e explique por que isso pode ser útil para métodos numéricos.\nConsidere um pequeno intervalo de tempo \\(\\delta t\\). Derive a expansão de Taylor de primeira ordem para \\(x(t+\\delta t)\\) e \\(p(t+\\delta t)\\) a partir das equações de primeira ordem.\nUma força é dita conservativa se existe um potencial \\(V(x)\\) tal que \\(F(x) = -\\dd V/\\dd x\\). Dê dois exemplos de forças conservativas e dois exemplos de forças não conservativas.\nVerifique que para o Hamiltoniano \\(H(x,p) = \\frac{p^2}{2m} + V(x)\\), as equações de Hamilton \\[\n\\dot{x} = \\frac{\\partial H}{\\partial p}, \\qquad \\dot{p} = -\\frac{\\partial H}{\\partial x}\n\\] são equivalentes às equações clássicas de movimento.\nExplique qualitativamente como a forma elíptica da trajetória no espaço de fases do oscilador harmônico reflete a conservação de energia.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Heisenberg Uncertainty Principle"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/heisenberg-uncertainty.html#álgebra-e-produto-vetorial",
    "href": "courses/introduction-to-physics/heisenberg-uncertainty.html#álgebra-e-produto-vetorial",
    "title": "Introdução ao Princípio da Incerteza de Heisenberg",
    "section": "Álgebra e Produto Vetorial",
    "text": "Álgebra e Produto Vetorial\nEm duas dimensões, a matriz \\[\nJ = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}\n\\] aparece naturalmente na formulação Hamiltoniana. Ela atua como uma rotação de \\(90^\\circ\\), e pode ser usada para construir um produto bilinear que captura a noção de área orientada: \\[\n\\vec{v}_1^T J \\vec{v}_2.\n\\]\nDe forma explícita, se \\(\\vec{v}_1 = \\begin{pmatrix}x_1 \\\\ p_1\\end{pmatrix}\\) e \\(\\vec{v}_2 = \\begin{pmatrix}x_2 \\\\ p_2\\end{pmatrix}\\), então \\[\n\\vec{v}_1^T J \\vec{v}_2 = x_1 p_2 - p_1 x_2.\n\\] Note que, ao escrever a equação acima, estamos tratando \\((x,p)\\) como coordenadas de um plano e interpretando esse determinante como uma área orientada. Essa interpretação, porém, exige certo cuidado: \\(x\\) e \\(p\\) não têm as mesmas unidades físicas nem o mesmo significado geométrico. O espaço de fase não é um espaço euclidiano comum, mas sim um espaço com estrutura própria, onde a matriz \\(J\\) define uma forma bilinear antissimétrica (a chamada forma simplética). Assim, falar em “área” aqui é uma analogia geométrica útil para construir intuição, mas ela só se torna rigorosa quando entendemos que o espaço \\((x,p)\\) é dotado dessa estrutura simplética, que substitui o produto interno usual.\nEsse determinante mede a área orientada do paralelogramo formado pelos dois vetores:\n\nSe o valor é zero, os vetores são linearmente dependentes, pois não formam área.\nSe é positivo ou negativo, ele indica tanto o tamanho da área quanto a orientação relativa entre \\(\\vec{v}_1\\) e \\(\\vec{v}_2\\).\n\nAssim, \\(J\\) fornece um critério geométrico para independência linear e orientação no espaço de fase. Essa construção é análoga ao produto vetorial em três dimensões, que também mede áreas e orientações.\n\nConexão com os Parênteses de Poisson\nA mesma estrutura aparece nas equações da mecânica Hamiltoniana. Dadas duas funções \\(f(x,p)\\) e \\(g(x,p)\\) no espaço de fase, seus parênteses de Poisson são definidos por\n\\[\n\\{f,g\\} =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial p}\n\\end{pmatrix}\nJ\n\\begin{pmatrix}\n\\frac{\\partial g}{\\partial x} \\\\\n\\frac{\\partial g}{\\partial p}\n\\end{pmatrix}.\n\\]\nOu seja, a mesma operação \\(\\vec{v}_1^T J \\vec{v}_2\\) que mede área entre vetores também mede a “independência” entre funções no espaço de fase. Essa estrutura é o coração da mecânica Hamiltoniana, e mais adiante veremos como ela se conecta às relações de comutação na mecânica quântica.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Vetores\nv1 = np.array([2, 1])\nv2 = np.array([1, 2])\n\n# Matriz J\nJ = np.array([[0, 1], [-1, 0]])\n\n# Área orientada\narea = v1 @ J @ v2\n\n# Plot\nfig, ax = plt.subplots(figsize=(5, 5))\nax.quiver(\n    0,\n    0,\n    v1[0],\n    v1[1],\n    angles=\"xy\",\n    scale_units=\"xy\",\n    scale=1,\n    color=\"blue\",\n    label=r\"$\\vec{v}_1$\",\n)\nax.quiver(\n    0,\n    0,\n    v2[0],\n    v2[1],\n    angles=\"xy\",\n    scale_units=\"xy\",\n    scale=1,\n    color=\"red\",\n    label=r\"$\\vec{v}_2$\",\n)\n\n# Preenche o paralelogramo\nparallelogram = np.array([[0, 0], v1, v1 + v2, v2])\nax.fill(parallelogram[:, 0], parallelogram[:, 1], alpha=0.3, color=\"gray\")\n\nax.set_xlim(0, 3)\nax.set_ylim(0, 3)\nax.set_aspect(\"equal\")\nax.axhline(0, color=\"black\", linewidth=0.5)\nax.axvline(0, color=\"black\", linewidth=0.5)\nax.legend()\nax.set_title(rf\"$\\vec{{v}}_1^T J \\vec{{v}}_2 = {area}$\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Área orientada gerada por dois vetores no plano de fase. O sinal de v1^T J v2 indica a orientação.\n\n\n\n\n\n\n\nExercícios\n\nVerifique que, para \\(\\vec{v}_1 = \\begin{pmatrix}x_1 \\\\ p_1\\end{pmatrix}\\) e \\(\\vec{v}_2 = \\begin{pmatrix}x_2 \\\\ p_2\\end{pmatrix}\\), o produto \\(\\vec{v}_1^T J \\vec{v}_2\\) é antissimétrico, ou seja, \\(\\vec{v}_1^T J \\vec{v}_2 = - \\vec{v}_2^T J \\vec{v}_1\\).\nMostre que \\(\\vec{v}_1^T J \\vec{v}_2 = 0\\) se e somente se os vetores \\(\\vec{v}_1\\) e \\(\\vec{v}_2\\) forem linearmente dependentes.\nPara os vetores \\(\\vec{v}_1 = \\begin{pmatrix}1\\\\0\\end{pmatrix}\\) e \\(\\vec{v}_2 = \\begin{pmatrix}0\\\\1\\end{pmatrix}\\), calcule \\(\\vec{v}_1^T J \\vec{v}_2\\) e interprete o resultado geometricamente.\nConsidere as funções \\(f(x,p) = x^2 p\\) e \\(g(x,p) = x p^2\\). Calcule os parênteses de Poisson \\({f,g}\\) explicitamente.\nDemonstre que os parênteses de Poisson satisfazem a antissimetria: \\({f,g} = -{g,f}\\).\nExplique como a interpretação de “área orientada” do determinante \\(\\vec{v}_1^T J \\vec{v}_2\\) se relaciona com a independência de funções no espaço de fase.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Heisenberg Uncertainty Principle"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/heisenberg-uncertainty.html#parênteses-de-poisson-e-mecânica-hamiltoniana",
    "href": "courses/introduction-to-physics/heisenberg-uncertainty.html#parênteses-de-poisson-e-mecânica-hamiltoniana",
    "title": "Introdução ao Princípio da Incerteza de Heisenberg",
    "section": "Parênteses de Poisson e Mecânica Hamiltoniana",
    "text": "Parênteses de Poisson e Mecânica Hamiltoniana\nA evolução temporal de uma função \\(f(x,p)\\) que depende de posição e momento é obtida pela regra da cadeia: \\[\n\\frac{\\dd}{\\dd t} f(x(t),p(t)) = \\frac{\\partial f}{\\partial x}\\dot{x} + \\frac{\\partial f}{\\partial p}\\dot{p}.\n\\] Essas funções são chamadas de observáveis físicos. Ou seja, definimos como observável qualquer quantidade que pode ser medida a partir do estado do sistema.\nSubstituindo as equações de Hamilton, essa expressão pode ser reescrita em termos do parêntese de Poisson: \\[\n\\frac{\\dd}{\\dd t} f = \\{f,H\\}, \\quad \\text{com} \\quad\n\\{f,H\\} = \\frac{\\partial f}{\\partial x}\\frac{\\partial H}{\\partial p} - \\frac{\\partial f}{\\partial p}\\frac{\\partial H}{\\partial x}.\n\\]\n\nO papel gerador dos parênteses de Poisson\nO parêntese de Poisson não é apenas uma notação conveniente: ele descreve como um observável gera transformações em outro.\n\nO Hamiltoniano \\(H\\) gera a evolução temporal: \\[\n\\frac{\\dd}{\\dd t} = \\{\\cdot, H\\}.\n\\]\nO momento \\(p\\) gera translações em \\(x\\): \\[\n\\{x,p\\} = 1 \\quad \\Rightarrow \\quad \\delta x = \\epsilon \\{x,p\\} = \\epsilon.\n\\]\nA posição \\(x\\) gera translações em \\(p\\): \\[\n\\{p,x\\} = -1 \\quad \\Rightarrow \\quad \\delta p = \\epsilon \\{p,x\\} = -\\epsilon.\n\\]\n\nAssim, podemos pensar nos observáveis como “geradores de transformações” uns sobre os outros. Essa é uma das ideias centrais da mecânica Hamiltoniana e antecipa a estrutura da mecânica quântica, onde os observáveis passam a gerar transformações via comutadores.\n\n\nConservação da energia\nQuando o Hamiltoniano não depende explicitamente do tempo, ele mesmo é um integral de movimento. Isso fica claro porque \\[\n\\frac{\\dd H}{\\dd t} = \\{H,H\\} = 0.\n\\] Portanto, a energia é sempre conservada. Lembrando que isso é verdade pois consideramos \\(H\\) como uma função de \\(x\\) e \\(p\\) e não de \\(t\\).\n\n\nEstrutura algébrica\nOs parênteses de Poisson satisfazem ainda a identidade de Jacobi, que garante a consistência da álgebra: \\[\n\\{ \\{A,B\\},C \\} + \\{ \\{C,A\\},B \\} + \\{ \\{B,C\\},A \\} = 0.\n\\] onde \\(A\\), \\(B\\) e \\(C\\) são funções de \\(x\\) e \\(p\\). Essa propriedade assegura que os observáveis formam uma estrutura matemática bem definida (uma álgebra de Lie), algo que será fundamental ao fazer a transição para a mecânica quântica.\n\n\nCampo de Hamilton\nA ideia central da mecânica Hamiltoniana consiste em estudar o fluxo no espaço de fase gerado pela evolução temporal do Hamiltoniano. A figura abaixo ilustra o fluxo gerado pelo Hamiltoniano do oscilador harmônico. Os vetores indicam a direção da evolução temporal: \\[\n\\vec{V} = J \\pmatrix{\\frac{\\partial H}{\\partial x} \\\\ \\frac{\\partial H}{\\partial p}}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parâmetros\nm, k = 1.0, 1.0\nx = np.linspace(-2.5, 2.5, 30)\np = np.linspace(-2.5, 2.5, 30)\nX, P = np.meshgrid(x, p)\n\n# Equações de Hamilton\ndxdt = P / m\ndpdt = -k * X\n\n# Energia (curvas de nível)\nH = P**2 / (2 * m) + 0.5 * k * X**2\n\nplt.figure(figsize=(7, 6))\n# Curvas de nível de energia\ncontours = plt.contour(X, P, H, levels=6, colors=\"gray\", linewidths=1)\nplt.clabel(contours, inline=True, fontsize=8, fmt=\"E=%.1f\")\n\n# Campo vetorial (fluxo de Hamilton)\nplt.quiver(X, P, dxdt, dpdt, color=\"blue\", alpha=0.6)\n\nplt.xlabel(\"x (posição)\")\nplt.ylabel(\"p (momento)\")\nplt.title(\"Fluxo Hamiltoniano no Espaço de Fase\")\nplt.grid(alpha=0.3)\nplt.axhline(0, color=\"black\", linewidth=0.5)\nplt.axvline(0, color=\"black\", linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Fluxo no espaço de fase gerado pelo Hamiltoniano do oscilador harmônico. As curvas representam níveis de energia constante e os vetores indicam a direção da evolução temporal.\n\n\n\n\n\nNote que as curvas de nível de energia, como mostrado na Figure 3, sinalizam os níveis de energia, enquanto o campo vetorial ilustra o fluxo no espaço de fase. As curvas de nível coincidem com as trajetórias clássicas. Essa visualização reforça a interpretação de que o Hamiltoniano atua como gerador de transformações temporais: ao calcular \\(\\dot{f} = {f,H}\\), vemos que o parêntese de Poisson com \\(H\\) fornece a regra de evolução de qualquer observável no tempo.\n\n\nExercícios\n\nMostre que a regra da cadeia aplicada a uma função \\(f(x(t),p(t))\\) leva a \\(\\dot{f} = \\{f,H\\}\\) usando as equações de Hamilton.\nVerifique explicitamente que \\(\\{x,p\\} = 1\\) e \\(\\{p,x\\} = -1\\), e explique o significado físico dessas relações.\nConsidere o Hamiltoniano \\(H = \\frac{p^2}{2m} + \\frac{1}{2}kx^2\\). Calcule \\(\\dot{x}\\) e \\(\\dot{p}\\) usando parênteses de Poisson e confirme que obtém as equações de movimento do oscilador harmônico.\nMostre que se \\(H\\) não depende explicitamente do tempo, então \\(\\dot{H} = {H,H} = 0\\), garantindo a conservação da energia.\nPara três funções \\(A=x\\), \\(B=p\\), \\(C=H\\), verifique a identidade de Jacobi: \\[\n\\{\\{A,B\\},C\\} + \\{\\{C,A\\},B\\} + \\{\\{B,C\\},A\\} = 0.\n\\]\nExplique qualitativamente como o fluxo Hamiltoniano no espaço de fase, representado por \\(\\vec{V} = J \\begin{pmatrix} \\partial H/\\partial x \\\\ \\partial H/\\partial p \\end{pmatrix}\\), garante que as curvas de nível de energia sejam percorridas pelas trajetórias clássicas.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Heisenberg Uncertainty Principle"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/heisenberg-uncertainty.html#quantização",
    "href": "courses/introduction-to-physics/heisenberg-uncertainty.html#quantização",
    "title": "Introdução ao Princípio da Incerteza de Heisenberg",
    "section": "Quantização",
    "text": "Quantização\nO formalismo da mecânica quântica é construído promovendo as variáveis clássicas a operadores lineares sobre funções de onda. A regra de correspondência fundamental, inspirada pelos parênteses de Poisson, é: \\[\n[x,p] = i\\hbar \\{x,p\\}.\n\\]\nNo caso canônico, isso leva à relação de comutação: \\[\n[\\hat{x}, \\hat{p}] = i\\hbar,\n\\]\nonde \\([\\hat{A},\\hat{B}] \\equiv \\hat{A}\\hat{B} - \\hat{B}\\hat{A}\\). Essa relação não apenas substitui a noção de variável clássica por operador, mas também codifica a estrutura de incerteza da mecânica quântica.\nNote que, em uma álgebra não comutativa, o produto de dois operadores \\(\\hat{A}\\) e \\(\\hat{B}\\) pode sempre ser decomposto em uma parte simétrica e uma parte antissimétrica: \\[\n\\hat{A}\\hat{B} = \\frac{\\hat{A}\\hat{B} + \\hat{B}\\hat{A}}{2} + \\frac{\\hat{A}\\hat{B} - \\hat{B}\\hat{A}}{2}.\n\\]\n\nO primeiro termo, \\(\\hat{A}\\hat{B}+\\hat{B}\\hat{A}\\), é o anti-comutador, definido como \\[\n\\{\\hat{A},\\hat{B}\\} \\equiv \\hat{A}\\hat{B} + \\hat{B}\\hat{A},\n\\] e representa a parte simétrica do produto. Para operadores hermitianos, essa parte é real quando calculada em um estado.\nO segundo termo, \\(\\hat{A}\\hat{B}-\\hat{B}\\hat{A}\\), é o comutador, definido como \\[\n[\\hat{A},\\hat{B}] \\equiv \\hat{A}\\hat{B} - \\hat{B}\\hat{A},\n\\] e representa a parte antissimétrica do produto. Para operadores hermitianos, essa parte é puramente imaginária quando calculada em um estado.\n\nEssa decomposição é útil para entender a desigualdade de Heisenberg: a parte imaginária (comutador) determina o limite fundamental de incerteza, enquanto a parte real (anti-comutador) pode aumentar o módulo do produto, mas não reduz o limite mínimo.\nNão confunda com a notação do parêntese de Poisson, que é usado na mecânica clássica; o contexto normalmente indica se estamos tratando de comutadores quânticos ou parênteses de Poisson.\n\nRepresentação de posição\nNa representação em que usamos funções de onda \\(\\psi(x)\\), os operadores agem como: \\[\n\\hat{x}\\psi(x) = x \\psi(x), \\quad\n\\hat{p}\\psi(x) = -i\\hbar \\frac{\\partial}{\\partial x}\\psi(x).\n\\]\nAqui, \\(\\hat{x}\\) atua multiplicando pela coordenada, enquanto \\(\\hat{p}\\) se torna um operador diferencial. Essa escolha garante que a relação de comutação seja satisfeita e que a evolução quântica tenha a mesma estrutura de transformação que vimos na mecânica clássica via parênteses de Poisson.\n\n\nOperadores como geradores de transformações\nUm dos conceitos centrais da mecânica quântica, herdado da mecânica Hamiltoniana, é que cada observável pode ser visto como um gerador de transformações.\n\nNa mecânica clássica, vimos que o Hamiltoniano \\(H\\) gera a evolução temporal através do parêntese de Poisson: \\(\\dot{f} = \\{f,H\\}\\).\nEm mecânica quântica, essa ideia é promovida para operadores: um operador \\(\\hat{A}\\) gera uma transformação unitária sobre estados \\(\\ket{\\psi}\\) via o operador exponencial \\(e^{-i \\epsilon \\hat{A}/\\hbar}\\).\nPor exemplo, o momento \\(\\hat{p}\\) gera translações em posição, enquanto a posição \\(\\hat{x}\\) gera translações em momento, refletindo a mesma estrutura observável \\(\\to\\) transformação que vimos no espaço de fase clássico.\n\nEssa interpretação é crucial, porque mostra que os operadores não são apenas “valores para medir”, mas agentes de transformação dentro do espaço de Hilbert (que é o espaço de funções de onda). Ela estabelece a ponte para o formalismo estatístico e para a evolução temporal, e é uma das ideias centrais para entender por que a mecânica quântica difere da clássica.\n\nCada operador representa um observável físico, como posição ou momento.\nO comutador \\([\\hat{x},\\hat{p}]\\) mostra que não podemos medir \\(x\\) e \\(p\\) simultaneamente com precisão arbitrária.\nA ação de um operador sobre a função de onda determina a distribuição de valores possíveis de um observável.\n\nEssas definições estabelecem a base da mecânica quântica e preparam o terreno para introduzir conceitos estatísticos, como valor esperado, variância e, mais adiante, o princípio da incerteza de Heisenberg.\n\n\nExercícios\n\nMostre que a decomposição de um produto de operadores \\[\n\\hat{A}\\hat{B} = \\frac{\\hat{A}\\hat{B} + \\hat{B}\\hat{A}}{2} + \\frac{\\hat{A}\\hat{B} - \\hat{B}\\hat{A}}{2}\n\\] é sempre válida, identificando explicitamente o anti-comutador e o comutador.\nVerifique que, na representação de posição, os operadores \\[\n\\hat{x}\\psi(x) = x \\psi(x), \\quad \\hat{p}\\psi(x) = -i\\hbar \\frac{\\partial}{\\partial x} \\psi(x)\n\\] satisfazem a relação de comutação \\([\\hat{x},\\hat{p}] = i\\hbar\\).",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Heisenberg Uncertainty Principle"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/heisenberg-uncertainty.html#valor-esperado-e-probabilidade",
    "href": "courses/introduction-to-physics/heisenberg-uncertainty.html#valor-esperado-e-probabilidade",
    "title": "Introdução ao Princípio da Incerteza de Heisenberg",
    "section": "Valor Esperado e Probabilidade",
    "text": "Valor Esperado e Probabilidade\nNa mecânica quântica, o valor de uma grandeza não é determinístico, mas probabilístico. Para entender isso, recordamos primeiro o caso clássico de variáveis aleatórias.\n\nExemplo discreto: dado de seis lados\nO valor esperado de uma variável discreta é uma média ponderada pelas probabilidades: \\[\n\\langle m \\rangle = \\sum_{i=1}^{6} m_i P_i,\n\\] onde \\(m_i\\) são os valores possíveis \\((1,2,\\dots,6)\\) e \\(P_i\\) é a probabilidade de cada valor. Para um dado justo, \\(P_i = 1/6\\), então: \\[\n\\langle m \\rangle = \\frac{1+2+3+4+5+6}{6} = 3.5.\n\\]\n\n\nExemplo contínuo: medindo a posição\nPara um sistema contínuo, como a posição de uma partícula confinada em um intervalo \\([a,b]\\), podemos dividir o intervalo em \\(N\\) “caixas” de comprimento \\(\\Delta x = (b-a)/N\\). A posição média pode ser escrita como soma ponderada: \\[\n\\langle x \\rangle \\approx \\sum_{i=1}^{N} x_i P(x_i) \\Delta x,\n\\] onde \\(P(x_i)\\) é a probabilidade de encontrar a partícula na caixa \\(i\\). No limite \\(N \\to \\infty\\) (ou \\(\\Delta x \\to 0\\)), isso se transforma na integral usual: \\[\n\\langle x \\rangle = \\int_a^b x P(x) \\dd x.\n\\] Para a distribuição uniforme \\(P(x) = 1/(b-a)\\), temos: \\[\n\\langle x \\rangle = \\frac{1}{b-a} \\int_a^b x \\dd x = \\frac{a+b}{2}.\n\\]\nEsse exemplo mostra claramente a transição do discreto para o contínuo, que é exatamente o que ocorre ao medir observáveis em mecânica quântica: valores possíveis discretos ou contínuos, mas sempre ponderados por uma probabilidade derivada da função de onda.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Intervalo e distribuição\na, b = 0, 1\nN = 5  # número de caixas discretas\nx_discrete = np.linspace(a + (b-a)/(2*N), b - (b-a)/(2*N), N)\ndx = (b-a)/N\nP_discrete = np.full(N, 1/(b-a))\n\n# Valores esperados\nexpected_discrete = np.sum(x_discrete * P_discrete * dx)\nx_cont = np.linspace(a, b, 1000)\nP_cont = np.ones_like(x_cont)/(b-a)\n\nplt.figure(figsize=(8,5))\n# Barras discretas\nplt.bar(x_discrete, P_discrete, width=dx, alpha=0.4, color='orange', label='Soma discreta')\n# Linha contínua\nplt.plot(x_cont, P_cont, 'b-', linewidth=2, label='Densidade contínua')\n# Valor esperado\nplt.axvline(expected_discrete, color='r', linestyle='--', label=f'Valor esperado ≈ {expected_discrete:.2f}')\n\nplt.xlabel(\"x\")\nplt.ylabel(\"Probabilidade\")\nplt.title(\"Valor esperado: discreto vs contínuo\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Ilustração do valor esperado: barras representam a soma discreta das probabilidades em caixas, enquanto a linha azul mostra a função de densidade contínua.\n\n\n\n\n\nNa figura, as barras laranjas representam a soma discreta das probabilidades em caixas de mesmo tamanho, enquanto a linha azul mostra a densidade contínua correspondente. À medida que aumentamos o número de caixas, a soma discreta se aproxima da integral contínua, ilustrando o limite $ x $. O valor esperado, indicado pela linha vermelha tracejada, é a média ponderada pela probabilidade, seja na forma discreta ou contínua. Esse conceito é diretamente aplicável à mecânica quântica, onde a função de onda fornece a densidade de probabilidade e o valor esperado de um observável é obtido como integral ponderada.\n\n\nExercícios\n\nCalcule o valor esperado de um dado justo de seis lados e de um dado viciado, onde \\(P_6 = 1/2\\) e os outros cinco lados têm probabilidades iguais.\nDivida o intervalo \\([0,2]\\) em \\(N=4\\) caixas e considere uma distribuição de probabilidade \\(P(x_i) = x_i/(\\sum_j x_j)\\). Calcule o valor esperado \\(\\langle x \\rangle\\) usando a soma discreta e compare com o limite contínuo.\nDemonstre que, para uma distribuição uniforme em \\([a,b]\\), o valor esperado é \\(\\langle x \\rangle = (a+b)/2\\).",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Heisenberg Uncertainty Principle"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/heisenberg-uncertainty.html#variância-e-desvio-padrão",
    "href": "courses/introduction-to-physics/heisenberg-uncertainty.html#variância-e-desvio-padrão",
    "title": "Introdução ao Princípio da Incerteza de Heisenberg",
    "section": "Variância e Desvio Padrão",
    "text": "Variância e Desvio Padrão\nAlém do valor esperado, é importante caracterizar a dispersão dos valores possíveis de uma variável aleatória. Uma medida natural é a variância, definida como \\[\n\\text{Var}(x) \\equiv \\langle (x - \\mu)^2 \\rangle,\n\\] onde \\(\\mu = \\langle x \\rangle\\) é o valor esperado. Essa definição garante que:\n\nA variância é sempre positiva ou zero.\nMedidas como \\(\\langle |x-\\mu| \\rangle\\) também existem, mas \\((x-\\mu)^2\\) tem propriedades matemáticas convenientes, como ser aditiva para variáveis independentes.\n\nExpandindo o quadrado, obtemos: \\[\n\\text{Var}(x) = \\langle x^2 - 2\\mu x + \\mu^2 \\rangle = \\langle x^2 \\rangle - 2\\mu \\langle x \\rangle + \\mu^2 = \\langle x^2 \\rangle - \\mu^2.\n\\]\n\nExemplo: distribuição uniforme em \\([a,b]\\)\nPara uma distribuição uniforme, \\(P(x) = 1/(b-a)\\). O valor esperado é \\[\n\\mu = \\langle x \\rangle = \\int_a^b x P(x)\\dd x = \\frac{1}{b-a} \\int_a^b x\\dd  = \\frac{a+b}{2}.\n\\]\nO segundo momento é \\[\n\\langle x^2 \\rangle = \\int_a^b x^2 P(x)\\dd x = \\frac{1}{b-a} \\int_a^b x^2\\dd x = \\frac{b^3 - a^3}{3(b-a)} = \\frac{a^2 + ab + b^2}{3}.\n\\] Portanto, a variância é \\[\n\\text{Var}(x) = \\langle x^2 \\rangle - \\mu^2 = \\frac{a^2 + ab + b^2}{3} - \\left(\\frac{a+b}{2}\\right)^2 = \\frac{(b-a)^2}{12}.\n\\] O desvio padrão é a raiz quadrada da variância: \\[\n\\sigma = \\sqrt{\\text{Var}(x)} = \\frac{b-a}{2\\sqrt{3}}.\n\\]\n\n\nCaso particular: \\([0,1]\\)\nSubstituindo \\(a=0\\) e \\(b=1\\): \\[\n\\mu = \\frac{0+1}{2} = \\frac{1}{2}, \\quad\n\\text{Var}(x) = \\frac{(1-0)^2}{12} = \\frac{1}{12}, \\quad\n\\sigma = \\frac{1}{2\\sqrt{3}} \\approx 0.2887.\n\\]\nEssa forma mostra que a dispersão é invariante sob deslocamentos lineares e que o desvio padrão fornece uma medida intuitiva do “tamanho típico do desvio” em torno da média.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Intervalo da distribuição uniforme\na, b = 0, 1\nmu = (a+b)/2\nsigma = (b-a)/(2*np.sqrt(3))\n\n# Função de densidade\nx = np.linspace(-0.1, 1.1, 500)\nP = np.where((x &gt;= a) & (x &lt;= b), 1/(b-a), 0)\n\nplt.figure(figsize=(8,5))\nplt.plot(x, P, 'b-', lw=2, label='Densidade uniforme')\n# Média\nplt.axvline(mu, color='r', linestyle='--', label=f'Média μ={mu:.2f}')\n# Região ±σ\nplt.fill_between(x, 0, P, where=((x &gt;= mu-sigma) & (x &lt;= mu+sigma)), color='green', alpha=0.3, label=f'±σ ≈ {sigma:.2f}')\n\nplt.xlabel(\"x\")\nplt.ylabel(\"Probabilidade\")\nplt.title(\"Variância e Desvio Padrão da Distribuição Uniforme [0,1]\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Distribuição uniforme em [0,1] mostrando média (linha vermelha) e região ±σ (sombras verdes).\n\n\n\n\n\nA linha vermelha indica a média da distribuição uniforme, enquanto a região sombreada verde mostra o intervalo ±σ em torno da média. Mesmo que todos os valores tenham a mesma probabilidade, essa região fornece uma medida típica da dispersão dos valores possíveis.\n\n\nExercícios\n\nMostre que, para qualquer distribuição, \\(\\text{Var}(x) = \\langle x^2 \\rangle - \\langle x \\rangle^2\\).\nCalcule a variância e o desvio padrão de uma distribuição uniforme no intervalo \\([2,5]\\).\nPara uma variável discreta com valores \\(x_i = 1,2,3,4\\) e probabilidades \\(P_i = 1/10, 2/10, 3/10, 4/10\\), calcule o valor esperado, a variância e o desvio padrão.\nDemonstre que a variância é sempre não negativa e explique por que isso é consistente com a definição de dispersão.\nExplique qualitativamente como o desvio padrão fornece uma medida típica do “tamanho do desvio” dos valores em torno da média, usando o exemplo da distribuição uniforme em \\([0,1]\\).",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Heisenberg Uncertainty Principle"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/heisenberg-uncertainty.html#desigualdade-de-cauchy-schwarz",
    "href": "courses/introduction-to-physics/heisenberg-uncertainty.html#desigualdade-de-cauchy-schwarz",
    "title": "Introdução ao Princípio da Incerteza de Heisenberg",
    "section": "Desigualdade de Cauchy-Schwarz",
    "text": "Desigualdade de Cauchy-Schwarz\nA desigualdade de Cauchy-Schwarz é uma ferramenta fundamental em álgebra linear e mecânica quântica. Ela estabelece que, para quaisquer vetores \\(\\vec{u}\\) e \\(\\vec{v}\\) em um espaço vetorial com produto interno, temos: \\[\n|\\langle \\vec{u}, \\vec{v} \\rangle|^2 \\leq \\langle \\vec{u}, \\vec{u} \\rangle , \\langle \\vec{v}, \\vec{v} \\rangle,\n\\] ou equivalentemente, \\[\n|\\langle \\vec{u}, \\vec{v} \\rangle| \\leq |\\vec{u}| , |\\vec{v}|.\n\\]\n\nInterpretação geométrica em \\(\\mathbb{R}^2\\)\nPara vetores em \\(\\mathbb{R}^2\\), o produto interno é o produto escalar usual \\(\\vec{u} \\cdot \\vec{v} = u_1 v_1 + u_2 v_2\\). A desigualdade garante que o quadrado do produto escalar nunca é maior que o produto dos quadrados das normas: \\[\n(u_1 v_1 + u_2 v_2)^2 \\leq (u_1^2 + u_2^2)(v_1^2 + v_2^2).\n\\] Geometricamente, isso significa que o cosseno do ângulo entre os vetores sempre está entre -1 e 1: \\[\n\\cos \\theta = \\frac{\\vec{u}\\cdot \\vec{v}}{|\\vec{u}||\\vec{v}|}.\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Vetores u e v\nu = np.array([2, 1])\nv = np.array([1, 2])\n\n# Origem\norigin = np.array([0, 0])\n\nplt.figure(figsize=(6, 6))\nplt.quiver(\n    *origin, u[0], u[1], color=\"blue\", scale=1, scale_units=\"xy\", angles=\"xy\", label=\"u\"\n)\nplt.quiver(\n    *origin,\n    v[0],\n    v[1],\n    color=\"orange\",\n    scale=1,\n    scale_units=\"xy\",\n    angles=\"xy\",\n    label=\"v\",\n)\n\n# Ângulo\ntheta = np.arccos(np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v)))\nplt.text(0.3, 0.3, f\"θ ≈ {np.degrees(theta):.1f}°\", fontsize=12, color=\"purple\")\n\nplt.xlim(-0.5, 3)\nplt.ylim(-0.5, 3)\nplt.gca().set_aspect(\"equal\", adjustable=\"box\")\nplt.grid(True, alpha=0.3)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Desigualdade de Cauchy-Schwarz em $R^2$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Ilustração geométrica da desigualdade de Cauchy-Schwarz em R^2. O ângulo theta entre u e v está relacionado ao produto interno e às normas dos vetores.\n\n\n\n\n\n\n\nGeneralização\nA desigualdade de Cauchy-Schwarz vale em qualquer espaço vetorial com produto interno, inclusive o espaço de Hilbert da mecânica quântica. Para vetores \\(|\\psi\\rangle\\) e \\(|\\phi\\rangle\\): \\[\n|\\langle \\psi | \\phi \\rangle|^2 \\leq \\langle \\psi | \\psi \\rangle \\, \\langle \\phi | \\phi \\rangle.\n\\] Essa propriedade é fundamental para provar o princípio da incerteza e para várias outras desigualdades envolvendo valores esperados de operadores.\n\n\nExercícios\n\nVerifique a desigualdade de Cauchy-Schwarz para os vetores \\(\\vec{u} = (3,1)\\) e \\(\\vec{v} = (1,2)\\) em \\(\\mathbb{R}^2\\).\nPara vetores \\(\\vec{u}, \\vec{v} \\in \\mathbb{R}^3\\), mostre que \\[\n|\\vec{u}\\cdot \\vec{v}| \\leq |\\vec{u}|\\,|\\vec{v}|\n\\] usando a definição do produto interno e das normas.\nProve que a igualdade em Cauchy-Schwarz ocorre se e somente se \\(\\vec{u}\\) e \\(\\vec{v}\\) são linearmente dependentes.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Heisenberg Uncertainty Principle"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/heisenberg-uncertainty.html#princípio-da-incerteza",
    "href": "courses/introduction-to-physics/heisenberg-uncertainty.html#princípio-da-incerteza",
    "title": "Introdução ao Princípio da Incerteza de Heisenberg",
    "section": "Princípio da Incerteza",
    "text": "Princípio da Incerteza\nNa mecânica quântica, os valores de posição (\\(\\hat{x}\\)) e momento (\\(\\hat{p}\\)) não podem ser determinados simultaneamente com precisão arbitrária. Isso decorre da estrutura dos operadores quânticos e não de limitações experimentais.\nO valor esperado de uma observável \\(\\hat{A}\\) em um estado \\(|\\psi\\rangle\\) é: \\[\n\\langle A \\rangle = \\langle \\psi | \\hat{A} | \\psi \\rangle,\n\\] e a variância, que mede a dispersão em torno do valor médio, é: \\[\n\\sigma_A^2 = \\langle (\\hat{A} - \\langle A \\rangle)^2 \\rangle = \\langle \\psi | (\\hat{A} - \\langle A \\rangle)^2 | \\psi \\rangle.\n\\]\nDefinimos os estados deslocados para posição e momento: \\[\n|f\\rangle = (\\hat{x} - \\langle x \\rangle)|\\psi\\rangle, \\quad\n|g\\rangle = (\\hat{p} - \\langle p \\rangle)|\\psi\\rangle,\n\\] com \\[\n\\sigma_x^2 = \\langle f | f \\rangle, \\quad \\sigma_p^2 = \\langle g | g \\rangle.\n\\]\nAplicando a desigualdade de Cauchy–Schwarz: \\[\n\\langle f|f \\rangle , \\langle g|g \\rangle \\ge |\\langle f|g \\rangle|^2 \\implies \\sigma_x^2 \\sigma_p^2 \\ge |\\langle f|g \\rangle|^2.\n\\]\nO produto \\(\\langle f|g \\rangle\\) pode ser decomposto em comutador e anti-comutador: $$ f|g = | {-x, -p} | \n\n | [-x, -p] | , $$ onde \\({\\cdot,\\cdot}\\) é o anti-comutador e \\([\\cdot,\\cdot]\\) é o comutador.\nO anti-comutador é hermitiano e, portanto, produz um número real.\nO comutador é anti-hermitiano e, portanto, produz um número imaginário puro.\n\nSe considerarmos apenas a contribuição do comutador, que fornece o limite fundamental, temos: \\[\n|\\langle f|g \\rangle|^2 \\ge \\left|\\frac{1}{2}\\langle [\\hat{x},\\hat{p}] \\rangle\\right|^2 = \\frac{\\hbar^2}{4},\n\\] o que leva à forma usual da desigualdade de Heisenberg: \\[\n\\sigma_x , \\sigma_p \\ge \\frac{\\hbar}{2}.\n\\]\nA interpretação é clara: quanto mais preciso conhecemos a posição ((\\(\\sigma_x\\)) pequeno), menos preciso é o momento ((\\(\\sigma_p\\)) grande), e vice-versa. O termo do anti-comutador pode aumentar a desigualdade, mas não diminui o limite fundamental estabelecido pelo comutador.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Valores ilustrativos\nanticom_real = 0.3  # Parte real (anti-comutador)\ncomm_imag = 0.5  # Parte imaginária (comutador, |hbar/2|)\n\nplt.figure(figsize=(6, 6))\nplt.axhline(0, color=\"black\", linewidth=0.5)\nplt.axvline(0, color=\"black\", linewidth=0.5)\n\n# Plotando o vetor ⟨f|g⟩\nplt.quiver(\n    0,\n    0,\n    anticom_real,\n    comm_imag,\n    angles=\"xy\",\n    scale_units=\"xy\",\n    scale=1,\n    color=\"purple\",\n    label=\"⟨f|g⟩\",\n)\n\n# Componentes real e imaginária\nplt.plot([0, anticom_real], [0, 0], \"r--\", label=\"Parte real (anti-comutador)\")\nplt.plot(\n    [anticom_real, anticom_real],\n    [0, comm_imag],\n    \"b--\",\n    label=\"Parte imaginária (comutador)\",\n)\n\nplt.xlim(-0.1, 0.6)\nplt.ylim(-0.1, 0.6)\nplt.xlabel(\"Parte real\")\nplt.ylabel(\"Parte imaginária\")\nplt.title(\"Visualização de ⟨f|g⟩ no plano complexo\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.gca().set_aspect(\"equal\", adjustable=\"box\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Representação de ⟨f|g⟩ no plano complexo. A parte real (anti-comutador) e a parte imaginária (comutador) ilustram a origem da desigualdade de Heisenberg.\n\n\n\n\n\n\nO vetor roxo representa (\\(\\langle f|g \\rangle\\)) como um número complexo.\nA parte real (vermelha tracejada) corresponde ao anti-comutador, que pode variar dependendo do estado, mas não determina o limite fundamental.\nA parte imaginária (azul tracejada) corresponde ao comutador, que define o limite mínimo (\\(\\hbar/2\\)) da desigualdade de Heisenberg. Note que essa parte está sempre presente, independentemente do estado.\n\n\nExercícios\n\nPara operadores hermitianos \\(\\hat{A}\\) e \\(\\hat{B}\\) quaisquer, defina \\[\n|f\\rangle = (\\hat{A}-\\langle A\\rangle)|\\psi\\rangle, \\quad |g\\rangle = (\\hat{B}-\\langle B\\rangle)|\\psi\\rangle.\n\\] Mostre que a desigualdade de Cauchy-Schwarz leva a \\[\n\\sigma_A^2 \\sigma_B^2 \\ge |\\langle f|g \\rangle|^2.\n\\]\nExplique por que o anti-comutador não diminui o limite mínimo da desigualdade de Heisenberg.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Heisenberg Uncertainty Principle"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/determinism-statistics.html",
    "href": "courses/introduction-to-physics/determinism-statistics.html",
    "title": "Determinismo e Estatística",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Determinism and Statistics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/determinism-statistics.html#a-linguagem-da-incerteza-o-que-é-estatística",
    "href": "courses/introduction-to-physics/determinism-statistics.html#a-linguagem-da-incerteza-o-que-é-estatística",
    "title": "Determinismo e Estatística",
    "section": "A Linguagem da Incerteza: O que é Estatística?",
    "text": "A Linguagem da Incerteza: O que é Estatística?\nSe a matemática do contínuo (cálculo, geometria) é a linguagem natural do determinismo, descrevendo trajetórias exatas e forças precisas, a estatística é a linguagem que desenvolvemos para conversar com a incerteza.\nNão usamos estatística apenas porque nossos instrumentos são imperfeitos. Em um nível fundamental, a natureza parece se recusar a dar respostas definitivas para certas perguntas. A pergunta “onde exatamente está o elétron agora?” pode não ter uma resposta. Em vez disso, a física nos oferece uma resposta diferente: “qual a probabilidade de encontrá-lo aqui ou ali se eu procurar?”.\nA estatística, então, não é um paliativo para nossa ignorância; é o vocabulário necessário para descrever um aspecto intrínseco da realidade.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Determinism and Statistics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/determinism-statistics.html#dois-dialetos-da-linguagem-estatística-frequentista-vs.-bayesiana",
    "href": "courses/introduction-to-physics/determinism-statistics.html#dois-dialetos-da-linguagem-estatística-frequentista-vs.-bayesiana",
    "title": "Determinismo e Estatística",
    "section": "Dois Dialetos da Linguagem Estatística: Frequentista vs. Bayesiana",
    "text": "Dois Dialetos da Linguagem Estatística: Frequentista vs. Bayesiana\nAo adotar a linguagem estatística, encontramos dois dialetos principais, duas formas de interpretar o que a palavra probabilidade realmente significa.\n\nA Visão Frequentista: A Probabilidade como Frequência de Ocorrência\n\nA Ideia Central: A probabilidade de um evento é a sua frequência relativa de ocorrência em um grande número de tentativas idênticas.\nComo Funciona: Imagine lançar uma moeda. A probabilidade de dar “cara” é \\(P = 0.5\\) porque, após lançarmos a moeda \\(N\\) vezes (com \\(N\\) muito grande), esperamos encontrar aproximadamente \\(N/2\\) caras. \\(P = (\\text{Número de sucessos}) / (\\text{Número total de tentativas})\\).\nO Conceito de Limite: Crucialmente, a probabilidade é definida como o limite da frequência relativa quando o número de tentativas tende ao infinito: \\(P(A) = \\lim_{n \\to \\infty} \\frac{n_A}{n}\\). Esta é uma idealização matemática: algo que supomos existir mas nunca podemos alcançar na prática. Nunca podemos realizar infinitas tentativas, mas observamos que as frequências relativas tendem a se estabilizar around um valor específico à medida que coletamos mais dados. Esta estabilidade das frequências é o que nos permite falar em “probabilidade” no sentido frequentista.\nO Foco: Está inteiramente nos dados coletados. É objetiva e baseada puramente na repetição.\nLimitação: Não é fácil aplicar a eventos únicos e não repetíveis. Qual é a probabilidade frequentista de um candidato específico ganhar uma eleição? A eleição só acontece uma vez. O frequentista diria que isso não é uma probabilidade, mas sim uma certeza que nós desconhecemos.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Configuração do experimento\nnp.random.seed(42)  # Para reproducibilidade\nn_lancamentos = 10000\nmoeda_justa = [0.5, 0.5]  # Probabilidades: [0] = cara, [1] = coroa\n\n# Simulação dos lançamentos\nresultados = np.random.choice([0, 1], size=n_lancamentos, p=moeda_justa)\nfrequencias_relativas = np.cumsum(resultados) / np.arange(1, n_lancamentos + 1)\n\n# Criação da figura\nplt.figure(figsize=(10, 6))\nplt.plot(\n    np.arange(1, n_lancamentos + 1),\n    frequencias_relativas,\n    alpha=0.7,\n    linewidth=1,\n    label=\"Frequência relativa acumulada\",\n)\nplt.axhline(y=0.5, color=\"red\", linestyle=\"--\", label=\"Probabilidade teórica (0.5)\")\n\n# Detalhes estéticos\nplt.xscale(\"log\")  # Escala log para melhor visualização do comportamento inicial\nplt.xlabel(\"Número de lançamentos (escala logarítmica)\")\nplt.ylabel(\"Frequência relativa de caras\")\nplt.title(\"Convergência da frequência relativa para a probabilidade teórica\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Convergência da frequência relativa de ‘caras’ para o valor teórico de 0.5\n\n\n\n\n\n\n\nA Visão Bayesiana: A Probabilidade como Grau de Crença\n\nA Ideia Central: A probabilidade é uma medida do grau de crença ou grau de plausibilidade que temos em uma hipótese, com base no conhecimento disponível.\nComo Funciona: Começamos com uma probabilidade a priori: uma estimativa inicial de nossa crença. Então, coletamos dados. Por fim, usamos o Teorema de Bayes para atualizar nossa crença, calculando uma nova probabilidade a posteriori.\nTeorema de Bayes: A ferramenta fundamental para atualizar crenças é dada por:\n\\[\nP(H|D) = \\frac{P(D|H) \\cdot P(H)}{P(D)}\n\\]\nOnde:\n\n\\(P(H|D)\\): Probabilidade a posteriori da hipótese H dado os dados D\n\\(P(D|H)\\): Verossimilhança - probabilidade dos dados D se a hipótese H for verdadeira\n\\(P(H)\\): Probabilidade a priori da hipótese H\n\\(P(D)\\): Probabilidade total dos dados (constante de normalização)\n\nO Foco: Está na atualização do conhecimento. É subjetiva no sentido de que começa com um estado de conhecimento prévio, mas é completamente objetiva na forma como esse conhecimento deve ser atualizado diante de novos dados.\nVantagem: É poderosa para lidar com eventos únicos, incorporar conhecimento prévio e aprender continuamente.\n\n\nExemplo Detalhado: Teste de Doença Rara\nContexto: Uma doença afeta 1% da população. Um teste tem 99% de precisão.\nDefinindo as variáveis:\n\n\\(H\\): Hipótese de ter a doença\n\\(\\neg H\\): Hipótese de não ter a doença\n\n\\(D\\): Dado do teste positivo\n\nProbabilidades a priori:\n\n\\(P(H) = 0.01\\) (1% da população tem a doença)\n\\(P(\\neg H) = 0.99\\) (99% não tem)\n\nVerossimilhanças (acurácia do teste):\n\n\\(P(D|H) = 0.99\\) (99% de chance de positivo se doente)\n\\(P(D|\\neg H) = 0.01\\) (1% de chance de falso positivo)\n\nProbabilidade total dos dados (\\(P(D)\\)): \\[P(D) = P(D|H) \\cdot P(H) + P(D|\\neg H) \\cdot P(\\neg H)\\] \\[P(D) = (0.99 \\times 0.01) + (0.01 \\times 0.99) = 0.0099 + 0.0099 = 0.0198\\]\nAplicando o Teorema de Bayes: \\[P(H|D) = \\frac{P(D|H) \\cdot P(H)}{P(D)} = \\frac{0.99 \\times 0.01}{0.0198} = \\frac{0.0099}{0.0198} = 0.5\\]\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Dados do problema\nP_H = 0.01  # Probabilidade a priori de ter a doença\nP_negH = 0.99  # Probabilidade a priori de não ter a doença\nP_D_dado_H = 0.99  # Probabilidade de teste positivo se estiver doente\nP_D_dado_negH = 0.01  # Probabilidade de teste positivo se não estiver doente\n\n# Calculando a probabilidade total P(D)\nP_D = (P_D_dado_H * P_H) + (P_D_dado_negH * P_negH)\n\n# Calculando a probabilidade a posteriori P(H|D)\nP_H_dado_D = (P_D_dado_H * P_H) / P_D\n\n# Criando visualização\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Gráfico 1: Probabilidades a priori\nlabels_priori = [\"Tem a doença\", \"Não tem a doença\"]\nvalues_priori = [P_H, P_negH]\ncolors_priori = [\"lightcoral\", \"lightblue\"]\n\nax1.pie(\n    values_priori,\n    labels=labels_priori,\n    autopct=\"%1.1f%%\",\n    colors=colors_priori,\n    startangle=90,\n)\nax1.set_title(\"Probabilidades a Priori\\n(antes do teste)\")\n\n# Gráfico 2: Probabilidades a posteriori\nlabels_posteriori = [\n    \"Tem a doença\\n(dado positivo)\",\n    \"Não tem a doença\\n(dado positivo)\",\n]\nvalues_posteriori = [P_H_dado_D, 1 - P_H_dado_D]\n\nax2.pie(\n    values_posteriori,\n    labels=labels_posteriori,\n    autopct=\"%1.1f%%\",\n    colors=colors_priori,\n    startangle=90,\n)\nax2.set_title(\"Probabilidades a Posteriori\\n(após teste positivo)\")\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Probabilidade a posteriori P(H|D) = {P_H_dado_D:.3f} ({P_H_dado_D*100:.1f}%)\")\n\n\n\n\n\n\n\n\nFigure 2: Análise Bayesiana do teste de doença\n\n\n\n\n\nProbabilidade a posteriori P(H|D) = 0.500 (50.0%)\n\n\nInterpretação do Resultado: Apesar do teste ter 99% de precisão, a probabilidade de realmente ter a doença após um teste positivo é de apenas 50%. Isso ocorre porque a doença é muito rara (apenas 1% da população), então mesmo com uma baixa taxa de falsos positivos (1%), o número absoluto de falsos positivos é similar ao número de verdadeiros positivos.\nA visão Bayesiana nos ensina que dados não falam por si só - eles só ganham significado quando interpretados à luz do conhecimento prévio existente.\n\n\n\nExercícios\n\nExplique em palavras simples a diferença entre a interpretação frequentista e bayesiana de probabilidade.\nConsidere o lançamento de uma moeda justa 1000 vezes. Calcule a frequência relativa de “caras” se forem observadas 510 ocorrências. Compare com a probabilidade teórica.\nUma doença afeta \\(2\\%\\) da população. Um teste tem \\(95\\%\\) de acerto tanto para positivos quanto para negativos. Calcule a probabilidade de realmente estar doente dado que o teste deu positivo, usando o Teorema de Bayes.\nSuponha que você lance uma moeda 10 vezes e observe apenas uma vez cara. Qual seria a interpretação frequentista e bayesiana da probabilidade de sair cara?\nDiscuta por que o enfoque bayesiano é mais adequado para eventos únicos, como a eleição de um candidato ou previsão de desastres naturais.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Determinism and Statistics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/determinism-statistics.html#distribuição-de-probabilidade-vs-probabilidade-a-importância-da-medida",
    "href": "courses/introduction-to-physics/determinism-statistics.html#distribuição-de-probabilidade-vs-probabilidade-a-importância-da-medida",
    "title": "Determinismo e Estatística",
    "section": "Distribuição de Probabilidade vs Probabilidade: A Importância da Medida",
    "text": "Distribuição de Probabilidade vs Probabilidade: A Importância da Medida\nAté agora, trabalhamos com espaços de estados discretos (cara/coroa, doente/saudável). Mas na física, frequentemente lidamos com espaços contínuos, como a posição de uma partícula ao longo de uma linha.\nAqui surge uma diferença fundamental: em espaços contínuos, a probabilidade de qualquer estado específico é zero.\n\nPor que probabilidade zero não significa impossibilidade?\nConsidere uma partícula que pode estar em qualquer ponto do intervalo [0, 1]. Se atribuíssemos probabilidades iguais a cada ponto, teríamos um paradoxo:\n\nProbabilidade de cada ponto: \\(P(x) = \\frac{1}{\\infty} = 0\\)\nMas a probabilidade total no intervalo: \\(\\sum_{x=0}^1 P(x) = 0 + 0 + 0 + \\cdots = 0\\)\n\nIsso viola o axioma fundamental de que a probabilidade total deve ser 1!\n\n\nA Solução: Densidade de Probabilidade\nA solução matemática é trabalhar não com probabilidades de pontos, mas com probabilidades de intervalos. Introduzimos uma função densidade de probabilidade \\(\\rho(x)\\) tal que:\n\\[\nP(a \\leq x \\leq b) = \\int_a^b \\rho(x)  \\dd x.\n\\]\nPropriedades fundamentais:\n\n\\(\\rho(x) \\geq 0\\) para todo \\(x\\)\n\\(\\int_{-\\infty}^{\\infty} \\rho(x)  \\dd x = 1\\)\n\nA probabilidade de encontrar a partícula exatamente no ponto \\(x\\) é: \\[\nP(x = x_0) = \\int_{x_0}^{x_0} \\rho(x)  \\dd x = 0.\n\\]\nMas a densidade \\(\\rho(x_0)\\) nos diz o quão “provável” é a vizinhança de \\(x_0\\).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Configuração\nnp.random.seed(42)\nx = np.linspace(-4, 4, 1000)\ndx = x[1] - x[0]\n\n# Duas distribuições normais com diferentes variâncias\nrho1 = norm.pdf(x, 0, 0.5)  # Distribuição mais concentrada\nrho2 = norm.pdf(x, 0, 1.0)  # Distribuição mais espalhada\n\n# Cálculo de probabilidades em intervalos\nintervalo = [-1, 1]\nprob1 = np.sum(rho1[(x &gt;= intervalo[0]) & (x &lt;= intervalo[1])]) * dx\nprob2 = np.sum(rho2[(x &gt;= intervalo[0]) & (x &lt;= intervalo[1])]) * dx\n\n# Visualização\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Gráfico das densidades\nax1.plot(x, rho1, \"b-\", linewidth=2, label=\"ρ(x) concentrada (\\sigma=0.5)\")\nax1.plot(x, rho2, \"r-\", linewidth=2, label=\"ρ(x) espalhada (\\sigma=1.0)\")\nax1.fill_between(\n    x[(x &gt;= intervalo[0]) & (x &lt;= intervalo[1])],\n    rho1[(x &gt;= intervalo[0]) & (x &lt;= intervalo[1])],\n    alpha=0.3,\n    color=\"blue\",\n)\nax1.fill_between(\n    x[(x &gt;= intervalo[0]) & (x &lt;= intervalo[1])],\n    rho2[(x &gt;= intervalo[0]) & (x &lt;= intervalo[1])],\n    alpha=0.3,\n    color=\"red\",\n)\nax1.set_xlabel(\"Posição (x)\")\nax1.set_ylabel(\"Densidade de probabilidade ρ(x)\")\nax1.set_title(\"Densidades de Probabilidade\")\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Gráfico de barras das probabilidades\nlabels = [f\"P({intervalo[0]} ≤ x ≤ {intervalo[1]})\"]\nwidth = 0.35\nx_pos = np.arange(len(labels))\n\nax2.bar(\n    x_pos - width / 2,\n    [prob1],\n    width,\n    label=\"Distribuição concentrada\",\n    color=\"blue\",\n    alpha=0.7,\n)\nax2.bar(\n    x_pos + width / 2,\n    [prob2],\n    width,\n    label=\"Distribuição espalhada\",\n    color=\"red\",\n    alpha=0.7,\n)\nax2.set_ylabel(\"Probabilidade\")\nax2.set_title(\"Probabilidades no Intervalo [-1, 1]\")\nax2.set_xticks(x_pos)\nax2.set_xticklabels(labels)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Probabilidade no intervalo [-1, 1]:\")\nprint(f\"Distribuição concentrada: {prob1:.3f}\")\nprint(f\"Distribuição espalhada: {prob2:.3f}\")\n\n\n&lt;&gt;:23: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:24: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:23: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:24: SyntaxWarning: invalid escape sequence '\\s'\n/tmp/ipykernel_2692/3023404317.py:23: SyntaxWarning: invalid escape sequence '\\s'\n  ax1.plot(x, rho1, \"b-\", linewidth=2, label=\"ρ(x) concentrada (\\sigma=0.5)\")\n/tmp/ipykernel_2692/3023404317.py:24: SyntaxWarning: invalid escape sequence '\\s'\n  ax1.plot(x, rho2, \"r-\", linewidth=2, label=\"ρ(x) espalhada (\\sigma=1.0)\")\n\n\n\n\n\n\n\n\nFigure 3: Ilustração da diferença entre probabilidade e densidade de probabilidade\n\n\n\n\n\nProbabilidade no intervalo [-1, 1]:\nDistribuição concentrada: 0.955\nDistribuição espalhada: 0.683\n\n\n\n\nInterpretação Física Crucial\n\n\\(\\rho(x)\\) não é uma probabilidade: É uma densidade. Suas unidades são [probabilidade/comprimento].\nApenas integrais de \\(\\rho(x)\\) têm significado probabilístico: \\(P(a \\leq x \\leq b) = \\int_a^b \\rho(x)  \\dd x\\).\nValores relativos importam: \\(\\rho(x_2)&gt;\\rho(x_1)\\) significa que intervalos infinitesimais around \\(x_2\\) são mais prováveis que around \\(x_1\\).\n\nEsta transição conceitual é fundamental para a mecânica quântica, onde a função de onda \\(\\psi(x)\\) está relacionada à densidade de probabilidade por: \\[\n\\rho(x) = |\\psi(x)|^2.\n\\] Apesar de \\(\\psi(x)\\) poder ter valores em qualquer ponto, apenas integrais de \\(|\\psi(x)|^2\\) sobre intervalos finitos têm interpretação probabilística direta.\n\n\nExercícios\n\nExplique em palavras a diferença entre probabilidade em um espaço discreto e densidade de probabilidade em um espaço contínuo.\nConsidere uma distribuição normal \\(N(0,1)\\). Calcule a probabilidade de a variável estar no intervalo \\([-1,1]\\) usando a integral da densidade. (avançado)\nDuas distribuições normais têm médias iguais a 0, mas desvios-padrão diferentes: \\(\\sigma_1 = 0.5\\) e \\(\\sigma_2 = 1.0\\). Compare as probabilidades de estar no intervalo \\([-1,1]\\). Explique qualitativamente por que elas diferem.\nPor que a probabilidade de a partícula estar exatamente em \\(x_0\\) é zero, mesmo que \\(\\rho(x_0)\\) seja grande?\nEm mecânica quântica, a função de onda \\(\\psi(x)\\) define a densidade de probabilidade \\(\\rho(x) = |\\psi(x)|^2\\). Discuta a importância de integrar \\(\\rho(x)\\) sobre um intervalo finito em vez de olhar para um ponto específico.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Determinism and Statistics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/determinism-statistics.html#do-estado-determinístico-à-descrição-probabilística",
    "href": "courses/introduction-to-physics/determinism-statistics.html#do-estado-determinístico-à-descrição-probabilística",
    "title": "Determinismo e Estatística",
    "section": "Do Estado Determinístico à Descrição Probabilística",
    "text": "Do Estado Determinístico à Descrição Probabilística\n\nO Espaço de Estados e o Determinismo Clássico\nNa mecânica newtoniana, o estado completo de uma partícula em 1D é descrito por duas variáveis: posição \\(x\\) e velocidade \\(v\\) (ou momento \\(p = mv\\)). Juntas, elas definem um ponto no espaço de fase \\((x, p)\\).\nA evolução temporal é governada pelas leis de Newton: \\[\n\\frac{\\dd x}{\\dd t} = v, \\quad \\frac{dp}{\\dd t} = F(x)\n\\]\nSe conhecermos exatamente o estado inicial \\((x_0, p_0)\\) no instante \\(t_0\\), podemos (em princípio) determinar univocamente o estado \\((x(t), p(t))\\) em qualquer instante futuro ou passado. Esta é a essência do determinismo laplaciano.\n\n\nA Quebra Prática do Determinismo\nNa prática, no entanto, nunca conhecemos o estado inicial com precisão infinita:\n\nLimitações instrumentais: Medidas sempre têm incertezas experimentais\nPreparação imperfeita: Sistemas reais nunca estão perfeitamente isolados\nComplexidade: Sistemas com muitas partículas são praticamente intratáveis\n\nMais fundamentalmente, na mecânica quântica, o próprio princípio da incerteza de Heisenberg proíbe o conhecimento simultâneo e exato de \\(x\\) e \\(p\\).\n\n\nA Necessidade da Descrição Probabilística\nDiante dessa impossibilidade prática (e fundamental) de determinar o estado exato, adotamos uma descrição probabilística:\nEm vez de um ponto \\((x_0, p_0)\\) no espaço de fase, temos uma distribuição de probabilidade \\(\\rho(x, p, t_0)\\) que representa nosso conhecimento sobre o estado do sistema.\nInterpretação: \\(\\rho(x, p, t_0)  \\dd x \\dd p\\) representa a probabilidade de encontrar o sistema em um elemento infinitesimal do espaço de fase around do ponto \\((x, p)\\) no instante \\(t_0\\).\n\n\nEvolução Temporal da Distribuição\nA grande vantagem desta abordagem é que podemos evoluir temporalmente toda a distribuição:\nDada \\(\\rho(x, p, t_0)\\), a distribuição em qualquer instante posterior \\(t\\) é determinada pela dinâmica subjacente: \\[\n\\rho(x, p, t) = \\rho(x_0(x,p,t), p_0(x,p,t), t_0)\n\\] onde \\((x_0(x,p,t), p_0(x,p,t))\\) é o estado inicial que evolui para \\((x, p)\\) no tempo \\(t\\) (seguindo as leis de Newton).\n\n\nExercícios\n\nExplique por que, mesmo na mecânica clássica, é necessário usar uma descrição probabilística na prática.\nPara uma partícula em 1D com distribuição de probabilidade inicial \\(\\rho(x,p,0)\\), descreva qualitativamente como essa distribuição evolui ao longo do tempo.\nConsidere uma distribuição gaussiana inicial em \\(x\\) com momento \\(p\\) fixo. Escreva a expressão da distribuição \\(\\rho(x,p,t)\\) em função do tempo, assumindo movimento livre (sem forças).\nPor que conhecer apenas o estado inicial médio \\((\\langle x_0 \\rangle, \\langle p_0 \\rangle)\\) não é suficiente para prever com precisão o comportamento de um sistema probabilístico?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Determinism and Statistics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/determinism-statistics.html#exemplo-concreto-cinemática-1d-com-incertezas",
    "href": "courses/introduction-to-physics/determinism-statistics.html#exemplo-concreto-cinemática-1d-com-incertezas",
    "title": "Determinismo e Estatística",
    "section": "Exemplo Concreto: Cinemática 1D com Incertezas",
    "text": "Exemplo Concreto: Cinemática 1D com Incertezas\n\nCaso 1: Movimento Retilíneo Uniforme (MRU)\nSuponha uma partícula com velocidade constante \\(v\\), mas com posição inicial incerta:\n\nEquação de movimento: \\(x(t) = x_0 + v t\\)\nIncerteza inicial: \\(x_0 = 1.0 \\pm 0.2\\) m (distribuição uniforme)\nVelocidade: \\(v = 2.0\\) m/s (conhecida exatamente)\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parâmetros\nv = 2.0  # m/s\nx0_min, x0_max = 0.8, 1.2  # intervalo inicial de posição\nt = np.linspace(0, 2, 100)  # 0 a 2 segundos\n\n# Evolução da incerteza\nx_min = x0_min + v * t\nx_max = x0_max + v * t\n\nplt.figure(figsize=(10, 6))\nplt.fill_between(t, x_min, x_max, alpha=0.3, label=\"Zona de incerteza\")\nplt.plot(t, (x0_min + x0_max) / 2 + v * t, \"r-\", linewidth=2, label=\"Trajetória média\")\nplt.xlabel(\"Tempo (s)\")\nplt.ylabel(\"Posição (m)\")\nplt.title(\"Evolução da Incerteza no Movimento Retilíneo Uniforme\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Evolução da incerteza posicional no MRU\n\n\n\n\n\nInterpretação: A incerteza inicial de \\(\\pm0.2\\)m se propaga sem amplificação: a largura da distribuição permanece constante no tempo.\n\n\nCaso 2: Movimento Uniformemente Acelerado (MUA)\nAgora considere queda livre com aceleração constante, mas com incertezas tanto na posição quanto na velocidade iniciais:\n\nEquação de movimento: \\(x(t) = x_0 + v_0 t + \\frac{1}{2} a t^2\\)\nIncertezas: \\(x_0 = 0 \\pm 0.1\\) m, \\(v_0 = 0 \\pm 0.2\\) m/s\nAceleração: \\(a = -9.8\\) m/s²\n\n\n\nCode\n# Parâmetros\na = -9.8  # m/s^2\nx0_sigma = 0.1  # incerteza inicial na posição\nv0_sigma = 0.2  # incerteza inicial na velocidade\n\n# Tempo\nt = np.linspace(0, 0.2, 100)\n\n# Evolução da incerteza total (propagação de erros)\nsigma_x = np.sqrt(x0_sigma**2 + (v0_sigma * t) ** 2 + (0.5 * a * t**2) ** 2)\n\nplt.figure(figsize=(10, 6))\nplt.plot(\n    t, 0.5 * a * t**2, \"b-\", linewidth=2, label=\"Trajetória nominal ($x_0=0, v_0=0$)\"\n)\nplt.fill_between(\n    t,\n    0.5 * a * t**2 - sigma_x,\n    0.5 * a * t**2 + sigma_x,\n    alpha=0.3,\n    color=\"red\",\n    label=\"Zona de incerteza ($\\pm\\sigma$)\",\n)\nplt.xlabel(\"Tempo (s)\")\nplt.ylabel(\"Posição (m)\")\nplt.title(\"Evolução da Incerteza no Movimento Uniformemente Acelerado\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Incerteza inicial: $\\sigma_x$ = {x0_sigma:.3f} m\")\nprint(f\"Incerteza final (t=1s): $\\sigma_x$ = {sigma_x[-1]:.3f} m\")\n\n\n&lt;&gt;:22: SyntaxWarning: invalid escape sequence '\\p'\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:32: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:22: SyntaxWarning: invalid escape sequence '\\p'\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:32: SyntaxWarning: invalid escape sequence '\\s'\n/tmp/ipykernel_2692/3749421272.py:22: SyntaxWarning: invalid escape sequence '\\p'\n  label=\"Zona de incerteza ($\\pm\\sigma$)\",\n/tmp/ipykernel_2692/3749421272.py:31: SyntaxWarning: invalid escape sequence '\\s'\n  print(f\"Incerteza inicial: $\\sigma_x$ = {x0_sigma:.3f} m\")\n/tmp/ipykernel_2692/3749421272.py:32: SyntaxWarning: invalid escape sequence '\\s'\n  print(f\"Incerteza final (t=1s): $\\sigma_x$ = {sigma_x[-1]:.3f} m\")\n\n\n\n\n\n\n\n\nFigure 5: Evolução da incerteza no movimento uniformemente acelerado\n\n\n\n\n\nIncerteza inicial: $\\sigma_x$ = 0.100 m\nIncerteza final (t=1s): $\\sigma_x$ = 0.224 m\n\n\nInterpretação: A incerteza cresce rapidamente com o tempo devido à contribuição da incerteza inicial na velocidade, que é amplificada pelo termo quadrático.\n\n\nLição Fundamental\nEstes exemplos mostram que:\n\nMesmo com leis determinísticas, incertezas iniciais se propagam\nA forma da propagação depende da dinâmica do sistema\nSistemas com não-linearidades (como o termo t²) podem amplificar rapidamente incertezas iniciais\n\nEsta é a ponte entre o determinismo das equações de Newton e a necessidade de descrições estatísticas na prática experimental!\n\n\nExemplo Concreto: Oscilador Harmônico\nNeste exemplo, vamos estudar o comportamento de um oscilador harmônico com incertezas iniciais na posição e na velocidade.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\nfrom scipy.stats import multivariate_normal\n\n\n# Sistema físico: oscilador harmônico\ndef oscilador_harmonico(estado, t):\n    x, p = estado\n    k = 1.0  # constante elástica\n    m = 1.0  # massa\n    dxdt = p / m\n    dpdt = -k * x\n    return [dxdt, dpdt]\n\n\n# Condição inicial (distribuição)\nx0, p0 = 1.0, 0.0  # centro da distribuição\nsigma_x, sigma_p = 0.2, 0.3  # incertezas iniciais\n\n# Tempos para evolução\nt = np.linspace(0, 2 * np.pi, 5)  # Um período completo\n\n# Criar grid no espaço de fase\nx = np.linspace(-2, 2, 100)\np = np.linspace(-2, 2, 100)\nX, P = np.meshgrid(x, p)\n\n# Visualização\nfig, axes = plt.subplots(1, len(t), figsize=(14, 4))\nfig.suptitle(\"Evolução Temporal de uma Distribuição no Espaço de Fase\", fontsize=14)\n\nfor i, time in enumerate(t):\n    # Para cada tempo, evoluir a distribuição\n    # (Aproximação: para sistema linear, a distribuição gaussiana permanece gaussiana)\n    # Calcular a evolução do centro da distribuição\n    estado_evoluido = odeint(oscilador_harmonico, [x0, p0], [0, time])[-1]\n    x_evol, p_evol = estado_evoluido\n\n    # Distribuição no tempo t (aproximação)\n    pos = np.dstack((X, P))\n    cov = [[sigma_x**2, 0], [0, sigma_p**2]]  # Matriz de covariância\n    rho = multivariate_normal([x_evol, p_evol], cov).pdf(pos)\n\n    # Plot\n    ax = axes[i]\n    contour = ax.contourf(X, P, rho, levels=20, cmap=\"viridis\")\n    ax.set_xlabel(\"Posição (x)\")\n    if i == 0:\n        ax.set_ylabel(\"Momento (p)\")\n    ax.set_title(f\"t = {time:.2f}\")\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim(-2, 2)\n    ax.set_ylim(-2, 2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Evolução temporal de uma distribuição no espaço de fase\n\n\n\n\n\n\n\nImplicações Fundamentais\n\nDeterminismo vs. Previsibilidade: Embora a dinâmica subjacente seja determinística, nossa capacidade de previsão é limitada pela incerteza inicial.\nEspalhamento da Informação: Em sistemas caóticos, pequenas incertezas iniciais crescem exponencialmente, limitando drasticamente o horizonte de previsibilidade.\nConexão com Mecânica Quântica: Na MQ, a função de onda \\(\\psi(x)\\) vive em um espaço de Hilbert, mas a densidade de probabilidade \\(|\\psi(x)|^2\\) representa nossa informação sobre o sistema, de forma análoga à \\(\\rho(x, p)\\) na física clássica.\n\nEsta transição do ponto no espaço de fase para distribuições de probabilidade marca a passagem fundamental do determinismo ideal para a descrição estatística prática que permeia toda a física moderna.\n\n\nExercícios\n\nNo MRU do exemplo, explique por que a largura da zona de incerteza permanece constante ao longo do tempo, enquanto no MUA ela cresce.\nPara o MUA, derive a expressão da incerteza \\(\\sigma_x(t)\\) considerando incertezas iniciais em \\(x_0\\) e \\(v_0\\) e compare com a aproximação mostrada no gráfico.\nNo oscilador harmônico, explique por que uma distribuição gaussiana inicial permanece aproximadamente gaussiana ao longo do tempo.\nDiscuta qualitativamente como o espalhamento da distribuição no espaço de fase muda se a constante elástica \\(k\\) ou a massa \\(m\\) forem alteradas.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Determinism and Statistics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/determinism-statistics.html#a-mecânica-quântica-e-a-natureza-intrinsecamente-probabilística-da-realidade",
    "href": "courses/introduction-to-physics/determinism-statistics.html#a-mecânica-quântica-e-a-natureza-intrinsecamente-probabilística-da-realidade",
    "title": "Determinismo e Estatística",
    "section": "A Mecânica Quântica e a Natureza Intrinsecamente Probabilística da Realidade",
    "text": "A Mecânica Quântica e a Natureza Intrinsecamente Probabilística da Realidade\nTudo que discutimos até agora pode ser visto como uma “correção prática” ao determinismo: não sabemos medir perfeitamente, então usamos estatística. Mas a Mecânica Quântica (MQ) traz uma revolução muito mais profunda.\n\nA Quebra Fundamental do Determinismo\nNa MQ, mesmo em condições ideais, com instrumentos perfeitos:\n\nO Princípio da Incerteza de Heisenberg proíbe o conhecimento simultâneo e exato de posição e momento: \\[\\sigma_x \\sigma_p \\geq \\frac{\\hbar}{2}\\] Isso não é uma limitação tecnológica; é uma propriedade fundamental da natureza.\nO resultado de uma medida individual é fundamentalmente imprevisível. Podemos saber tudo que é possível saber sobre um sistema (sua função de onda \\(\\psi(x)\\)) e ainda assim só poder prever probabilidades para o resultado de uma medida.\n\n\n\nA Função de Onda: Uma Entidade Probabilística\nO estado de um sistema quântico é descrito pela função de onda \\(\\psi(x)\\), uma entidade matemática complexa. A interpretação física foi proposta por Max Born: \\[|\\psi(x)|^2 dx = \\text{Probabilidade de encontrar a partícula entre } x \\text{ e } x+dx\\]\nA evolução temporal de \\(\\psi(x)\\) é governada pela Equação de Schrödinger, uma equação diferencial perfeitamente determinística. Apesar disso, os resultados de medições individuais são fundamentalmente probabilísticos.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parâmetros do pacote de onda gaussiano\nx0 = 0.0  # Centro do pacote\nsigma = 0.5  # Largura\nk0 = 5.0  # Número de onda médio\n\n# Espaço de configuração\nx = np.linspace(-2, 2, 1000)\n\n# Função de onda: pacote gaussiano\npsi = np.exp(-((x - x0) ** 2) / (2 * sigma**2)) * np.exp(1j * k0 * x)\npsi_real = np.real(psi)\npsi_imag = np.imag(psi)\npsi_mod_sq = np.abs(psi) ** 2  # Densidade de probabilidade\n\n# Normalização para melhor visualização\npsi_real /= np.max(np.abs(psi_real))\npsi_imag /= np.max(np.abs(psi_imag))\npsi_mod_sq /= np.max(psi_mod_sq)\n\n# Plot\nplt.figure(figsize=(10, 6))\n\nplt.plot(x, psi_real, \"b-\", linewidth=2, label=\"Parte real [Re(ψ(x))]\")\nplt.plot(x, psi_imag, \"r-\", linewidth=2, label=\"Parte imaginária [Im(ψ(x))]\")\nplt.plot(x, psi_mod_sq, \"k-\", linewidth=2, label=\"Densidade de probabilidade [|ψ(x)|²]\")\n\nplt.xlabel(\"Posição (x)\")\nplt.ylabel(\"Amplitude (normalizada)\")\nplt.title(\"Função de Onda Quântica e sua Densidade de Probabilidade\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Partes real e imaginária, e módulo ao quadrado de um pacote de onda gaussiano\n\n\n\n\n\n\n\nInterpretação Física\n\nPartes Real e Imaginária: Componentes matemáticas da função de onda. Não são diretamente mensuráveis.\nMódulo Quadrado (\\(|\\psi(x)|^2\\)): Densidade de probabilidade fisicamente mensurável. Representa a probabilidade por unidade de comprimento de encontrar a partícula na posição \\(x\\).\n\n\n\nO Papel da Equação de Schrödinger\nA equação: \\[i\\hbar\\frac{\\partial}{\\partial t}\\psi(x,t) = \\left(-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x)\\right)\\psi(x,t)\\]\ndetermina deterministicamente como \\(\\psi(x,t)\\) evolui no tempo. No entanto, o que esta equação evolui é uma distribuição de probabilidade.\nEsta combinação - equação de evolução determinística para uma entidade probabilística - constitui a base conceitual da mecânica quântica.\n\n\nA Relação Determinismo-Estatística\nA MQ nos apresenta um contraste definitivo:\n\nA equação de evolução (Schrödinger) é perfeitamente determinística.\nOs resultados das medições são intrinsecamente probabilísticos.\n\nPortanto, a MQ é a realização máxima do tema desta aula: o casamento inevitável entre o determinismo das leis fundamentais e a estatística necessária para descrever os fenômenos mensuráveis.\nEla eleva a estatística de uma ferramenta para lidar com nossa ignorância a um marco central da descrição da realidade física.\n\n\nConclusão: Do Micro ao Macro, uma Única Linguagem\nDa trajetória de um planeta (onde erros iniciais se amplificam) à posição de um elétron (onde a probabilidade é inerente), passando por testes médicos e previsões do tempo, encontramos uma mesma linguagem: a linguagem das probabilidades.\nO determinismo nos diz como as possibilidades evoluem; a estatística nos diz como lidar com qual possibilidade se realizará.\n\n\nExercícios\n\nUma função de onda normalizada satisfaz \\(\\int_{-\\infty}^{\\infty} |\\psi(x)|^2 \\dd x = 1\\). Por que isso é necessário?\nCompare a densidade de probabilidade quântica \\(|\\psi(x)|^2\\) com a distribuição clássica de probabilidade \\(\\rho(x,p)\\) discutida nos exemplos anteriores.\nDiscuta como o Princípio da Incerteza de Heisenberg limita a precisão com que podemos conhecer simultaneamente \\(x\\) e \\(p\\), mesmo que a função de onda seja perfeitamente conhecida.\nImagine medir a posição de um elétron repetidamente em um estado gaussiano. Qual distribuição você esperaria obter para os resultados experimentais? Explique.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Determinism and Statistics"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/time-dependent-green-functions.html",
    "href": "courses/mathematical-physics/time-dependent-green-functions.html",
    "title": "Time Dependent Green Functions",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Time Dependent Green Functions"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/time-dependent-green-functions.html#exercises",
    "href": "courses/mathematical-physics/time-dependent-green-functions.html#exercises",
    "title": "Time Dependent Green Functions",
    "section": "Exercises",
    "text": "Exercises\nThese exercises guide the student through the derivation and interpretation of the Green’s function associated with the wave (D’Alembert) equation in flat spacetime. They are structured to gradually build the solution, emphasizing the role of Fourier analysis, singularities, and causal structure.\n\nWrite the D’Alembert (wave) equation for a scalar field \\(\\phi(t, \\vec{x})\\) in Minkowski spacetime: \\[\n\\Box \\phi(t, \\vec{x}) = \\left(-\\frac{1}{c^2}\\frac{\\partial^2}{\\partial t^2} + \\nabla^2 \\right)\\phi(t, \\vec{x}) = S(t, \\vec{x}),\n\\] where \\(S(t, \\vec{x})\\) is a source term.\nDefine the Green’s function \\(G(t, \\vec{x}; t', \\vec{x}')\\) as the solution to: \\[\n\\Box G(t, \\vec{x}; t', \\vec{x}') = \\delta(t - t') \\delta^{3}(\\vec{x} - \\vec{x}'),\n\\] and discuss its physical interpretation, including boundary/causality conditions (retarded, advanced).\nWrite the spatial Fourier transform of the Green’s function: \\[\nG(t - t', \\vec{x} - \\vec{x}') = \\int \\frac{d^3k\\dd w}{(\\sqrt{2\\pi})^4} \\, e^{iw c(t-t')+i \\vec{k} \\cdot (\\vec{x} - \\vec{x}')} \\tilde{G}(w, \\vec{k}),\n\\] and insert it into the wave equation to obtain the equation for \\(\\tilde{G}(w, \\vec{k})\\).\nShow that inserting the Fourier ansatz into the wave equation leads to an algebraic equation for \\(\\tilde{G}(w, \\vec{k})\\), and solve it: \\[\n\\tilde{G}(w, \\vec{k}) = \\frac{1}{(\\sqrt{2\\pi})^4}\\frac{1}{w^2 - k^2},\n\\] where \\(k = |\\vec{k}|\\). Discuss the location of singularities in the complex \\(w\\)-plane and the ambiguity in the inversion.\nWrite the inverse Fourier transform expression for the Green’s function: \\[\nG(t - t', \\vec{x} - \\vec{x}') = \\int \\frac{d^3k\\dd w}{(2\\pi)^4} \\, \\frac{e^{iw c(t-t') + i \\vec{k} \\cdot (\\vec{x} - \\vec{x}')}}{w^2 - k^2}.\n\\]\nAnalyze the singular behavior of the integrand. Discuss how the pole structure of the integrand demands a prescription to define the integral and how different choices correspond to different Green’s functions: retarded and advanced.\nSeparate the integral into:\n\n\nthe \\(w\\)-integral (time dependence),\n\n\nthe \\(\\vec{k}\\)-integral (spatial dependence),\n\n\nand discuss the physical role of each part in building the full solution.\nSolve the \\(w\\)-integral using contour integration for the retarded Green’s function by choosing a contour that ensures \\(G(t &lt; t') = 0\\). Discuss the deformation of the contour, the residue calculation, and the emergence of the Heaviside function \\(\\Theta(t - t')\\).\nUsing the result from the \\(w\\)-integral, compute the remaining \\(\\vec{k}\\)-integral and show that in three spatial dimensions: \\[\nG_{\\text{ret}}(t - t', \\vec{x} - \\vec{x}') = \\frac{\\delta(t - t' - |\\vec{x} - \\vec{x}'|/c)}{4\\pi |\\vec{x} - \\vec{x}'|}.\n\\]\nDiscuss the physical interpretation of this result:\n\n\n\nWhy is the support of the Green’s function constrained to the light cone?\n\n\nHow does this reflect causality in relativistic field theory?\n\n\nHow would this structure differ in other dimensions or if boundary conditions were imposed?",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Time Dependent Green Functions"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/index.html",
    "href": "courses/mathematical-physics/index.html",
    "title": "Mathematical Physics",
    "section": "",
    "text": "These are the lecture notes and exercise lists for the Mathematical Physics course by Sandro Vitenti. The material is organized by topic, and currently focuses on core concepts such as the Dirac delta distribution, Green’s functions, and time-dependent operators. The content is under development; most sections contain only exercises for now.\n\n\n\nDirac Delta\nGreen Functions\nTime Dependent Green Functions\n\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Overview"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/index.html#course-outline",
    "href": "courses/mathematical-physics/index.html#course-outline",
    "title": "Mathematical Physics",
    "section": "",
    "text": "Dirac Delta\nGreen Functions\nTime Dependent Green Functions\n\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Overview"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html",
    "href": "courses/vector-calculus/vector-spaces.html",
    "title": "Vector Spaces Products and Maps",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#vector-spaces",
    "href": "courses/vector-calculus/vector-spaces.html#vector-spaces",
    "title": "Vector Spaces Products and Maps",
    "section": "Vector Spaces",
    "text": "Vector Spaces\nVector spaces are used in physics to represent a wide variety of quantities. For example, in Newtonian mechanics, we use vector spaces to represent positions in space, velocities, and displacements. It is worth noting that these three physical quantities have distinct natures, which is reflected in the meaning of the operations we can perform with them. For instance, velocity vectors can be added or subtracted when we want to represent velocities in different inertial reference frames. Similarly, position vectors can be added when we change the origin of a coordinate system. However, there is no natural meaning to the sum of a position vector and a velocity vector, even though such a sum is mathematically well-defined. This highlights that when working with vector analysis, we are typically dealing with several different vector spaces. This becomes more evident when working with curvilinear coordinates, as we will see later.\nTo make our discussion more rigorous, we begin by defining vector spaces. In our treatment, we will restrict ourselves to real vector spaces, meaning our vector spaces are defined over the field of real numbers \\(\\mathbb{R}\\). In practice, this means we can multiply our vectors by real numbers. However, before defining vector spaces, we introduce some basic mathematical concepts that will be useful throughout this exposition. These concepts may seem overly formal and unnecessary for understanding the subject, but it is important for students to have these definitions available for reference to clarify the meaning of other expressions used in this exposition.\n\nDefinition 1 (Cartesian Product) Given two sets \\(A\\) and \\(B\\), the Cartesian product is simply the set of ordered pairs defined as: \\[A \\times B \\equiv \\{(a, b) \\mid a \\in A \\text{ and } b \\in B\\}.\\]\n\n\nDefinition 2 (Function Between Sets) Let \\(A\\) and \\(B\\) be two sets. We denote a function \\(f\\) with domain \\(A\\) and codomain \\(B\\) as: \\[f: A \\to B.\\] We use the notation \\(f(a) \\in B\\) to denote the value of the function \\(f\\) when applied to an element \\(a \\in A\\). It is important to note that some functions are denoted differently. For example, the addition of real numbers, \\(+: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}\\), is represented as \\(a + b \\in \\mathbb{R}\\) for \\(a, b \\in \\mathbb{R}\\). In these cases, we often refer to such functions as operators.\n\nIn Definition 2, we saw that we use different notation for some functions, such as the addition of two real numbers. Furthermore, some functions are denoted even more compactly. For example, the multiplication of two real numbers is typically denoted by the mere juxtaposition of two numbers. That is, when we write \\(ab\\) where \\(a, b \\in \\mathbb{R}\\), we are compactly representing the application of the multiplication function to two real numbers.\nFinally, we can define the concept of a real vector space:\n\nDefinition 3 (Real Vector Space) A vector space over the real numbers \\(\\mathbb{R}\\) is given by a set \\(\\mathbb{V}\\) and two operations: scalar multiplication and vector addition. The elements of \\(\\mathbb{V}\\) are denoted with an arrow above the symbol, i.e., \\(\\vec{v} \\in \\mathbb{V}\\). The operations are defined as: \\[\\cdot: \\mathbb{R} \\times \\mathbb{V} \\to \\mathbb{V}, \\quad +: \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{V}.\\] The addition operation \\(+\\) satisfies the following properties (where \\(\\vec{v}, \\vec{u}, \\vec{w} \\in \\mathbb{V}\\)):\n\nAssociativity: \\((\\vec{v} + \\vec{u}) + \\vec{w} = \\vec{v} + (\\vec{u} + \\vec{w})\\).\nCommutativity: \\(\\vec{v} + \\vec{u} = \\vec{u} + \\vec{v}\\).\nExistence of the additive identity: \\(\\vec{v} + \\vec{0} = \\vec{v}\\).\nExistence of the additive inverse: \\(\\vec{v} + (-\\vec{v}) = \\vec{0}\\), where \\(-\\vec{v} \\in \\mathbb{V}\\).\n\nThe scalar multiplication operation satisfies (where \\(\\vec{v}, \\vec{u} \\in \\mathbb{V}\\) and \\(a, b \\in \\mathbb{R}\\)):\n\nCompatibility with real multiplication: \\(a(b\\vec{v}) = (ab)\\vec{v}\\).\nExistence of the multiplicative identity: \\(1\\vec{v} = \\vec{v}\\).\nDistributivity over vector addition: \\(a(\\vec{v} + \\vec{u}) = a\\vec{v} + a\\vec{u}\\).\nDistributivity over scalar addition: \\((a + b)\\vec{v} = a\\vec{v} + b\\vec{v}\\).\n\n\nAn obvious first example of a real vector space is the set of real numbers \\(\\mathbb{R}\\) itself, whose elements we call scalars. It is easy to verify that this set satisfies all the properties described above. We can use this fact to construct more complex vector spaces. Consider the Cartesian product of two real spaces, \\(\\mathbb{R}^2 \\equiv \\mathbb{R} \\times \\mathbb{R}\\). Now, we can define the addition of two elements of \\(\\mathbb{R}^2\\) and scalar multiplication by a real number as: \\[\\begin{align}\n(a, b) + (c, d) &\\equiv (a + c, b + d), \\quad a, b, c, d \\in \\mathbb{R}, \\\\\na(b, c) &\\equiv (ab, ac), \\quad a, b, c \\in \\mathbb{R}.\n\\end{align}\\] It is straightforward to verify that all properties of Definition 3 are automatically satisfied. It is interesting to note exactly what we are doing: by defining addition and scalar multiplication as above, we are using the concepts of addition and multiplication of real numbers to define addition and multiplication in more complex sets, in this case \\(\\mathbb{R}^2\\). In other words, the set of ordered pairs of real numbers forms a vector space once we define the operations above. Naturally, we can make the same definitions for \\(\\mathbb{R}^3 \\equiv \\mathbb{R} \\times \\mathbb{R} \\times \\mathbb{R}\\). Henceforth, we will always use these definitions of addition and scalar multiplication when dealing with spaces \\(\\mathbb{R}^n \\equiv \\mathbb{R} \\times \\mathbb{R} \\times \\dots \\times \\mathbb{R}\\).",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#linear-independence",
    "href": "courses/vector-calculus/vector-spaces.html#linear-independence",
    "title": "Vector Spaces Products and Maps",
    "section": "Linear Independence",
    "text": "Linear Independence\nTo highlight the difference between \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\), we introduce a new definition:\nDefinition (Linear Independence): A set of vectors \\(\\vec{v}_i\\), \\(i \\in \\{1, 2, \\dots, n\\}\\), is said to be linearly independent if their linear combination is zero if and only if all coefficients are zero, i.e., \\[\\sum_{i=1}^n a^i \\vec{v}_i = 0 \\text{ if and only if } a^i = 0 \\text{ for all } i \\in \\{1, 2, \\dots, n\\}.\\]\nIn practical terms, a set of vectors is linearly independent if none of the vectors can be written as a linear combination of the others. That is, when they are not linearly independent, their combination may be zero even if some coefficients are non-zero (say \\(a^1 \\neq 0\\)), so: \\[\\begin{equation}\n\\vec{v}_1 = -\\frac{1}{a^1} \\sum_{i=2}^n a^i \\vec{v}_i.\n\\end{equation}\\]\nDefinition (Basis): A set of vectors forms a basis if they are linearly independent and any element of \\(\\mathbb{V}\\) can be written as a linear combination of them. The number of vectors in the basis denotes the dimension of \\(\\mathbb{V}\\).\nIt is easy to verify that the vectors \\(\\vec{e}_1 \\equiv (1, 0, 0)\\), \\(\\vec{e}_2 \\equiv (0, 1, 0)\\), and \\(\\vec{e}_3 \\equiv (0, 0, 1)\\) form a basis for \\(\\mathbb{R}^3\\). Thus, any vector in this space can be written as a linear combination of these vectors, i.e., \\[\\begin{equation}\n\\vec{v} = \\sum_{i=1}^n v^i \\vec{e}_i.\n\\end{equation}\\]\nFrom a physics perspective, we are interested in using mathematical concepts to represent concrete elements found in nature. Our real-world experience shows that we need three coordinates to specify a point in space. If we calculate the three coordinates of an object relative to an origin and then the coordinates of the origin relative to another reference point, the position of the object relative to the other reference point is given by the simple sum of the coordinates. This example shows that we can map the concept of position in space to elements of \\(\\mathbb{R}^3\\), and moreover, the operations defined in this space have physical meaning. In this sense, it is important to understand what we do when we apply physical modeling: we are seeking mathematical objects that can be mapped to physical quantities and whose operations correspond to elements of the physical world. In the example above, points in \\(\\mathbb{R}^3\\) are mapped to coordinate positions in the real world, and the addition of two vectors corresponds to a change in the coordinate origin.\nIn this chapter, we will focus on three-dimensional vector spaces, which is the arena where electromagnetism will be developed in this course.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#exercise-vector-space",
    "href": "courses/vector-calculus/vector-spaces.html#exercise-vector-space",
    "title": "Vector Spaces Products and Maps",
    "section": "Exercise Vector Space",
    "text": "Exercise Vector Space\n\nShow that \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) are vector spaces of dimension two and three, respectively.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#inner-product",
    "href": "courses/vector-calculus/vector-spaces.html#inner-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Inner Product",
    "text": "Inner Product\nThe inner product is defined as a function on a vector space that is used to calculate magnitudes and angles between vectors. The mathematical definition is as follows:\n\nDefinition 4 (Inner Product) Given a vector space \\(\\mathbb{V}\\), we define the inner product as the map: \\[\\cdot: \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}.\\] It satisfies the following properties (where \\(\\vec{v}, \\vec{u}, \\vec{w} \\in \\mathbb{V}\\)):\n\nSymmetry: \\(\\vec{v} \\cdot \\vec{w} = \\vec{w} \\cdot \\vec{v}\\).\nLinearity on the left: \\((\\vec{v} + \\vec{u}) \\cdot \\vec{w} = \\vec{v} \\cdot \\vec{w} + \\vec{u} \\cdot \\vec{w}\\).\nPositivity: \\(\\vec{v} \\cdot \\vec{v} \\geq 0\\), and equality holds if and only if \\(\\vec{v} = \\vec{0}\\).\n\nIt is worth noting that left linearity, combined with symmetry, implies right linearity. In other words, this product is bilinear. Finally, given an inner product, we define the norm or magnitude of a vector as: \\[\\begin{equation}\n|\\vec{v}| \\equiv \\sqrt{\\vec{v} \\cdot \\vec{v}},\n\\end{equation}\\] where the positivity property ensures that the square root is always well-defined.\n\nIn general, there are numerous possible choices for inner products. To see this, note that linearity implies that the inner product of any two vectors is: \\[\\vec{v} \\cdot \\vec{u} = \\left( \\sum_{i=1}^3 v^i \\vec{e}_i \\right) \\cdot \\left( \\sum_{j=1}^3 u^j \\vec{e}_j \\right) = \\sum_{i,j=1}^3 v^i u^j \\vec{e}_i \\cdot \\vec{e}_j.\\] Thus, we can compute the inner product between any two vectors if we know the inner products between the basis elements. To determine this, we need to define the six quantities \\(\\vec{e}_i \\cdot \\vec{e}_j\\). Note that, in general, there would be nine combinations of \\(i\\) and \\(j\\), but symmetry provides three equations: \\[\\begin{equation}\n\\vec{e}_2 \\cdot \\vec{e}_1 = \\vec{e}_1 \\cdot \\vec{e}_2, \\quad \\vec{e}_3 \\cdot \\vec{e}_1 = \\vec{e}_1 \\cdot \\vec{e}_3, \\quad \\vec{e}_3 \\cdot \\vec{e}_2 = \\vec{e}_2 \\cdot \\vec{e}_3.\n\\end{equation}\\] Apart from the positivity condition,1 we have complete freedom to choose these six quantities.\n1 This condition imposes constraints on the signs and inequalities between the terms but does not introduce additional equations and thus does not reduce the dimensionality of the problem.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#kronecker-delta",
    "href": "courses/vector-calculus/vector-spaces.html#kronecker-delta",
    "title": "Vector Spaces Products and Maps",
    "section": "Kronecker Delta",
    "text": "Kronecker Delta\nIn our treatment, we assume the space is flat, and in this case, the inner product that reproduces the familiar Euclidean geometry is given by \\(\\vec{e}_i \\cdot \\vec{e}_j = \\delta_{ij}\\), where we introduce the Kronecker delta: \\[\\begin{equation}\n\\delta_{ij} \\equiv \\begin{cases}\n1 & \\text{if } i = j, \\\\\n0 & \\text{if } i \\neq j.\n\\end{cases}\n\\end{equation}\\] In these notes, the inner product will always be the Euclidean inner product defined above.\nTo understand the geometric meaning of this product, we start by calculating the magnitude of a vector: \\[\\begin{equation}\n|\\vec{v}| = \\sqrt{(v^1)^2 + (v^2)^2 + (v^3)^2},\n\\end{equation}\\] which is simply the Euclidean norm we are familiar with. We can now implicitly define the angle \\(\\theta\\) between two vectors as: \\[\\begin{equation}\n\\vec{v} \\cdot \\vec{u} = |\\vec{v}| |\\vec{u}| \\cos \\theta.\n\\end{equation}\\] To verify that this definition aligns with the usual Euclidean geometry, consider the inner product between the vector \\(\\vec{v} = a^1 \\vec{e}_1 + a^2 \\vec{e}_2\\) and the vector \\(\\vec{u} = \\vec{e}_1\\), i.e., \\[\\begin{equation}\n\\vec{v} \\cdot \\vec{u} = \\left( a^1 \\vec{e}_1 + a^2 \\vec{e}_2 \\right) \\cdot \\vec{e}_1 = a^1 = |\\vec{v}| |\\vec{u}| \\frac{a^1}{\\sqrt{(a^1)^2 + (a^2)^2}} \\implies \\cos \\theta = \\frac{a^1}{\\sqrt{(a^1)^2 + (a^2)^2}}.\n\\end{equation}\\] In this example, the cosine of the angle between the vectors is exactly the adjacent side over the hypotenuse.\nIn general, the inner product defined above can be written as: \\[\n\\vec{v} \\cdot \\vec{u} = \\left( \\sum_{i=1}^3 v^i \\vec{e}_i \\right) \\cdot \\left( \\sum_{j=1}^3 u^j \\vec{e}_j \\right) = \\sum_{i,j=1}^3 v^i u^j \\vec{e}_i \\cdot \\vec{e}_j = \\sum_{i,j=1}^3 v^i u^j \\delta_{ij} = \\sum_{i=1}^3 v^i u^i.\n\\tag{1}\\] Note that in the last equality, we used the fact that \\(\\delta_{ij} = 0\\) for \\(i \\neq j\\) to eliminate one summation. Finally, we say that two vectors \\(\\vec{v}\\) and \\(\\vec{u}\\) are orthogonal if \\(\\vec{v} \\cdot \\vec{u} = 0\\).\nUsing the norm of a vector, we define the associated unit vector as: \\[\\begin{equation}\n\\hat{v} \\equiv \\frac{\\vec{v}}{|\\vec{v}|}.\n\\end{equation}\\]",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#exercises-inner-product",
    "href": "courses/vector-calculus/vector-spaces.html#exercises-inner-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Exercises Inner Product",
    "text": "Exercises Inner Product\n\nShow that left linearity and symmetry imply right linearity.\nWrite out the summations in Equation 1 explicitly and show that the presence of the Kronecker delta can be used to eliminate one summation.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#sec-eprod",
    "href": "courses/vector-calculus/vector-spaces.html#sec-eprod",
    "title": "Vector Spaces Products and Maps",
    "section": "Exterior Product",
    "text": "Exterior Product\nThe vector analysis developed so far pertains to objects representing points and directions in space. However, other concepts may be necessary to describe physical phenomena, namely planes and volumes. To describe a plane, we need two directions in space, meaning there is only one plane defined by two vectors that are not collinear.2 A volume, on the other hand, requires three vectors that are not coplanar to be defined. The concept needed to define planes and volumes is the tensor product. In this exposition, we provide a simplified discussion to build the reader’s intuition. Loosely speaking, the tensor product of a space \\(\\mathbb{V}\\) with itself, \\(\\mathbb{V} \\otimes \\mathbb{V}\\), is also a vector space (in the sense of Definition 3) formed by the basis vectors \\(\\vec{e}_i \\otimes \\vec{e}_j\\), \\(i, j \\in \\{1, 2, 3\\}\\), meaning it is a vector space of dimension nine. However, to describe planes and volumes, we want to exclude cases with collinear vectors. To do so, we restrict ourselves to the antisymmetric subspace of \\(\\mathbb{V} \\otimes \\mathbb{V}\\), as the antisymmetric product of two collinear vectors is always zero.\n2 Collinear vectors are those that correspond to a simple scalar multiple, i.e., if \\(\\vec{v}\\) and \\(\\vec{u}\\) are collinear, there exists a real number \\(a \\neq 0\\) such that \\(\\vec{v} = a \\vec{u}\\).\nDefinition 5 (Exterior Product) The basis of this antisymmetric second-order tensor space is defined as: \\[\\vec{e}_i \\wedge \\vec{e}_j \\in \\mathbb{V} \\wedge \\mathbb{V}, \\quad \\vec{e}_i \\wedge \\vec{e}_j = \\left( \\vec{e}_i \\otimes \\vec{e}_j - \\vec{e}_j \\otimes \\vec{e}_i \\right). \\tag{2}\\]\n\nWe denote vectors in the space \\(\\mathbb{V} \\wedge \\mathbb{V}\\) as bivectors. In this definition, we use the exterior product, but we will not delve into the mathematical details required for its formal definition. The important properties in our context are: \\[\\begin{aligned}\n\\vec{v} \\wedge \\vec{u} &= -\\vec{u} \\wedge \\vec{v}, \\quad (\\vec{v} + \\vec{u}) \\wedge \\vec{w} = \\vec{v} \\wedge \\vec{w} + \\vec{u} \\wedge \\vec{w}, \\quad (a \\vec{v}) \\wedge \\vec{u} = a (\\vec{v} \\wedge \\vec{u}), \\\\\n\\left( \\vec{v} \\wedge \\vec{u} \\right) \\wedge \\vec{w} &= \\vec{v} \\wedge \\left( \\vec{u} \\wedge \\vec{w} \\right), \\quad \\vec{v}, \\vec{u}, \\vec{w} \\in \\mathbb{V}, \\ a \\in \\mathbb{R}.\n\\end{aligned} \\tag{3}\\] It is easy to verify that this space is three-dimensional, meaning there are only three non-zero basis vectors: \\(\\vec{e}_1 \\wedge \\vec{e}_2\\), \\(\\vec{e}_1 \\wedge \\vec{e}_3\\), and \\(\\vec{e}_2 \\wedge \\vec{e}_3\\). Due to this coincidence—that the dimension of the antisymmetric tensor space in three dimensions is also three—J. Willard Gibbs introduced the concept of the vector product in his vector analysis textbook. The idea is to create a linear, bijective map (an isomorphism) between the space \\(\\mathbb{V} \\wedge \\mathbb{V}\\) and the original vector space \\(\\mathbb{V}\\). To do this, note that there is only one vector orthogonal to the plane \\(\\vec{e}_1 \\wedge \\vec{e}_2\\) (i.e., a vector orthogonal to both vectors used to construct the plane), namely \\(\\vec{e}_3\\).",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#vector-product",
    "href": "courses/vector-calculus/vector-spaces.html#vector-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Vector Product",
    "text": "Vector Product\nTo define our map, we can set \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\to \\vec{e}_3\\). However, note that planes can have different signs (e.g., \\(\\vec{e}_1 \\wedge \\vec{e}_2 = -\\vec{e}_2 \\wedge \\vec{e}_1\\)). Thus, to determine the map, we need to choose the signs of the planes mapped to vectors. Here, we follow the standard “right-hand rule” convention.3 We choose the sign according to the position of the thumb of the right hand when the first vector of the product is aligned with the palm and the second with the fingertips, i.e., we define the linear map \\(V: \\mathbb{V} \\wedge \\mathbb{V} \\to \\mathbb{V}\\) as: \\[V(\\vec{e}_1 \\wedge \\vec{e}_2) = \\vec{e}_3, \\quad V(\\vec{e}_3 \\wedge \\vec{e}_1) = \\vec{e}_2, \\quad V(\\vec{e}_2 \\wedge \\vec{e}_3) = \\vec{e}_1. \\tag{4}\\] Let us now explicitly calculate the exterior product of two vectors and compute its map to the space \\(\\mathbb{V}\\): \\[\\begin{aligned}\n\\vec{v} \\wedge \\vec{u} &= \\left( \\sum_{i=1}^3 v^i \\vec{e}_i \\right) \\wedge \\left( \\sum_{j=1}^3 u^j \\vec{e}_j \\right), \\\\\n&= \\left( v^1 u^2 - v^2 u^1 \\right) \\vec{e}_1 \\wedge \\vec{e}_2 - \\left( v^1 u^3 - v^3 u^1 \\right) \\vec{e}_3 \\wedge \\vec{e}_1 + \\left( v^2 u^3 - v^3 u^2 \\right) \\vec{e}_2 \\wedge \\vec{e}_3.\n\\end{aligned} \\tag{5}\\] Finally, we define the vector product by applying our isomorphism \\(V\\) to this product: \\[\\vec{v} \\times \\vec{u} \\equiv V(\\vec{v} \\wedge \\vec{u}) = \\left( v^2 u^3 - v^3 u^2 \\right) \\vec{e}_1 - \\left( v^1 u^3 - v^3 u^1 \\right) \\vec{e}_2 + \\left( v^1 u^2 - v^2 u^1 \\right) \\vec{e}_3. \\tag{6}\\] To simplify vector product calculations, we can compute the product of basis vectors and establish a general rule: \\[\\vec{e}_i \\times \\vec{e}_j = \\sum_{k=1}^3 \\epsilon_{ijk} \\vec{e}_k. \\tag{7}\\] The antisymmetry of the vector product shows that the term \\(\\epsilon_{ijk}\\), called the Levi-Civita symbol, must be antisymmetric in its first two indices: \\(\\epsilon_{ijk} = -\\epsilon_{jik}\\). Moreover, examining Equation 4, we see that this symbol is also antisymmetric in the last two indices, i.e., \\(\\epsilon_{ijk} = -\\epsilon_{ikj}\\), and that \\(\\epsilon_{123} = 1\\). Using the Levi-Civita symbol, we can write Equation 6 more compactly: \\[\\vec{v} \\times \\vec{u} = \\left( \\sum_{i=1}^3 v^i \\vec{e}_i \\right) \\times \\left( \\sum_{j=1}^3 u^j \\vec{e}_j \\right) = \\sum_{i,j,k=1}^3 \\epsilon_{ijk} v^i u^j \\vec{e}_k. \\tag{8}\\] Not only is this form more compact, but it is also much more convenient for computing more complex products, as we will see later.\n3 Collinear vectors are those that correspond to a simple scalar multiple, i.e., if \\(\\vec{v}\\) and \\(\\vec{u}\\) are collinear, there exists a real number \\(a \\neq 0\\) such that \\(\\vec{v} = a \\vec{u}\\).For completeness we provide the following definition:\n\nDefinition 6 (Levi-Civita Symbol) The Levi-Civita symbol is defined the complete antisymmetric tensor \\(\\epsilon_{ijk}\\) such that \\(\\epsilon_{123} = 1\\).\n\nFrom a geometric perspective, we can understand the meaning of this product by calculating the vector product between the vector \\(\\vec{v} = \\vec{e}_1\\) and the vector \\(\\vec{u} = a^1 \\vec{e}_1 + a^2 \\vec{e}_2\\): \\[\\vec{v} \\times \\vec{u} = a^2 \\vec{e}_3 = |\\vec{v}| |\\vec{u}| \\frac{a^2}{\\sqrt{(a^1)^2 + (a^2)^2}} \\vec{e}_3 = |\\vec{v}| |\\vec{u}| \\sin \\theta \\vec{e}_3. \\tag{9}\\] In other words, the vector product of two vectors results in a vector orthogonal to the plane formed by the two vectors, with a magnitude equal to the area of the parallelogram they form. This makes it clear that the result of a vector product does not have the same physical meaning as an ordinary vector. In our construction, the vector product is actually an antisymmetric tensor product used to represent planes (hence, its magnitude is associated with areas, not lengths). However, in three dimensions, it has the same dimensionality as the original vector space, allowing us to map between these spaces.\nThe case of volumes is even simpler. Following the previous logic, we want the composition of three non-coplanar vectors, which corresponds to the antisymmetric tensor product \\(\\mathbb{V} \\wedge \\mathbb{V} \\wedge \\mathbb{V}\\), called the space of trivectors. The only basis vector in this space is \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3\\); any other combination is either zero or proportional to this vector due to antisymmetry. This shows that this space is one-dimensional. For this reason, we can map this space to \\(\\mathbb{R}\\) using the map \\(\\Upsilon: \\mathbb{V} \\wedge \\mathbb{V} \\wedge \\mathbb{V} \\to \\mathbb{R}\\), defined by \\(\\Upsilon(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3) = 1\\). Like the linear map \\(V\\) used for the vector product, this map also depends on an arbitrary choice of sign. Moreover, the map \\(\\Upsilon\\) can be understood as computing the “magnitude” of this trivector, which represents the volume of the parallelepiped formed by the vectors.\nIn Gibbs’ vector analysis, instead of using the exterior product, the vector product is directly defined as a map \\(\\times: \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{V}\\). However, these vectors do not behave the same as vectors in \\(\\mathbb{V}\\) under spatial inversions, i.e., \\(\\vec{e}_i \\to -\\vec{e}_i\\). It is easy to see that the vector resulting from a vector product does not change sign in this case. For this reason, such vectors are called pseudovectors. This reveals a disadvantage of Gibbs’ analysis: it avoids introducing tensors and the exterior product but requires distinguishing between different types of vectors, which can lead to confusion in more extensive calculations. Similarly, instead of introducing the space of trivectors, we use the map \\(\\Upsilon\\) to map these elements to scalars (i.e., \\(\\mathbb{R}\\)). Here, we encounter the same complication as with the vector product: the scalars resulting from \\(\\Upsilon\\) do not behave like ordinary elements of \\(\\mathbb{R}\\) under spatial inversion, as they change sign and are thus called pseudoscalars.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#exercises-vector-product",
    "href": "courses/vector-calculus/vector-spaces.html#exercises-vector-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Exercises Vector Product",
    "text": "Exercises Vector Product\n\nShow that the exterior product of two vectors \\(\\vec{v}\\) and \\(a \\vec{v}\\) is zero.\nExplicitly compute Equation 5.\nExplicitly compute Equation 8.\nShow that the exterior product of three vectors \\(\\vec{v} \\wedge \\vec{u} \\wedge \\vec{w}\\) is proportional to \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3\\).",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#sec-trip",
    "href": "courses/vector-calculus/vector-spaces.html#sec-trip",
    "title": "Vector Spaces Products and Maps",
    "section": "Relation to the Exterior Product",
    "text": "Relation to the Exterior Product\nAs discussed in the Definition 5, the exterior product can be used to define mathematical objects that describe planes and volumes. Naturally, \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3\\) is a first example of a triple product. However, we need to relate this quantity to the tools available, namely the vector product and inner product. Using the definition of the map \\(V\\) given in Equation 4, it is straightforward to see that there is a relation between this map and \\(\\Upsilon\\), i.e., \\[\\begin{aligned}\n\\vec{e}_1 \\cdot V(\\vec{e}_2 \\wedge \\vec{e}_3) &= \\Upsilon(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3), \\\\\n\\vec{e}_2 \\cdot V(\\vec{e}_3 \\wedge \\vec{e}_1) &= \\Upsilon(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3), \\\\\n\\vec{e}_3 \\cdot V(\\vec{e}_1 \\wedge \\vec{e}_2) &= \\Upsilon(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3).\n\\end{aligned} \\tag{10}\\] In fact, it is not difficult to show that, in general, \\[\\vec{e}_i \\cdot V(\\vec{e}_j \\wedge \\vec{e}_k) = \\vec{e}_i \\cdot (\\vec{e}_j \\times \\vec{e}_k) = \\Upsilon(\\vec{e}_i \\wedge \\vec{e}_j \\wedge \\vec{e}_k) = \\epsilon_{ijk}. \\tag{11}\\] With this result, it is also easy to see that \\[\\vec{e}_i \\wedge \\vec{e}_j \\wedge \\vec{e}_k = \\epsilon_{ijk} \\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3. \\tag{12}\\] Using the linearity of the maps \\(V\\) and \\(\\Upsilon\\) and the definition of the vector product in Equation 6, we have \\[\\vec{v} \\cdot (\\vec{u} \\times \\vec{w}) = \\Upsilon(\\vec{v} \\wedge \\vec{u} \\wedge \\vec{w}). \\tag{13}\\] This shows that the triple product, where we first compute the vector product of \\(\\vec{u}\\) and \\(\\vec{w}\\) and then take the inner product of the result with \\(\\vec{v}\\), is equivalent to computing the trivector formed by these vectors and then taking its magnitude, resulting in the volume of the parallelepiped they form.\nThe second triple product we can consider is the composition of two vector products, i.e., \\[(\\vec{e}_i \\times \\vec{e}_j) \\times \\vec{e}_k = V\\left( V\\left( \\vec{e}_i \\wedge \\vec{e}_j \\right) \\wedge \\vec{e}_k \\right),\\] where we rewrite the vector product in terms of the exterior product on the right-hand side. Using the map in Equation 4, it is straightforward to compute this product for a specific choice of basis elements, for example, \\[\\begin{aligned}\nV\\left( V\\left( \\vec{e}_1 \\wedge \\vec{e}_2 \\right) \\wedge \\vec{e}_1 \\right) &= V\\left( \\vec{e}_3 \\wedge \\vec{e}_1 \\right) = +\\vec{e}_2, \\\\\nV\\left( V\\left( \\vec{e}_1 \\wedge \\vec{e}_2 \\right) \\wedge \\vec{e}_2 \\right) &= V\\left( \\vec{e}_3 \\wedge \\vec{e}_2 \\right) = -\\vec{e}_1, \\\\\nV\\left( V\\left( \\vec{e}_1 \\wedge \\vec{e}_2 \\right) \\wedge \\vec{e}_3 \\right) &= V\\left( \\vec{e}_3 \\wedge \\vec{e}_3 \\right) = \\vec{0}.\n\\end{aligned} \\tag{14}\\] We can also use the result in Equation 7 in terms of the Levi-Civita symbol to compute the same product. After a tedious process, we can show that all such products can be represented by \\[(\\vec{e}_i \\times \\vec{e}_j) \\times \\vec{e}_k = V\\left( V\\left( \\vec{e}_i \\wedge \\vec{e}_j \\right) \\wedge \\vec{e}_k \\right) = \\left( \\vec{e}_k \\cdot \\vec{e}_i \\right) \\vec{e}_j - \\left( \\vec{e}_k \\cdot \\vec{e}_j \\right) \\vec{e}_i. \\tag{15}\\] Using the linearity of all products involved, it follows naturally that \\[\\left( \\vec{v} \\times \\vec{u} \\right) \\times \\vec{w} = V\\left( V\\left( \\vec{v} \\wedge \\vec{u} \\right) \\wedge \\vec{w} \\right) = \\left( \\vec{w} \\cdot \\vec{v} \\right) \\vec{u} - \\left( \\vec{w} \\cdot \\vec{u} \\right) \\vec{v}. \\tag{16}\\] With these rules, we can compute any product involving three or more terms.\nAnother useful rule for the upcoming sections comes from applying the inverse of \\(V\\), defined in Equation 4, i.e., \\[V^{-1}(\\vec{e}_3) = \\vec{e}_1 \\wedge \\vec{e}_2, \\quad V^{-1}(\\vec{e}_2) = \\vec{e}_3 \\wedge \\vec{e}_1, \\quad V^{-1}(\\vec{e}_1) = \\vec{e}_2 \\wedge \\vec{e}_3. \\tag{17}\\] It is evident that the exterior product of \\(V^{-1}(\\vec{e}_i)\\) with \\(\\vec{e}_j\\) is zero if \\(i \\neq j\\), and it is straightforward to show that it equals \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3\\) if \\(i = j\\). Thus, we have \\[V^{-1}(\\vec{e}_i) \\wedge \\vec{e}_j = \\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3 \\ \\delta_{ij}, \\quad \\Upsilon\\left[ V^{-1}(\\vec{e}_i) \\wedge \\vec{e}_j \\right] = \\delta_{ij}. \\tag{18}\\] In terms of vectors, we can easily compute that \\[\\Upsilon\\left[ V^{-1}(\\vec{v}) \\wedge \\vec{u} \\right] = \\vec{v} \\cdot \\vec{u}. \\tag{19}\\]\nWhen working in a vector space with both an exterior product and an inner product, it is natural to extend the inner product to compute not only the product between two vectors but also the product between a vector and a bivector or trivector. A possible definition, compatible with the antisymmetry of the exterior product, is \\[\\begin{aligned}\n\\vec{w} \\cdot (\\vec{v} \\wedge \\vec{u}) &= \\left( \\vec{w} \\cdot \\vec{v} \\right) \\vec{u} - \\left( \\vec{w} \\cdot \\vec{u} \\right) \\vec{v}, \\\\\n\\vec{w} \\cdot (\\vec{v} \\wedge \\vec{u} \\wedge \\vec{k}) &= \\left( \\vec{w} \\cdot \\vec{v} \\right) \\vec{u} \\wedge \\vec{k} - \\left( \\vec{w} \\cdot \\vec{u} \\right) \\vec{v} \\wedge \\vec{k} + \\left( \\vec{w} \\cdot \\vec{k} \\right) \\vec{v} \\wedge \\vec{u}.\n\\end{aligned} \\tag{20}\\] In words, the inner product of a vector with a bivector or trivector is the sum of the inner products of the vector with each constituent vector of the bivector or trivector, with the sign given by \\((-1)^n\\), where \\(n\\) is the number of vectors to the left of the vector being multiplied. Since any exterior product of more than three vectors in a three-dimensional vector space is zero, we can write \\[\\vec{k} \\wedge \\vec{v} \\wedge \\vec{u} \\wedge \\vec{w} = 0. \\tag{21}\\] Taking the inner product of the above expression with the vector \\(\\vec{l}\\), we obtain \\[\\left( \\vec{l} \\cdot \\vec{k} \\right) \\vec{v} \\wedge \\vec{u} \\wedge \\vec{w} - \\left( \\vec{l} \\cdot \\vec{v} \\right) \\vec{k} \\wedge \\vec{u} \\wedge \\vec{w} + \\left( \\vec{l} \\cdot \\vec{u} \\right) \\vec{k} \\wedge \\vec{v} \\wedge \\vec{w} - \\left( \\vec{l} \\cdot \\vec{w} \\right) \\vec{k} \\wedge \\vec{v} \\wedge \\vec{u} = 0. \\tag{22}\\] The result above can also be obtained directly through a long and tedious process by explicitly writing all vectors in terms of the basis \\(\\vec{e}_i\\).\nNote that Equation 16 also shows the relation between the map \\(V\\) and the inner product between vectors and bivectors or trivectors, i.e., \\[V\\left( V\\left( \\vec{v} \\wedge \\vec{u} \\right) \\wedge \\vec{w} \\right) = \\vec{w} \\cdot (\\vec{v} \\wedge \\vec{u}). \\tag{23}\\] Thus, similar to Equation 11, the exterior product has an intimate relation with the map \\(V\\). This arises because there exists a transformation between vectors, bivectors, and trivectors called the Hodge star map, and both \\(V\\) and \\(\\Upsilon\\) are related to the action of this map when applied to bivectors and trivectors, respectively.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#exercises-triple-product",
    "href": "courses/vector-calculus/vector-spaces.html#exercises-triple-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Exercises Triple Product",
    "text": "Exercises Triple Product\n\nShow that Equation 11 and Equation 15 are true.\nStarting from Equation 11, prove that Equation 13 is valid.\nUsing Equation 15, prove that Equation 16 is valid.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#sec-euclid",
    "href": "courses/vector-calculus/vector-spaces.html#sec-euclid",
    "title": "Vector Spaces Products and Maps",
    "section": "Euclidean Space",
    "text": "Euclidean Space\n\nCartesian Coordinates\nWith the tools for calculations in vector spaces, we now establish a correspondence between mathematical definitions and the geometric notions of the space we aim to describe. Our definition of \\(\\mathbb{R}^3\\) does not specify the meaning of each component of the ordered triple. Assuming the space we inhabit is well-described by Euclidean geometry, we can construct a coordinate system to describe points as follows. First, choose an origin point and three orthogonal directions, \\(\\vec{e}_1\\), \\(\\vec{e}_2\\), and \\(\\vec{e}_3\\) (using the right-hand rule to determine \\(\\vec{e}_3\\)’s direction). For any point \\(p\\), compute its coordinates by:\n\nMove along \\(\\vec{e}_1\\) until aligned with \\(p\\), measure the distance from the origin, assign a positive sign if moving in the direction of \\(\\vec{e}_1\\) or negative if opposite, and call this value \\(x^1\\).\nMove along \\(\\vec{e}_2\\) until aligned with \\(p\\), record the distance and sign, obtaining \\(x^2\\).\nRepeat along \\(\\vec{e}_3\\) to reach \\(p\\), computing \\(x^3\\).\n\n\nDefinition 7 (Cartesian Coordinates) The Cartesian coordinates of a point \\(p\\) are the ordered triple of signed distances traveled in three perpendicular directions from the origin to \\(p\\). The point is represented by the vector: \\[\\vec{x} = \\sum_{i=1}^3 x^i \\vec{e}_i \\equiv (x^1, x^2, x^3), \\quad \\text{where} \\quad \\begin{array}{c}\n\\vec{e}_1 = (1, 0, 0), \\\\ \\vec{e}_2 = (0, 1, 0), \\\\ \\vec{e}_3 = (0, 0, 1).\n\\end{array} \\tag{24}\\] The equality after the summation uses the representation of vectors as ordered triples in \\(\\mathbb{R}^3\\). We call \\(\\vec{x}\\) the position vector.\n\nCartesian coordinates are special because operations on position vectors have geometric meaning. For example, the magnitude \\(|\\vec{x}|\\) represents the distance from the origin to the point at the vector’s tip. The difference between two position vectors, \\(\\vec{x} - \\vec{y}\\), represents the displacement between their points.4 The magnitude \\(|\\vec{x} - \\vec{y}|\\) is the Euclidean distance between the points. Thus, the inner product, as defined in Equation 1, acts as a metric for calculating distances in Euclidean space when applied to position vectors.\n4 We implicitly assume displacements are straight lines, equivalent to geodesics in Euclidean space.In contrast, other coordinate systems can label points differently. For example, using spherical coordinates with the same origin and directions, we compute: - The distance \\(r\\) from the origin to \\(p\\). - The angle \\(\\theta\\) between the line from the origin to \\(p\\) and \\(\\vec{e}_3\\). - The angle \\(\\varphi\\) between the projection of this line onto the \\(\\vec{e}_1\\)-\\(\\vec{e}_2\\) plane and \\(\\vec{e}_1\\).\nThe triple \\((r, \\theta, \\varphi)\\) is a point in \\(\\mathbb{R}^3\\). While \\(\\mathbb{R}^3\\) has a natural vector space structure, allowing addition, subtraction, and magnitude calculations, these operations lack immediate geometric meaning in spherical coordinates. For instance, computing the distance between \\((r_1, \\theta_1, \\varphi_1)\\) and \\((r_2, \\theta_2, \\varphi_2)\\) requires parameterizing the straight line between them and evaluating a line integral.5\n5 The line integral, as discussed later, depends on a norm in the tangent vector space, requiring Euclidean geometry to define it.Spherical coordinates highlight how Cartesian coordinates and position vectors simplify geometric calculations. This approach, using Cartesian coordinates as the starting point, will be adopted throughout these notes for geometric information. However, this is a matter of convenience, not necessity; a full differential geometry approach is more complex but possible.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#sec-vectan",
    "href": "courses/vector-calculus/vector-spaces.html#sec-vectan",
    "title": "Vector Spaces Products and Maps",
    "section": "Tangent Vectors",
    "text": "Tangent Vectors\nUsing geometric concepts represented by position vectors, we define the space of tangent vectors. Given a curve described by the position vector \\(\\vec{x}(t)\\), the displacement due to a parameter change \\(t \\to t + \\delta t\\) is: \\[\\Delta \\vec{x}(t) \\equiv \\vec{x}(t + \\delta t) - \\vec{x}(t). \\tag{25}\\] As \\(\\delta t \\to 0\\), \\(\\Delta \\vec{x}(t)\\) approximates the true displacement along the curve: \\[\\Delta \\vec{x}(t) \\approx \\dot{\\vec{x}}(t) \\delta t, \\quad \\dot{\\vec{x}}(t) \\equiv \\frac{\\partial \\vec{x}(t)}{\\partial t}. \\tag{26}\\] We call \\(\\dot{\\vec{x}}(t)\\) the tangent vector to the curve \\(\\vec{x}(t)\\).\n\nDefinition 8 (Tangent Space) The space of all tangent vectors to curves passing through a point \\(\\vec{x}\\) is called the tangent space at \\(\\vec{x}\\), denoted \\(\\mathbb{V}_{\\vec{x}}\\).\n\nThe displacement’s magnitude is: \\[|\\Delta \\vec{x}(t)| \\approx |\\dot{\\vec{x}}| |\\delta t|, \\quad \\text{where} \\quad |\\dot{\\vec{x}}| = \\sqrt{\\dot{\\vec{x}} \\cdot \\dot{\\vec{x}}}. \\tag{27}\\] The inner product, used to measure distances in the position space, is “inherited” by the tangent space \\(\\mathbb{V}_{\\vec{x}}\\). This allows us to measure infinitesimal displacements from a point by computing the length of tangent vectors. Thus, the inner product defines angles and lengths in both the position space and \\(\\mathbb{V}_{\\vec{x}}\\).\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html",
    "href": "courses/special-relativity/galilean-relativity.html",
    "title": "Galilean Relativity",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#galilean-relativity",
    "href": "courses/special-relativity/galilean-relativity.html#galilean-relativity",
    "title": "Galilean Relativity",
    "section": "Galilean Relativity",
    "text": "Galilean Relativity\nIn Galilean relativity, space is modeled as a three-dimensional Euclidean vector space1:\n1 Euclidean space\\[\n\\mathbb{E}^3 = (\\mathbb{R}^3, +, \\cdot)\n\\]\nThis structure defines:\n\nA vector space: closed under vector addition and scalar multiplication.\nAn inner product \\((\\cdot)\\): allows us to define angles and lengths.\n\nLet \\(\\vec{x}, \\vec{y} \\in \\mathbb{E}^3\\). Then:\n\nAddition: \\(\\vec{x} + \\vec{y} \\in \\mathbb{E}^3\\)\nScalar multiplication: \\(a \\vec{x} \\in \\mathbb{E}^3\\), for \\(a \\in \\mathbb{R}\\)\nDistance between points is given by:\n\\[\n|\\vec{x} - \\vec{y}| = \\sqrt{(\\vec{x} - \\vec{y}) \\cdot (\\vec{x} - \\vec{y})}\n\\]\nwhich defines a norm, and hence a metric.\n\n\nCoordinates in \\(\\mathbb{E}^3\\)\nWe represent vectors in Cartesian coordinates:\n\\[\n\\vec{x} = (x^1, x^2, x^3)\n\\]\nEach component \\(x^i\\) corresponds to the projection of \\(\\vec{x}\\) onto the corresponding basis vector \\(\\vec{e}^i\\). These components carry physical meaning: they represent distances measured along fixed spatial directions from the origin.\nTo simplify visualization, we consider a 2-dimensional slice, \\(\\mathbb{E}^2\\), where:\n\\[\n\\vec{x} = (x^1, x^2)\n\\]\nBelow, we illustrate a vector \\(\\vec{x}\\) in \\(\\mathbb{E}^2\\), decomposed into its components.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 4))\n\ndraw_euclidean_2d(ax, title=r\"$\\mathbb{E}^2$\")\ndraw_vector_2d(ax, (1.0, 0.2), label=\"x\", color=\"blue\", components=True)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: In this diagram, \\(\\vec{x}\\) is shown along with its projections onto the \\(x^1\\) and \\(x^2\\) axes. The dashed lines and gray arrows indicate the coordinate decomposition that defines the Cartesian components.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#time",
    "href": "courses/special-relativity/galilean-relativity.html#time",
    "title": "Galilean Relativity",
    "section": "Time",
    "text": "Time\nSo far, we’ve discussed a single static space, like a snapshot of the world frozen at a particular instant. In such a snapshot, distances and directions are described by a Euclidean space, \\(\\mathbb{E}^2\\) or \\(\\mathbb{E}^3\\).\nBut time enters physics through measurement: We don’t just measure where things are, we also measure when.\n\nTwo Roles of Time\nTime appears in two distinct but related ways:\n\nSimultaneity: We want to compare the positions of multiple objects at the same instant. For example: “Two cars are 10 meters apart at 12:00.”\nChange: We want to track how the position of something evolves over time. For example: “This car was at \\(\\vec{x}_0\\) at 12:00 and at \\(\\vec{x}_1\\) at 12:05.”\n\nThis second case introduces a subtle question:",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#what-does-it-mean-for-a-point-to-stay-in-the-same-place",
    "href": "courses/special-relativity/galilean-relativity.html#what-does-it-mean-for-a-point-to-stay-in-the-same-place",
    "title": "Galilean Relativity",
    "section": "What Does It Mean for a Point to Stay in the Same Place?",
    "text": "What Does It Mean for a Point to Stay in the Same Place?\nTo say something “moved” from \\(\\vec{x}_0\\) to \\(\\vec{x}_1\\), we must assume:\n\n“The point that was at \\(\\vec{x}_0\\) at time \\(t_0\\) is the same place as the one at \\(\\vec{x}_1\\) at time \\(t_1\\).”\n\nBut how do we decide that?\nEach snapshot in time is a separate space, like \\(\\mathbb{E}^2_{t_0}\\) and \\(\\mathbb{E}^2_{t_1}\\). To describe motion, we must define a rule that identifies points across these spaces.\nLet’s visualize this idea:\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\ndraw_euclidean_2d(ax1, title=r\"$\\mathbb{E}^2_{t_0}$\")\ndraw_euclidean_2d(ax2, title=r\"$\\mathbb{E}^2_{t_1}$\")\n\ndraw_vector_2d(ax1, (0.5, 0.2), label=r\"x_0\", color=\"blue\", components=True)\ndraw_vector_2d(ax2, (0.5, 0.4), label=r\"x_1\", color=\"blue\", components=True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nEach figure is a full space at a fixed time: a spatial snapshot. But motion requires more than snapshots: it requires a way to track points across time.\n\nThe Identity Map (Trivial Frame)\nTo describe motion, we need to track spatial points across time: we must decide what it means for a point at time \\(t_0\\) to be “the same place” as a point at time \\(t_1\\).\nLet’s consider a simple scenario:\n\nAn observer is in a room and chooses a particular corner to serve as the origin of their coordinate system. From this origin, they align three fixed axes, say, along the edges of the room, and assign coordinates to every point based on measured distances along these axes.\n\nNow suppose this same observer makes two measurements, at times \\(t_0\\) and \\(t_1\\), using the same reference system: the same corner, the same directions, and the same notion of distance. Then, the observer naturally identifies the “same point in space” at both times by matching coordinates: \\[\n\\vec{x}_1 = \\vec{x}_0\n\\]\nThis defines the so-called identity map (or trivial identification): \\[\n\\phi: \\mathbb{E}^2_{t_0} \\to \\mathbb{E}^2_{t_1}, \\qquad \\phi(\\vec{x}_0) = \\vec{x}_0\n\\]\nUnder this rule, an object is said to remain at rest if its coordinates do not change with time. However, this identification is not absolute, it is tied to the specific reference system used by the observer.\nThis distinction is key:\n\nThe first part of our discussion (assigning coordinates) requires physical references: like walls, corners, rulers; to define positions at a single instant.\nThe second part (relating points across time) defines the reference frame: the rule we use to track positions through time.\n\nIn practice, when we draw diagrams in \\(\\mathbb{E}^2\\), we often implicitly assume this trivial map, we imagine the reference objects do not move over time. This allows us to focus on how other objects evolve relative to the fixed background.\nBut this is only one possibility. More general mappings reflect different frames of reference, for example, frames moving with constant velocity (Galilean boosts). We’ll return to this idea soon.\n\n\nMaps Must Reflect Physical Expectations\nOnce we allow more general observers or motion, we must impose some physical conditions on the maps we use between spatial snapshots.\nThese come from our experience with the physical world, and how we expect objects to move and interact:\n\nPoints must not collide: If two distinct points at time \\(t_0\\) map to the same point at \\(t_1\\), we cannot distinguish their identities anymore. This contradicts the idea that particles have individual, continuous paths.\nNo points should appear or vanish: Every point at \\(t_1\\) must correspond to a point at \\(t_0\\). Otherwise, matter could appear from nothing or disappear entirely.\n\nFrom these we conclude:\n\nThe map \\(\\phi: \\mathbb{E}^2_{t_0} \\to \\mathbb{E}^2_{t_1}\\) must be bijective (one-to-one and onto).\n\nFurthermore, based on how real objects move, without teleportation or discontinuous jumps, we also expect:\n\nThe map should be smooth: small changes in position at \\(t_0\\) lead to small changes at \\(t_1\\).\n\nThis leads us to model \\(\\phi\\) as a diffeomorphism, a smooth, invertible map with a smooth inverse.\nBut again: This isn’t a mathematical requirement: it’s a physical modeling assumption based on the continuity of motion observed in nature.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#describing-motion-the-law-of-dynamics",
    "href": "courses/special-relativity/galilean-relativity.html#describing-motion-the-law-of-dynamics",
    "title": "Galilean Relativity",
    "section": "Describing Motion: The Law of Dynamics",
    "text": "Describing Motion: The Law of Dynamics\nBefore we can speak of forces or equations of motion, we must clarify what it means for an object to move, and how we can measure that change.\nIn practice, what we have is a sequence of observations: the position of a particle at different instants of time. Each of these measurements belongs to a copy of space, say \\(\\mathbb{E}^3_t\\), at time \\(t\\). But to compare them, to say how much the object moved, or how fast it changed direction, we must first decide how to identify points across time.\nThis is not automatic. It involves a modeling choice, which we made earlier by introducing a map between spatial snapshots. This map tells us which points at different times are considered “the same” from the observer’s perspective. Only then does the notion of displacement make sense.\nOnce we can compare positions across time, we can take differences, and eventually define the acceleration: \\[\n\\ddot{\\vec{x}}(t) = \\lim_{\\delta t \\to 0} \\frac{\\vec{x}(t + \\delta t) + \\vec{x}(t - \\delta t) - 2\\vec{x}(t)}{\\delta t^2},\n\\] where the dot indicates derivatives with respect to time.\nOnly after setting up this structure, coordinates, maps, and vector space operations, does it become meaningful to introduce Newton’s second law: \\[\n\\vec{F} = m \\ddot{\\vec{x}}\n\\] This equation is not the starting point, but rather the culmination of a chain of ideas rooted in observation: we measure position, define displacement and acceleration, and from there, assign a force. Historically, this came from empirical regularities, but here we are unpacking what it means to write such an equation and what assumptions it relies on.\nUltimately, Newton’s law reflects not just an observed pattern, but a whole framework of measurement, a way of tracking objects through space and time using a coherent mathematical model.\n\nConsistency of Vector Operations\nTo define acceleration, we use the second derivative: \\[\n\\ddot{\\vec{x}}(t) = \\lim_{\\delta t \\to 0} \\frac{ \\vec{x}(t+\\delta t) + \\vec{x}(t-\\delta t) - 2\\vec{x}(t) }{\\delta t^2}\n\\] At first glance, this seems like a purely mathematical operation. But for this expression to make sense, we must be able to add and subtract vectors that come from different spaces: \\(\\vec{x}(t+\\delta t) \\in \\mathbb{E}^3_{t+\\delta t}\\), \\(\\vec{x}(t-\\delta t) \\in \\mathbb{E}^3_{t-\\delta t}\\), and \\(\\vec{x}(t) \\in \\mathbb{E}^3_t\\).\nTo do this, we need a rule that tells us how to bring all these vectors into the same vector space so that arithmetic operations are well-defined. This is precisely what our map \\(\\phi\\) does, it tells us how to identify points and vectors between spaces at different times.\nWhen we adopt the trivial map (identity), we are assuming that: \\[\n\\vec{x}(t+\\delta t) \\in \\mathbb{E}^3_{t+\\delta t} \\quad \\text{is mapped to} \\quad \\mathbb{E}^3_t \\quad \\text{via} \\quad \\phi^{-1},\n\\] and likewise for \\(\\vec{x}(t-\\delta t)\\). With this identification, all three vectors are interpreted as elements of \\(\\mathbb{E}^3_t\\), and the derivative is now well defined.\nThis idea, that to differentiate vectors we must decide how to compare them at different points, is not just a technicality. It is the seed of a deep mathematical concept known as parallel transport, a central idea in differential geometry. There, we study more general rules for transporting vectors across curved spaces, where no global trivial identification exists.\nIn our case, we are working in flat Euclidean space with a chosen frame, so the identity map suffices. But it’s important to recognize that even here, the notion of transporting vectors is already present, though implicitly. And it is this structure that allows us to go from a sequence of positions to the definition of motion and dynamics.\n\n\nWhat if We Could Not Adopt the Identity Map?\nSo far, we have implicitly assumed that vectors at different times can be directly compared using the identity map. But what if this were not the case? What if comparing vectors at different times required a more general rule?\nLet’s define a map \\(T_{t_1, t_2}\\) that takes a vector from the space at time \\(t_1\\), denoted \\(\\mathbb{E}^3_{t_1}\\), to the space at time \\(t_2\\), \\(\\mathbb{E}^3_{t_2}\\). A simple example of such a map is: \\[\nT_{t_1, t_2} \\vec{x} = \\vec{x} + \\vec{v}(t_2) - \\vec{v}(t_1),\n\\] where \\(\\vec{v}(t)\\) is some vector-valued function of time. This is actually a special case of a more general concept called a diffeomorphism, but this simple form is enough for our discussion.\nNow, let’s see how this affects how we define the second derivative. We can write: \\[\n\\frac{\\dD^2 \\vec{x}}{\\dD t^2} = \\lim_{\\delta t \\to 0} \\frac{T_{t+\\delta t, t} \\vec{x}(t + \\delta t) + T_{t-\\delta t, t} \\vec{x}(t - \\delta t) - 2 \\vec{x}(t)}{\\delta t^2}.\n\\]\nTo evaluate this, we expand both transported terms using a Taylor series. For small \\(\\delta t\\), we get: \\[\n\\begin{aligned}\nT_{t+\\delta t, t} \\vec{x}(t + \\delta t) &= \\vec{x}(t + \\delta t) + \\vec{v}(t) - \\vec{v}(t + \\delta t) \\\\\n&= \\vec{x}(t + \\delta t) - \\dot{\\vec{v}}(t)\\delta t - \\tfrac{1}{2} \\ddot{\\vec{v}}(t)\\delta t^2 + O(\\delta t^3), \\\\\nT_{t-\\delta t, t} \\vec{x}(t - \\delta t) &= \\vec{x}(t - \\delta t) + \\vec{v}(t) - \\vec{v}(t - \\delta t) \\\\\n&= \\vec{x}(t - \\delta t) + \\dot{\\vec{v}}(t)\\delta t - \\tfrac{1}{2} \\ddot{\\vec{v}}(t)\\delta t^2 + O(\\delta t^3).\n\\end{aligned}\n\\]\nNow plugging these into the expression for the second derivative: \\[\n\\frac{\\dD^2 \\vec{x}}{\\dD t^2} = \\lim_{\\delta t \\to 0} \\frac{\\vec{x}(t + \\delta t) + \\vec{x}(t - \\delta t) - 2\\vec{x}(t) - \\ddot{\\vec{v}}(t)\\delta t^2}{\\delta t^2}.\n\\]\nThe limit gives: \\[\n\\frac{\\dD^2 \\vec{x}}{\\dD t^2} = \\ddot{\\vec{x}}(t) - \\ddot{\\vec{v}}(t).\n\\]\nThis result shows that under the transport map, the second derivative of the position depends not only on the usual acceleration \\(\\ddot{\\vec{x}}\\), but also on how the map varies with time — specifically, on \\(\\ddot{\\vec{v}}\\). Thus, even if the particle remains at the same coordinates, the computed derivative may differ under a different map.\nThis implies that, unless \\(\\ddot{\\vec{v}} = 0\\), the measured acceleration and force will depend on the choice of transport map. In practice, classical mechanics traditionally adopts a physical coordinate system — typically the lab frame — and uses the identity map, which corresponds to \\(\\ddot{\\vec{v}} = 0\\). The empirical laws of motion are formulated in this context. However, the calculation above shows that this choice is not unique: we can equally use any other transport map with \\(\\ddot{\\vec{v}} = 0\\), and the resulting forces and accelerations will be unchanged. In other words, if a set of physical laws holds for one such map, it will hold for all of them. This invariance under transformations with constant relative velocity is the content of Galilean relativity.\nNow here’s the key physical point: classical mechanics is grounded on the empirical observation that the motion of a system is fully determined by its initial position and velocity. If we prepare a system twice with the same position and velocity, we observe the same trajectory. If we change the velocity while keeping the position fixed, the resulting motion changes. This is not a modeling assumption — it is a physical fact. Therefore, the equations of motion must involve second-order time derivatives: the state of the system is defined by both position and velocity.\nThis second-order structure has an important implication. In our expression for the transformed second derivative, only the term \\(\\ddot{\\vec{v}}(t)\\) from the transport map affects the result. Hence, if we restrict ourselves to maps for which \\(\\ddot{\\vec{v}}(t) = 0\\), all such maps yield the same equations of motion. The physical laws are thus invariant under transformations between frames in relative uniform motion — that is, Galilean relativity arises as a direct consequence of the second-order nature of dynamics.\nMoreover, this structure reveals a further symmetry: even if we begin in a non-inertial frame (i.e., one with \\(\\ddot{\\vec{v}}(t) \\ne 0\\)), we can still transform to any other frame that differs by a constant velocity (\\(\\ddot{\\vec{v}} = 0\\)) without changing the form of the equations governing that non-inertial frame. So even in accelerated frames, there remains an internal symmetry among all frames connected by uniform motion.\nIn summary:\n\nThe second-order nature of dynamics implies that all frames related by constant-velocity transformations describe the same physics.\nThis defines a symmetry of the laws of motion.\nThat symmetry persists even in non-inertial frames: we can still shift to another frame with constant relative velocity without altering our description of the observed phenomena.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#other-symmetries",
    "href": "courses/special-relativity/galilean-relativity.html#other-symmetries",
    "title": "Galilean Relativity",
    "section": "Other Symmetries",
    "text": "Other Symmetries\nChanging the reference frame is not the only symmetry present in classical mechanics. The equations of motion exhibit additional symmetries that are both empirically observed and reflected in the mathematical structure of the theory.\nFirst, we observe that the outcome of a mechanical experiment does not depend on where it is performed. That is, if we repeat the same setup at a different location in space, we obtain the same results. This is spatial translation symmetry. Similarly, the results of the experiment do not depend on the orientation of the coordinate system: rotating the entire setup in space leaves the physics unchanged. This is rotational symmetry. Both of these are geometric symmetries of the Euclidean space we assume, they are built into the invariance of the spatial metric under translations and rotations.\nSecond, we observe that repeating an experiment at a different time, today, tomorrow, or next year, yields the same results, provided external conditions remain the same. This is time translation symmetry.\nThese symmetries, spatial translations, spatial rotations, and time translations, are empirical facts that are encoded in the structure of Newtonian mechanics. When combined with the invariance under changes of inertial reference frame (i.e., transformations with constant relative velocity), they form the full symmetry group of Galilean relativity.\nIn summary:\n\nRotational symmetry: physics does not depend on orientation.\nSpatial translation symmetry: physics does not depend on position.\nTime translation symmetry: physics does not depend on absolute time.\nInertial frame symmetry: physics is the same in all frames moving at constant velocity relative to one another.\n\nThese symmetries not only reflect experimental observations but also constrain the form of the allowed physical laws. They play a central role in the formulation of conservation laws and the structure of classical dynamics.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#field-transformations-and-the-role-of-the-map",
    "href": "courses/special-relativity/galilean-relativity.html#field-transformations-and-the-role-of-the-map",
    "title": "Galilean Relativity",
    "section": "Field Transformations and the Role of the Map",
    "text": "Field Transformations and the Role of the Map\nLet us now examine how the structures we have built so far apply to fields and their dynamics.\nOperationally, a field is something we measure in space at a given instant of time. That is, at each fixed time \\(t\\), we measure a quantity defined over the corresponding three-dimensional Euclidean space \\(\\mathbb{E}^3_t\\). Therefore, we do not have a single function \\(\\phi(t, \\vec{x})\\) defined on \\(\\mathbb{R} \\times \\mathbb{E}^3_{t_0}\\), but rather a family of functions: \\[\n\\phi: t \\in \\mathbb{R} \\mapsto \\phi_t \\in \\text{Func}(\\mathbb{E}^3_t, \\mathbb{R}),\n\\] where each \\(\\phi_t\\) assigns values to points in the corresponding spatial slice \\(\\mathbb{E}^3_t\\). In this formulation, the field is a map from time to spatial functions, not a function of both time and space in a fixed space.\nTo write a global expression like \\(\\phi(t, \\vec{x})\\), we must first introduce a rule that identifies “the same point” across different spatial slices \\(\\mathbb{E}^3_t\\). This is the role of the maps \\[\nT_{t_0,t}: \\mathbb{E}^3_{t_0} \\to \\mathbb{E}^3_t,\n\\] which define how we track a fixed spatial point through time. Once such maps are chosen, we can represent the field as a function in the fixed space \\(\\mathbb{E}^3_{t_0}\\) via: \\[\n\\phi(t, \\vec{x}) \\equiv \\phi_t(T_{t_0,t}(\\vec{x})),\n\\quad \\text{with } \\vec{x} \\in \\mathbb{E}^3_{t_0}.\n\\] This global description depends on the chosen identification map \\(T_{t_0,t}\\), reflecting a modeling assumption about how space is related across time.\nThis structure becomes essential when describing field evolution, as it determines how we compare field values at different times. It encodes our implicit definition of “the same spatial point at different times”: a notion that becomes nontrivial when changing reference frames.\nFor instance, consider a frame moving with constant velocity \\(\\vec{v}_0\\). In this case, we define the map: \\[\nT_{t_0,t}(\\vec{x}) = \\vec{x} + \\vec{v}_0(t - t_0).\n\\] This corresponds to tracking a point that moves rigidly with velocity \\(\\vec{v}_0\\). Then, assuming \\(t_0 = 0\\) for convenience, the field seen from this moving frame is expressed as: \\[\n\\phi(t, \\vec{x}) = \\phi_t(\\vec{x} + \\vec{v}_0 t).\n\\]\n\nWhat Happens to the Equations of Motion?\nOnce we choose a reference frame, i.e., a coordinate system and an identification map \\(T_{t_0,t}\\), we can design experiments that probe the dynamics of a field. For instance, we may repeatedly measure the field \\(\\phi\\) at different points and times, in such a way that we can compare values across spatial slices. Within this operational setting, we may discover that the field obeys a certain dynamical law.\nSuppose, after performing such measurements in a given frame \\(S\\), we determine that the field satisfies the wave equation: \\[\n\\left( -\\frac{1}{c^2} \\frac{\\partial^2}{\\partial t^2} + \\nabla^2 \\right) \\phi(t, \\vec{x}) = 0.\n\\] This equation, which appears in the context of Maxwell’s equations in Lorenz gauge, is taken as a model for the field’s dynamics in that frame.\nWe could now repeat the entire set of experiments in a different frame \\(S'\\), defined by a different choice of coordinates and identification maps, for instance, one related to the original frame by a constant relative velocity \\(\\vec{v}_0\\). From our previous analysis of particle mechanics, we know that Newton’s laws retain their form under such Galilean changes of frame. This leads us to expect that the dynamical laws for fields, being part of physical reality, should also be invariant under such transformations.\nLet us test this expectation explicitly.\nIn the new frame \\(S'\\), we identify points using the map: \\[\nT_{0,t}(\\vec{x}) = \\vec{x} + \\vec{v}_0 t.\n\\] So the field as seen in \\(S'\\) is given by: \\[\n\\phi'(t, \\vec{x}) \\equiv \\phi(t, \\vec{x} + \\vec{v}_0 t),\n\\] which corresponds to observing the same field using the new coordinates.\nComputing the time derivative in this frame: \\[\n\\frac{\\partial \\phi'}{\\partial t}\n= \\frac{\\partial}{\\partial t} \\phi(t, \\vec{x} + \\vec{v}_0 t)\n= \\frac{\\partial \\phi}{\\partial t} + \\vec{v}_0 \\cdot \\nabla \\phi,\n\\] and \\[\n\\frac{\\partial^2 \\phi'}{\\partial t^2}\n= \\frac{\\partial^2 \\phi}{\\partial t^2}\n+ 2 (\\vec{v}_0 \\cdot \\nabla) \\frac{\\partial \\phi}{\\partial t}\n+ (\\vec{v}_0 \\cdot \\nabla)^2 \\phi.\n\\]\nSubstituting into the wave equation yields: \\[\n\\left[ -\\frac{1}{c^2} \\left( \\frac{\\partial}{\\partial t} + \\vec{v}_0 \\cdot \\nabla \\right)^2 + \\nabla^2 \\right] \\phi'(t, \\vec{x}) = 0.\n\\] This is not the same operator as in the original frame, and hence the field no longer satisfies the same wave equation in the transformed coordinates unless \\(\\vec{v}_0 = 0\\). The transformation deforms the structure of the equation.\nTherefore:\n\nThe form of Maxwell’s equations is not preserved under Galilean transformations.\n\nThis result was historically problematic: unlike Newtonian mechanics, electromagnetism did not exhibit Galilean invariance. The resolution came with Einstein’s formulation of special relativity, in which the maps between inertial frames are given by Lorentz transformations. These maps preserve the wave operator: \\[\n\\square = -\\frac{1}{c^2} \\frac{\\partial^2}{\\partial t^2} + \\nabla^2,\n\\] ensuring that Maxwell’s equations hold in all inertial frames. Thus, special relativity restores the principle of relativity at the level of field dynamics.\n\nThis concludes our first module. We’ve built from physical intuition a precise mathematical structure: spaces, maps, vectors, derivatives, that allows us to formulate and interpret physical laws. We’ve seen that consistency of operations across time depends on how we relate points in space, and how this structure is already implicit in the description of dynamics. Finally, we’ve learned that even the transformation properties of space and time are constrained by the physical laws we wish to preserve.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "",
    "text": "This website hosts lecture notes for undergraduate physics courses taught by Sandro Vitenti. These notes are designed to support students in understanding key concepts in physics through clear explanations, mathematical derivations, and practical examples. The content is open source, and contributions from the community are encouraged."
  },
  {
    "objectID": "index.html#welcome-to-undergraduate-physics-lecture-notes",
    "href": "index.html#welcome-to-undergraduate-physics-lecture-notes",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "",
    "text": "This website hosts lecture notes for undergraduate physics courses taught by Sandro Vitenti. These notes are designed to support students in understanding key concepts in physics through clear explanations, mathematical derivations, and practical examples. The content is open source, and contributions from the community are encouraged."
  },
  {
    "objectID": "index.html#available-courses",
    "href": "index.html#available-courses",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "Available Courses",
    "text": "Available Courses\nExplore the following courses, each with notes and exercises:\n\nVector Calculus: Covers vector spaces, inner products, vector products, and triple products, with applications to physics.\nSpecial Relativity: Introduces the principles of special relativity, including Lorentz transformations, spacetime, and relativistic kinematics."
  },
  {
    "objectID": "index.html#about-the-project",
    "href": "index.html#about-the-project",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "About the Project",
    "text": "About the Project\nThese notes are developed as an open-source project to provide accessible, high-quality resources for physics students. The content is hosted on GitHub, where you can:\n\nAccess the source files (written in Quarto).\nReport errors or ask questions.\nSuggest improvements or new topics.\nContribute directly by submitting pull requests.\n\nThe notes are released under the CC BY-NC-SA 4.0 license, allowing non-commercial use, remixing, and distribution with attribution to the original author by linking back to this site."
  },
  {
    "objectID": "index.html#contribute",
    "href": "index.html#contribute",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "How to Contribute",
    "text": "How to Contribute\nWe welcome feedback, questions, and contributions from students, educators, and physics enthusiasts. Whether you spot a typo, have a question, or want to add a new section, you can get involved through our GitHub repository. Below are quick tips to get started.\n\nCreating a GitHub Issue\nIf you find an error, have a question, or want to suggest an improvement, create a GitHub issue:\n\nGo to the Repository: Visit github.com/vitenti-physics/undergrad.\nNavigate to Issues: Click the “Issues” tab near the top of the page.\nCreate a New Issue: Click the green “New issue” button.\nFill in Details:\n\nTitle: Write a clear, concise title (e.g., “Typo in Vector Product Equation”).\nDescription: Provide details, such as the section (e.g., Vector Product), the issue, and any suggestions. For questions, include context (e.g., “I don’t understand why the Levi-Civita symbol is antisymmetric”).\nLabels (optional): Add labels like “bug,” “question,” or “enhancement” if prompted.\n\nSubmit: Click “Submit new issue” to post it. You’ll be notified of responses.\n\n\n\nSuggesting Improvements or Contributing Content\nTo propose changes (e.g., fixing equations, adding examples), you can submit a pull request:\n\nFork the Repository: On the GitHub repo, click “Fork” to create a copy under your account.\nClone Your Fork: Download your fork to your computer using:\n\n   git clone https://github.com/your-username/undergrad.git\n\nEdit Files: Modify the Quarto files (e.g., vector_calculus.qmd) using a text editor.\n\nFor example, fix a typo in Vector Product.\n\nCommit Changes: Save your changes and commit:\n\n    git add .\n    git commit -m \"Fix typo in vector product section\"\n\n    Push to GitHub: Upload your changes:\n    bash\n\n    git push origin main\n\nCreate a Pull Request: Go to your fork on GitHub, click “Contribute,” and select “Open pull request.”\n\nDescribe your changes and submit."
  },
  {
    "objectID": "courses/special-relativity/index.html",
    "href": "courses/special-relativity/index.html",
    "title": "Special Relativity",
    "section": "",
    "text": "Welcome to the Special Relativity course lecture notes by Sandro Vitenti.\n\n\n\nGalilean Relativity\nSpacetime Algebra\n\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Overview"
    ]
  },
  {
    "objectID": "courses/special-relativity/index.html#course-outline",
    "href": "courses/special-relativity/index.html#course-outline",
    "title": "Special Relativity",
    "section": "",
    "text": "Galilean Relativity\nSpacetime Algebra\n\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Overview"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html",
    "href": "courses/special-relativity/spacetime-algebra.html",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]\nThis document summarizes key aspects of the Clifford algebra \\(\\mathrm{Cl}(1,3)\\), which encodes the geometric structure of Minkowski spacetime. We outline its basis, multivector structure, products, and important identities. The geometric algebra approach provides an algebraic description of spacetime, encompassing Lorentz transformations and reference frames.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#clifford-algebra-of-spacetime",
    "href": "courses/special-relativity/spacetime-algebra.html#clifford-algebra-of-spacetime",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Clifford Algebra of Spacetime",
    "text": "Clifford Algebra of Spacetime\nThe algebra \\(\\mathrm{Cl}(1,3)\\) is generated by a set of 1-vectors \\({\\e^\\mu}\\) satisfying the Minkowski metric \\(\\eta\\_{\\mu\\nu} = \\mathrm{diag}(-1,1,1,1)\\). Its elements are constructed from wedge products of these generators: \\[\n\\mathrm{Cl}(1,3) = \\mathrm{span}\\left\\{1, \\e^\\mu, \\e^\\mu \\wedge \\e^\\nu, \\e^\\mu \\wedge \\e^\\nu \\wedge \\e^\\rho, \\e^0 \\wedge \\e^1 \\wedge \\e^2 \\wedge \\e^3\\right\\}\n\\] The algebra has 16 independent elements: one scalar, four vectors, six bivectors, four trivectors, and one pseudoscalar. These are classified by grade: a scalar is grade-0, a vector is grade-1, a bivector is grade-2, a trivector is grade-3, and the pseudoscalar is grade-4. The full algebra includes linear combinations of such elements, known collectively as multivectors.\nEach \\(k\\)-vector represents a specific geometric object: for instance, \\(\\e^\\mu\\) is a 1-vector (associated with directions), \\(\\e^\\mu \\wedge \\e^\\nu\\) is a bivector (representing oriented planes), and so on. This graded structure mirrors the decomposition of the algebra and plays a central role in defining its products and geometric interpretation.\nFor grade-1 elements \\(\\omega, u \\in \\mathrm{Cl}(1,3)\\), the symmetric (inner) and antisymmetric (exterior) products are defined as: \\[\n\\omega \\cdot u = \\frac{1}{2}(\\omega u + u \\omega), \\qquad\n\\omega \\wedge u = \\frac{1}{2}(\\omega u - u \\omega)\n\\] These operations extract the grade-0 and grade-2 parts of the Clifford product, respectively. The algebra is defined by the inner products of the basis elements: \\[\n\\e^\\mu \\cdot \\e^\\nu = \\eta^{\\mu\\nu}\n\\] where \\(\\eta_{\\mu\\nu}\\) is the Minkowski metric, used to raise and lower indices, including elements of the algebra and components of (multi)vectors.\nGiven vectors written in component form as \\(\\omega = \\omega^\\mu \\e_\\mu\\) and \\(u = u^\\nu \\e_\\nu\\), the inner product becomes: \\[\n\\omega \\cdot u = \\omega^\\mu u^\\nu \\eta_{\\mu\\nu}\n\\] while the exterior product is: \\[\n\\omega \\wedge u = (\\omega^\\mu u^\\nu - \\omega^\\nu u^\\mu) \\, \\e_\\mu \\wedge \\e_\\nu\n\\] The exterior product is antisymmetric and defines a bivector, corresponding to an oriented plane segment spanned by \\(\\omega\\) and \\(u\\).\nThe Clifford algebra encodes the geometric structure of spacetime, including Lorentz transformations and the behavior of reference frames. By defining the inner product as the symmetric part of the Clifford product, the Minkowski metric emerges directly from the algebraic relations among basis vectors. This allows calculations in special relativity to be carried out through purely algebraic manipulations.\nThe basis vectors satisfy: \\[\n(\\e^0)^2 = \\e^0 \\cdot \\e^0 = -1, \\qquad (\\e^i)^2 = \\e^i \\cdot \\e^i = 1 \\quad (i = 1, 2, 3)\n\\] which reflects the signature of the Minkowski metric \\(\\eta_{\\mu\\nu} = \\mathrm{diag}(-1, 1, 1, 1)\\). For distinct indices, the generators anticommute: \\[\n\\e^\\mu \\e^\\nu = -\\e^\\nu \\e^\\mu \\qquad (\\mu \\neq \\nu)\n\\] These relations define the algebraic structure that underlies spacetime geometry in the context of special relativity.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#bivectors-and-their-algebra",
    "href": "courses/special-relativity/spacetime-algebra.html#bivectors-and-their-algebra",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Bivectors and Their Algebra",
    "text": "Bivectors and Their Algebra\nBivectors are grade-2 elements constructed as wedge products of vectors. They span a 6-dimensional subspace of \\(\\mathrm{Cl}(1,3)\\), with basis elements of the form \\(\\e^\\mu \\wedge \\e^\\nu\\) for \\(\\mu &lt; \\nu\\). By construction, they are antisymmetric: \\[\n\\e^\\mu \\wedge \\e^\\nu = -\\e^\\nu \\wedge \\e^\\mu\n\\]\nThe square of a bivector depends on the causal character of the plane it spans. Bivectors involving the timelike basis vector \\(\\e^0\\) (e.g., \\(\\e^0 \\wedge \\e^1\\)) are called timelike and satisfy: \\[\n(\\e^0 \\wedge \\e^1)^2 = 1\n\\] In contrast, bivectors composed of spatial vectors only (e.g., \\(\\e^1 \\wedge \\e^2\\)) are spacelike and satisfy: \\[\n(\\e^1 \\wedge \\e^2)^2 = -1\n\\]\nWhen manipulating bivectors, it is useful to note that for orthogonal vectors \\(\\e^\\mu\\) and \\(\\e^\\nu\\), the wedge and Clifford products coincide: \\[\n\\e^\\mu \\wedge \\e^\\nu = \\e^\\mu \\e^\\nu \\quad (\\mu \\neq \\nu).\n\\] For example, \\[\n(\\e^0 \\wedge \\e^1)^2 = (\\e^0 \\e^1)^2 = \\e^0 \\e^1 \\e^0 \\e^1.\n\\] Using anticommutation, \\(\\e^1 \\e^0 = -\\e^0 \\e^1\\), we have: \\[\n\\e^0 \\e^1 \\e^0 \\e^1 = -\\e^0 \\e^0 \\e^1 \\e^1 = - (\\e^0)^2 (\\e^1)^2.\n\\] Since \\((\\e^0)^2 = -1\\) and \\((\\e^1)^2 = 1\\), it follows that: \\[\n(\\e^0 \\wedge \\e^1)^2 = 1.\n\\] This illustrates the general procedure for computing with multivectors: expand the product, apply anticommutation relations, and use the metric to evaluate contractions. This approach extends naturally to more complex expressions involving elements of arbitrary grade.\nThis distinction reflects the signature of the Minkowski metric and divides the six basis bivectors into two classes: three with positive square (timelike) and three with negative square (spacelike). Bivectors play a central role in representing infinitesimal Lorentz transformations, with spacelike bivectors generating spatial rotations and timelike bivectors generating boosts.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#trivectors",
    "href": "courses/special-relativity/spacetime-algebra.html#trivectors",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Trivectors",
    "text": "Trivectors\nTrivectors are grade-3 elements formed by wedge products of three distinct vectors. They span a 4-dimensional subspace of \\(\\mathrm{Cl}(1,3)\\) and are fully antisymmetric. A basis consists of one purely spatial trivector, \\[\n\\e^1 \\wedge \\e^2 \\wedge \\e^3,\n\\] and three mixed (temporal) trivectors, \\[\n\\e^0 \\wedge \\e^1 \\wedge \\e^2, \\qquad \\e^0 \\wedge \\e^2 \\wedge \\e^3, \\qquad \\e^0 \\wedge \\e^1 \\wedge \\e^3.\n\\]\nTheir squared norms reflect the causal nature of the planes they span. The purely spatial trivector satisfies: \\[\n(\\e^1 \\wedge \\e^2 \\wedge \\e^3)^2 = -1,\n\\] while temporal trivectors satisfy: \\[\n(\\e^0 \\wedge \\e^1 \\wedge \\e^2)^2 = 1.\n\\]\nTrivectors arise naturally in Clifford products involving three vectors, such as \\(\\e^0 \\e^1 \\e^2\\), and require careful attention to sign conventions due to their antisymmetry, e.g., \\[\n\\e^0 \\e^1 \\e^2 = -\\e^1 \\e^0 \\e^2.\n\\]\nAlthough trivectors have fewer direct physical interpretations than bivectors, they are useful in defining duals and in describing oriented volumes in spacetime.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#pseudoscalar",
    "href": "courses/special-relativity/spacetime-algebra.html#pseudoscalar",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Pseudoscalar",
    "text": "Pseudoscalar\nThe pseudoscalar is the unique grade-4 element of \\(\\mathrm{Cl}(1,3)\\): \\[\nI = \\e^0 \\wedge \\e^1 \\wedge \\e^2 \\wedge \\e^3 = \\e^0 \\e^1 \\e^2 \\e^3, \\qquad I^2 = 1.\n\\] It represents the oriented 4-volume element in spacetime—not to be confused with the usual 3-dimensional spatial volume, and plays a central role in the geometric structure of the algebra.\nThe pseudoscalar anticommutes with all 1-vectors: \\[\nI \\e^\\mu = -\\e^\\mu I,\n\\] and acts as the generator of Hodge duality via multiplication. For example: - \\(I \\e^\\mu\\) is a trivector, - \\(I (\\e^\\mu \\wedge \\e^\\nu)\\) yields a bivector, - \\(I \\cdot 1 = I\\) maps scalars to pseudoscalars.\nAs the oriented volume element, \\(I\\) performs the same operation as the Hodge star map, which is traditionally defined on components via contraction with the Levi-Civita symbol. For instance, \\[\n\\e^1 I = \\e^1 (\\e^0 \\e^1 \\e^2 \\e^3) = -\\e^0 \\e^2 \\e^3\n\\] maps the vector \\(\\e^1\\) to its dual trivector. This mirrors the way a cross product maps two vectors to a third in 3D, or more generally, how a \\(k\\)-vector is mapped to an \\((n{-}k)\\)-vector in \\(n\\)-dimensional Hodge duality.\nIn \\(\\mathrm{Cl}(1,3)\\), \\(I\\) implements the following dualities: - Scalars \\(\\leftrightarrow\\) pseudoscalars, - Vectors \\(\\leftrightarrow\\) trivectors, - Bivectors \\(\\to\\) bivectors (interchanging timelike and spacelike types).\nThis operation will be shown explicitly in later sections and is especially useful in formulating Lorentz-covariant quantities such as the electromagnetic field.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#additional-notes",
    "href": "courses/special-relativity/spacetime-algebra.html#additional-notes",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Additional Notes",
    "text": "Additional Notes\nThe Clifford algebra \\(\\mathrm{Cl}(1,3)\\) consists of 16 basis elements: 1 scalar, 4 vectors, 6 bivectors, 4 trivectors, and 1 pseudoscalar. This structure is isomorphic to the \\(4 \\times 4\\) matrix algebra used in Dirac’s formulation of quantum mechanics, providing a natural framework for relativistic physics. Scalars are Lorentz-invariant, vectors transform as four-vectors under Lorentz transformations, and bivectors correspond to antisymmetric rank-2 tensors, such as the electromagnetic field tensor \\(F^{\\mu\\nu}\\). The pseudoscalar \\(I = \\e^0 \\wedge \\e^1 \\wedge \\e^2 \\wedge \\e^3\\) represents the oriented 4-volume element of Minkowski spacetime and remains invariant under proper Lorentz transformations (determinant 1).\nFor computations in \\(\\mathrm{Cl}(1,3)\\), the following algebraic rules are essential:\n\nThe squares of basis vectors reflect the Minkowski metric: \\((\\e^0)^2 = -1\\), \\((\\e^i)^2 = 1\\) for spatial indices \\(i = 1, 2, 3\\).\nBasis vectors anticommute for distinct indices: \\(\\e^\\mu \\e^\\nu = -\\e^\\nu \\e^\\mu\\) when \\(\\mu \\neq \\nu\\).\nPermutations of generators in multivector products introduce sign changes due to antisymmetry, e.g., \\(\\e^0 \\e^1 \\e^2 \\e^3 = -\\e^1 \\e^0 \\e^2 \\e^3\\).\n\nThese properties enable the algebraic derivation of key results in special relativity, such as Lorentz transformations, invariant intervals, and spacetime rotations, by manipulating multivectors within the \\(\\mathrm{Cl}(1,3)\\) framework. This approach unifies geometric and algebraic operations, simplifying calculations in relativistic physics.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#duality-relations",
    "href": "courses/special-relativity/spacetime-algebra.html#duality-relations",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Duality Relations",
    "text": "Duality Relations\nThe pseudoscalar \\[\nI = \\e^0 \\wedge \\e^1 \\wedge \\e^2 \\wedge \\e^3 = \\e^0 \\e^1 \\e^2 \\e^3\n\\] implements the Hodge duality operation in \\(\\mathrm{Cl}(1,3)\\), mapping \\(k\\)-vectors to \\((4-k)\\)-vectors. In particular, multiplication by \\(I\\) maps bivectors to their dual bivectors. For the basis bivectors, the duality relations are: \\[\n\\begin{aligned}\nI (\\e^0 \\e^1) &= \\e^2 \\e^3, \\\\\nI (\\e^0 \\e^2) &= -\\e^1 \\e^3, \\\\\nI (\\e^0 \\e^3) &= \\e^1 \\e^2, \\\\\nI (\\e^1 \\e^2) &= -\\e^0 \\e^3, \\\\\nI (\\e^1 \\e^3) &= \\e^0 \\e^2, \\\\\nI (\\e^2 \\e^3) &= -\\e^0 \\e^1.\n\\end{aligned}\n\\] These follow from anticommutation relations and the property \\(I \\e^\\mu = -\\e^\\mu I\\). For example, \\[\nI (\\e^0 \\e^1) = (\\e^0 \\e^1 \\e^2 \\e^3)(\\e^0 \\e^1) = \\e^2 \\e^3,\n\\] where anticommutation and metric contractions are used.\n\nDuality in 3D\nWithin the 3D spatial subalgebra of \\(\\mathrm{Cl}(1,3)\\) generated by \\[\n\\sigma^i \\equiv \\e^0 \\e^i, \\quad i=1,2,3,\n\\] the spatial pseudoscalar \\[\nI_3 = \\sigma^1 \\sigma^2 \\sigma^3 = I\n\\] defines a 3D duality operation analogous to the cross product. This subalgebra uses timelike-spatial bivectors \\(\\sigma^i\\), which satisfy \\[\n\\sigma^i \\sigma^j = -\\sigma^j \\sigma^i, \\quad (\\sigma^i)^2 = 1,\n\\] and their symmetric product reproduces the standard Euclidean inner product: \\[\n\\sigma^i \\cdot \\sigma^j \\equiv \\frac{\\sigma^i \\sigma^j + \\sigma^j \\sigma^i}{2} = \\delta^{ij}.\n\\]\nThe 3D cross product emerges naturally as \\[\nu \\times v \\equiv -I (u \\wedge v),\n\\] for vectors \\(u,v \\in \\mathrm{span}\\{\\sigma^1, \\sigma^2, \\sigma^3\\}\\). For example, \\[\n\\begin{aligned}\n\\sigma^1 \\times \\sigma^2 &= -I (\\sigma^1 \\wedge \\sigma^2) = \\sigma^3, \\\\\n\\sigma^1 \\times \\sigma^3 &= -I (\\sigma^1 \\wedge \\sigma^3) = -\\sigma^2, \\\\\n\\sigma^2 \\times \\sigma^3 &= -I (\\sigma^2 \\wedge \\sigma^3) = \\sigma^1.\n\\end{aligned}\n\\] Here, the wedge product \\(\\sigma^i \\wedge \\sigma^j\\) is a bivector, and multiplication by \\(I\\) maps it back to a vector, revealing the bivector nature of the cross product.\nThis framework provides a natural algebraic interpretation of the magnetic field in electromagnetism as a bivector transforming under spatial rotations consistent with the duality \\(I_3\\), unifying the cross product with spacetime geometry.\n\n\nContraction with the Pseudoscalar\nContraction with the pseudoscalar \\(I\\) in \\(\\mathrm{Cl}(1,3)\\) implements the Hodge dual operation algebraically. For a \\(k\\)-vector \\(A\\), the product \\(A I\\) produces a \\((4-k)\\)-vector, preserving the graded structure of the algebra.\nSpecific examples include: - Scalars \\(c \\in \\mathbb{R}\\) map to pseudoscalars via \\(c I = c\\, \\e^0 \\e^1 \\e^2 \\e^3\\). - Vectors transform into trivectors, e.g., \\[\n  I \\e^\\mu = (\\e^0 \\e^1 \\e^2 \\e^3) \\e^\\mu, \\quad \\text{with} \\quad I \\e^1 = \\e^0 \\e^2 \\e^3.\n  \\] - Bivectors map to bivectors with timelike and spacelike components interchanged, as discussed previously. - Trivectors map to vectors: \\[\n  I (\\e^\\mu \\e^\\nu \\e^\\rho) = \\pm \\e^\\sigma,\n  \\] where \\(\\e^\\sigma\\) is the complementary basis vector completing the 4-vector basis.\nThis multiplication by \\(I\\) corresponds to the Hodge star operator on differential forms, but here it is realized entirely through algebraic multiplication, eliminating the need for component-wise contractions with the Levi-Civita tensor.\nFor example, the electromagnetic field tensor \\[\nF = \\frac{1}{2} F_{\\mu\\nu} \\e^\\mu \\e^\\nu\n\\] is mapped to its dual \\[\n\\star F = I F,\n\\] which exchanges electric and magnetic components. This duality plays a central role in expressing Maxwell’s equations covariantly in spacetime.\nThe algebraic simplicity of contraction with \\(I\\) exemplifies the unifying power of \\(\\mathrm{Cl}(1,3)\\) in linking geometric and physical structures.\n\n\nContraction with the Pseudoscalar\nFor a vector \\(v = v^\\mu \\e_\\mu\\), contraction with the pseudoscalar \\(I\\) produces its dual trivector: \\[\nI v = I (v^\\mu \\e_\\mu) = -\\frac{v^\\mu}{3!} \\epsilon_{\\mu\\nu\\rho\\sigma} \\, \\e^\\nu \\e^\\rho \\e^\\sigma,\n\\] where \\(\\epsilon_{0123} = 1\\) is the totally antisymmetric Levi-Civita symbol. This expresses the Hodge dual of \\(v\\) as a trivector in \\(\\mathrm{Cl}(1,3)\\).\n\n\nVector Notation and Tensor Relations\nGeometric objects in \\(\\mathrm{Cl}(1,3)\\) can be represented with tensor components as follows: - Trivector: \\[\n  T = \\frac{1}{3!} T^{\\mu\\nu\\alpha} \\e_\\mu \\wedge \\e_\\nu \\wedge \\e_\\alpha,\n  \\] - Bivector: \\[\n  B = \\frac{1}{2!} B^{\\mu\\nu} \\e_\\mu \\wedge \\e_\\nu,\n  \\] - Pseudoscalar: \\[\n  \\alpha I, \\quad \\alpha \\in \\mathbb{R}.\n  \\]\nThe geometric product of vectors \\(v = v^\\mu \\e_\\mu\\) and \\(w = w^\\nu \\e_\\nu\\) decomposes into scalar (inner) and bivector (exterior) parts: \\[\nv w = v^\\mu w^\\nu \\left(\\e_\\mu \\cdot \\e_\\nu + \\e_\\mu \\wedge \\e_\\nu\\right).\n\\]\nThe Faraday tensor \\(F\\), encoding the electromagnetic field, is naturally a bivector in \\(\\mathrm{Cl}(1,3)\\): \\[\nF = \\frac{1}{2} F_{\\mu\\nu} \\e^\\mu \\wedge \\e^\\nu.\n\\] Its components combine electric fields (associated with bivectors \\(\\e^0 \\wedge \\e^i\\)) and magnetic fields (associated with spatial bivectors \\(\\e^i \\wedge \\e^j\\)), totaling six independent components. Under Lorentz transformations, electric and magnetic parts mix, but \\(F\\) as a bivector remains invariant, unifying electric and magnetic fields within the algebraic framework of spacetime geometry.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#algebra-of-bivectors",
    "href": "courses/special-relativity/spacetime-algebra.html#algebra-of-bivectors",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Algebra of Bivectors",
    "text": "Algebra of Bivectors\nIf two bivectors \\(\\omega\\) and \\(u\\) satisfy \\(\\omega \\wedge u = 0\\), they commute under the Clifford product: \\[\n\\omega u = u \\omega.\n\\] Conversely, if they are orthogonal in the sense that their inner product vanishes, \\(\\omega \\cdot u = 0\\), they anticommute: \\[\n\\omega u = -u \\omega.\n\\] In this case, their product squares as \\[\n(\\omega u)^2 = -\\omega^2 u^2,\n\\] with the sign depending on the signatures of \\(\\omega^2\\) and \\(u^2\\): - If \\(\\omega^2 &lt; 0\\) and \\(u^2 &gt; 0\\), then \\(-\\omega^2 u^2 &gt; 0\\). - If both \\(\\omega^2 &lt; 0\\) and \\(u^2 &lt; 0\\), then \\(-\\omega^2 u^2 &lt; 0\\).\nThe exponential of a bivector generates Lorentz transformations: \\[\ne^{\\omega \\wedge u} = \\cosh\\left(\\sqrt{-\\omega^2 u^2}\\right) + \\frac{\\omega \\wedge u}{\\sqrt{-\\omega^2 u^2}} \\sinh\\left(\\sqrt{-\\omega^2 u^2}\\right).\n\\]\nBivectors generate boosts or rotations depending on their causal character. For instance, the timelike bivector \\(\\e^0 \\wedge \\e^1\\), with \\((\\e^0 \\wedge \\e^1)^2 = 1\\), generates boosts: \\[\n\\e^{\\lambda (\\e^0 \\wedge \\e^1)} = \\cosh \\lambda + (\\e^0 \\wedge \\e^1) \\sinh \\lambda.\n\\] Applying this boost to the vector \\(w = \\e^0\\) gives \\[\n\\e^{-\\lambda (\\e^0 \\wedge \\e^1)} \\e^0 \\e^{\\lambda (\\e^0 \\wedge \\e^1)} = (\\cosh \\lambda) \\e^0 + (\\sinh \\lambda) \\e^1,\n\\] corresponding to a Lorentz boost along \\(\\e^1\\) with \\(\\cosh \\lambda = \\gamma\\), \\(\\sinh \\lambda = \\gamma \\beta\\).\nIn contrast, a spacelike bivector such as \\(\\e^1 \\wedge \\e^2\\), with \\((\\e^1 \\wedge \\e^2)^2 = -1\\), generates spatial rotations: \\[\n\\e^{\\lambda (\\e^1 \\wedge \\e^2)} = \\cos \\lambda + (\\e^1 \\wedge \\e^2) \\sin \\lambda.\n\\]",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#electromagnetism-in-clifford-algebra",
    "href": "courses/special-relativity/spacetime-algebra.html#electromagnetism-in-clifford-algebra",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Electromagnetism in Clifford Algebra",
    "text": "Electromagnetism in Clifford Algebra\nThe electromagnetic field is naturally represented by the Faraday bivector \\[\nF = \\frac{1}{2} F_{\\mu\\nu} \\e^\\mu \\wedge \\e^\\nu,\n\\] which contains six components: three electric (associated with bivectors \\(\\e^0 \\wedge \\e^i\\)) and three magnetic (associated with \\(\\e^i \\wedge \\e^j\\)). Under Lorentz transformations, electric and magnetic parts mix, but the bivector \\(F\\) itself remains invariant. This formalism unifies electric and magnetic fields as frame-dependent projections of the electromagnetic field tensor, simplifying the formulation of electromagnetism and related calculations such as the Lorentz force. Further applications will be developed in the course.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#exercises",
    "href": "courses/special-relativity/spacetime-algebra.html#exercises",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Exercises",
    "text": "Exercises\nTo improve your understanding of \\(\\mathrm{Cl}(1,3)\\), work through these exercises:\n\nSquares of Basis Bivectors\nCompute the square of the following bivectors and classify each as timelike, spacelike, or lightlike:\n\n\\((\\e^0 \\wedge \\e^1)^2\\)\n\n\\((\\e^1 \\wedge \\e^2)^2\\)\n\n\\((\\e^2 \\wedge \\e^3)^2\\)\n\n\\((\\e^0 \\wedge \\e^2)^2\\)\n\nDuals with the Pseudoscalar\nCalculate the duals of bivectors by multiplying with the pseudoscalar \\(I\\):\n\n\\(I (\\e^0 \\e^1)\\)\n\n\\(I (\\e^1 \\e^2)\\)\n\n\\(I (\\e^0 \\e^2)\\)\n\n\\(I (\\e^2 \\e^3)\\)\nVerify that these results are consistent with the duality relations in \\(\\mathrm{Cl}(1,3)\\).\n\nSquares of Trivectors\nCompute the squares of these trivectors and determine their causal character:\n\n\\((\\e^0 \\wedge \\e^1 \\wedge \\e^2)^2\\)\n\n\\((\\e^1 \\wedge \\e^2 \\wedge \\e^3)^2\\)\n\nPseudoscalar Commutation\nVerify the anticommutation relation with the pseudoscalar for each basis vector:\n\\[\nI \\e^\\mu = - \\e^\\mu I, \\quad \\mu=0,1,2,3.\n\\]\nIdentify the grade and type of the multivector \\(I \\e^\\mu\\).\nExponential of a Bivector\nFor \\(\\omega = \\e^0\\), \\(u = \\e^1\\):\n\nCompute the wedge product \\(\\omega \\wedge u\\).\n\nCalculate \\((\\omega \\wedge u)^2\\).\n\nWrite down the exponential \\(\\e^{\\omega \\wedge u}\\) and interpret whether it corresponds to a boost or a rotation.\n\nAnticommutators and Commutators\nCalculate and interpret:\n\nThe anticommutator \\(\\e^0 \\e^1 + \\e^1 \\e^0\\).\n\nThe commutator \\(\\e^1 \\e^2 - \\e^2 \\e^1\\).\nRelate these to the inner (symmetric) and exterior (antisymmetric) products of vectors.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#inversion-and-hermitian-conjugates",
    "href": "courses/special-relativity/spacetime-algebra.html#inversion-and-hermitian-conjugates",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Inversion and Hermitian Conjugates",
    "text": "Inversion and Hermitian Conjugates\nIn \\(\\mathrm{Cl}(1,3)\\), the reversion (also called transpose) is an involutive operation defined on multivectors by: \\[\n(ab)^\\mathrm{T} = b^\\mathrm{T} a^\\mathrm{T},\n\\] for any multivectors \\(a\\) and \\(b\\). Reversion reverses the order of the product but leaves individual vectors and scalars unchanged: \\[\n(\\e^\\mu)^\\mathrm{T} = \\e^\\mu, \\quad (\\alpha)^\\mathrm{T} = \\alpha, \\quad \\text{for any } \\alpha \\in \\mathbb{R}.\n\\]\nFor the pseudoscalar \\(I = \\e^0 \\e^1 \\e^2 \\e^3\\): \\[\nI^\\mathrm{T} = (\\e^0 \\e^1 \\e^2 \\e^3)^\\mathrm{T} = \\e^3 \\e^2 \\e^1 \\e^0 = I,\n\\] since reversing all four basis vectors introduces an even number of sign changes.\nFor a bivector \\(B = \\frac{1}{2} B_{\\mu\\nu} \\e^\\mu \\wedge \\e^\\nu\\), the reversion changes the sign: \\[\nB^\\mathrm{T} = \\frac{1}{2} B_{\\mu\\nu} (\\e^\\nu \\wedge \\e^\\mu) = -\\frac{1}{2} B_{\\mu\\nu} \\e^\\mu \\wedge \\e^\\nu = -B,\n\\] reflecting the antisymmetry of bivectors under reversion.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#exponentials-of-bivectors",
    "href": "courses/special-relativity/spacetime-algebra.html#exponentials-of-bivectors",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Exponentials of Bivectors",
    "text": "Exponentials of Bivectors\nThe exponential of a bivector \\(B\\) is defined through the usual power series: \\[\ne^{\\alpha B} = \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} B^n,\n\\] with \\(\\alpha \\in \\mathbb{R}\\). The closed form depends on the square of \\(B\\), and can be derived by separating the even and odd terms:\n\nFor \\(B^2 = +1\\), (e.g., \\(B = \\e^0 \\wedge \\e^1\\)) the exponential is given by: \\[\n\\begin{aligned}\ne^{\\alpha B} &= \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} B^n \\\\\n             &= \\sum_{n=0}^{\\infty} \\left( \\frac{\\alpha^{2n}}{(2n)!} B^{2n} + \\frac{\\alpha^{2n+1}}{(2n+1)!} B^{2n+1} \\right) \\\\\n             &= \\sum_{n=0}^{\\infty} \\frac{\\alpha^{2n}}{(2n)!} + B \\sum_{n=0}^{\\infty} \\frac{\\alpha^{2n+1}}{(2n+1)!} \\\\\n             &= \\cosh(\\alpha) + B \\sinh(\\alpha).\n\\end{aligned}\n\\]\nFor \\(B^2 = -1\\), (e.g., \\(B = \\e^2 \\wedge \\e^3\\)) the exponential is given by: \\[\n\\begin{aligned}\ne^{\\alpha B} &= \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} B^n \\\\\n             &= \\sum_{n=0}^{\\infty} \\left( \\frac{\\alpha^{2n}}{(2n)!} B^{2n} + \\frac{\\alpha^{2n+1}}{(2n+1)!} B^{2n+1} \\right) \\\\\n             &= \\sum_{n=0}^{\\infty} \\frac{(-1)^n \\alpha^{2n}}{(2n)!} + B \\sum_{n=0}^{\\infty} \\frac{(-1)^n \\alpha^{2n+1}}{(2n+1)!} \\\\\n             &= \\cos(\\alpha) + B \\sin(\\alpha).\n\\end{aligned}\n\\]\n\n\nLorentz Transformations with Rotors\nLorentz transformations are implemented using rotors: \\[\nR = e^{\\alpha B / 2},\n\\] where \\(B\\) is a bivector (e.g., \\(\\e^0 \\wedge \\e^1\\) for boosts or \\(\\e^1 \\wedge \\e^2\\) for rotations), and \\(\\alpha \\in \\mathbb{R}\\) is a transformation parameter. The rotor acts on a vector \\(u\\) via the sandwich product: \\[\nu \\mapsto R u R^\\mathrm{T}, \\quad \\text{with} \\quad R^\\mathrm{T} = e^{-\\alpha B / 2}.\n\\]\nThis ensures that the result is a vector, since: \\[\n(R u R^\\mathrm{T})^\\mathrm{T} = R u R^\\mathrm{T}.\n\\] In contrast, a simple left multiplication like \\(R u\\) is not a vector: \\[\n(R u)^\\mathrm{T} = u R^\\mathrm{T} \\neq R u.\n\\]\nThe use of both \\(R\\) and \\(R^\\mathrm{T}\\) in the transformation law necessitates the factor \\(\\alpha/2\\) in the exponent, ensuring the correct group composition law: \\[\nR(\\alpha_1) R(\\alpha_2) = R(\\alpha_1 + \\alpha_2).\n\\]\n\n\nSpinors and Lorentz Rotations\nRotors exhibit spinorial behavior. For example, a \\(2\\pi\\) rotation gives: \\[\nR(2\\pi) = \\cos(\\pi) + B \\sin(\\pi) = -1 = -R(0),\n\\] indicating that a full \\(4\\pi\\) rotation is needed to return to the identity: \\[\nR(4\\pi) = 1.\n\\]\nThis is the hallmark of spin-1/2 behavior. Spinors arise naturally in the even subalgebra of \\(\\mathrm{Cl}(1,3)\\), forming the group \\(\\mathrm{Spin}(1,3)\\), which is a double cover of the Lorentz group \\(\\mathrm{SO}(1,3).\\) The same rotor \\(R\\) that acts on vectors as: \\[\nu \\mapsto R u R^\\mathrm{T},\n\\] acts on spinors \\(\\psi\\) (e.g., four-component complex vectors) as: \\[\n\\psi \\mapsto R \\psi.\n\\] Note, however, that this approach would require introducing a matrix representation of the algebra. While this is a common route, it is not strictly necessary. Instead, we can represent the “vector” \\(\\psi\\) directly as an element of \\(\\mathrm{Cl}(1,3)\\), more precisely, as an even multivector.\nThe pseudoscalar \\(I = \\e_0 \\e_1 \\e_2 \\e_3\\) anticommutes with all vectors and satisfies \\(I^2 = -1\\), playing a role analogous to the complex unit \\(i\\) in the Dirac algebra. In the spinor representation, it appears in constructions involving \\(\\gamma^5\\) and chirality.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#reference-frames-and-4-velocity",
    "href": "courses/special-relativity/spacetime-algebra.html#reference-frames-and-4-velocity",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Reference Frames and 4-Velocity",
    "text": "Reference Frames and 4-Velocity\nAn observer’s 4-velocity is \\(w^\\mu = (1, 0, 0, 0)\\), or \\(w = \\e_0\\), with \\(w^2 = -1\\). A particle’s 4-velocity is: \\[\nu^\\mu = \\left( \\gamma, \\gamma \\frac{v^1}{c}, \\gamma \\frac{v^2}{c}, \\gamma \\frac{v^3}{c} \\right), \\quad \\gamma = (1 - v^2/c^2)^{-1/2},\n\\] so \\(u = u^\\mu \\e_\\mu\\), with: \\[\nu^2 = -\\gamma^2 (1 - v^2/c^2) = -1, \\quad u \\cdot w = -\\gamma.\n\\]\n\nTrajectories and Decomposition\nA spacetime trajectory is: \\[\nx = x^\\mu \\e_\\mu = x^0 \\e_0 + x^i \\e_i, \\quad x \\cdot w = -x^0.\n\\] The spatial projection orthogonal to \\(w = \\e_0\\): \\[\nx_\\perp = \\frac{x + w x w}{2} = x^i \\e_i,\n\\] since: \\[\nw x w = -x^0 \\e_0 + x^i \\e_i, \\quad x + w x w = 2 x^i \\e_i.\n\\]",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#exercises-1",
    "href": "courses/special-relativity/spacetime-algebra.html#exercises-1",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Exercises",
    "text": "Exercises\n\nBasic Comprehension\n\nShow that for \\(u = u^\\mu \\e_\\mu\\), we have \\(u^2 = -1\\) and \\(u \\cdot w = -\\gamma\\) for \\(w = \\e_0\\).\nCompute the spatial projection \\(x_\\perp = \\frac{x + w x w}{2}\\) for \\(x = x^0 \\e_0 + x^1 \\e_1\\), and check that \\(x_\\perp \\cdot w = 0\\).\nLet \\(B = \\e^2 \\wedge \\e^3\\). Compute \\(B^2\\) and verify the closed form of \\(e^{\\theta B}\\).\nProve that for any bivector \\(B\\) and scalar \\(\\alpha\\), we have: \\[\n\\left(e^{\\alpha B}\\right)^\\mathrm{T} = e^{-\\alpha B}.\n\\]\nLet \\(R = e^{\\theta B/2}\\) for a bivector \\(B\\) with \\(B^2 = -1\\). Show that \\(R^\\mathrm{T} = R^{-1}\\).\n\n\n\nApplications\n\nShow that the action \\(u \\mapsto R u R^\\mathrm{T}\\) preserves the norm: \\((R u R^\\mathrm{T})^2 = u^2\\).\nLet \\(B = \\e^0 \\wedge \\e^1\\) and \\(R = e^{\\eta B / 2}\\) with rapidity \\(\\eta\\). Compute \\(R \\e^0 R^\\mathrm{T}\\) and interpret the result as a Lorentz boost along the \\(x^1\\) direction.\nVerify explicitly that a full \\(2\\pi\\) rotation gives \\(R = -1\\) for \\(B = \\e^1 \\wedge \\e^2\\).\nLet \\(x = x^0 \\e_0 + x^1 \\e_1\\) and \\(w = \\e_0\\). Show that the time component of \\(x\\) is recovered by: \\[\nt = -x \\cdot w.\n\\]\nLet \\(B = \\e^1 \\wedge \\e^2\\). Compute \\(R = \\cos(\\theta/2) + B \\sin(\\theta/2)\\) and then compute \\(R^2\\). What is the result? Why does this make sense?\nShow that the pseudoscalar \\(I = \\e_0 \\e_1 \\e_2 \\e_3\\) anticommutes with all vectors: \\(I \\e^\\mu = -\\e^\\mu I\\), and deduce that it also anticommutes with all bivectors.\nLet \\(x = x^\\mu \\e_\\mu\\) and \\(w = \\e_0\\). Show that \\(x = -t w + x_\\perp\\), where \\(t = -x \\cdot w\\) and \\(x_\\perp = \\frac{x + w x w}{2}\\).",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/vector-calculus/index.html",
    "href": "courses/vector-calculus/index.html",
    "title": "Vector Calculus Overview",
    "section": "",
    "text": "This module introduces the mathematical framework of vector calculus, essential for understanding physics in three or more dimensions. It covers fundamental concepts, operations, and applications frequently used across physics courses, especially electromagnetism and mechanics.\n\n\n\nVector Spaces: Definitions, linear independence, and the structure of spaces where vectors live.\nInner Product: Properties of the dot product, including the Kronecker delta notation.\nVector Product: Cross product, exterior product, and their properties.\nTriple Products: Relations involving scalar and vector triple products.\nEuclidean Space: Cartesian coordinates and tangent vectors.\nDifferential Calculus: Gradient, divergence, curl, and higher-order derivatives.\nIntegral Calculus: Line, surface, and volume integrals.\nCurvilinear Coordinates: Basis vectors, differential operators, and orthogonal coordinate systems.\nDirac Delta: Introduction and properties of the delta distribution.\nVector Field Decomposition: Techniques to break down vector fields into simpler components.\n\nThis course aims to build a solid foundation for applying these mathematical tools throughout your physics studies.\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Overview"
    ]
  },
  {
    "objectID": "courses/vector-calculus/index.html#contents",
    "href": "courses/vector-calculus/index.html#contents",
    "title": "Vector Calculus Overview",
    "section": "",
    "text": "Vector Spaces: Definitions, linear independence, and the structure of spaces where vectors live.\nInner Product: Properties of the dot product, including the Kronecker delta notation.\nVector Product: Cross product, exterior product, and their properties.\nTriple Products: Relations involving scalar and vector triple products.\nEuclidean Space: Cartesian coordinates and tangent vectors.\nDifferential Calculus: Gradient, divergence, curl, and higher-order derivatives.\nIntegral Calculus: Line, surface, and volume integrals.\nCurvilinear Coordinates: Basis vectors, differential operators, and orthogonal coordinate systems.\nDirac Delta: Introduction and properties of the delta distribution.\nVector Field Decomposition: Techniques to break down vector fields into simpler components.\n\nThis course aims to build a solid foundation for applying these mathematical tools throughout your physics studies.\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Overview"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/green-functions.html",
    "href": "courses/mathematical-physics/green-functions.html",
    "title": "Green Functions",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Green Functions"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/green-functions.html#greens-functions-in-one-dimension",
    "href": "courses/mathematical-physics/green-functions.html#greens-functions-in-one-dimension",
    "title": "Green Functions",
    "section": "Green’s Functions in One Dimension",
    "text": "Green’s Functions in One Dimension\nGreen’s functions provide a powerful and general method for solving linear differential equations with prescribed boundary conditions. The core idea is to express the solution as a convolution between a Green’s function and a source term. This method is particularly useful in both ordinary and partial differential equations, especially in physics and engineering contexts where fundamental solutions or propagators are central tools.\nThe exercises and derivations that follow develop the concept of Green’s functions from basic examples in one dimension, building toward general properties such as symmetry, boundary behavior, and their role in solving inhomogeneous equations. These one-dimensional constructions are then extended to higher dimensions, where the Green’s functions become essential in solving the Poisson and Laplace equations.\nThe step-by-step approach mirrors the structure used in previous sections, especially those on the Dirac delta distribution, highlighting how distributions and fundamental solutions are interconnected. This transition from delta sequences to Green’s functions reveals the deeper structure behind linear differential operators and their inverses.",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Green Functions"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/green-functions.html#exercises",
    "href": "courses/mathematical-physics/green-functions.html#exercises",
    "title": "Green Functions",
    "section": "Exercises",
    "text": "Exercises\n\nThe inverse of the derivative of a function \\(f(x)\\) for \\(x \\in (a, b)\\), with \\(f(a) = 0 = f(b)\\), can be written as: \\[\nf(x) = \\int_{a}^{x}f'(y)\\,\\mathrm{d}y, \\qquad\nf(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\int_{a}^{x}f(y)\\,\\mathrm{d}y.\n\\]\n\nShow how to rewrite these expressions using the Heaviside step function \\(\\theta(x)\\): \\[\n\\theta(x) = \\begin{cases}\n0, & x &lt; 0, \\\\\n1, & x \\geq 0.\n\\end{cases}\n\\] That is, \\(\\theta(x)\\) acts as the Green’s function for the first derivative: \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}x} \\int_{a}^{b} \\theta(x-y)f(y)\\,\\mathrm{d}y = f(x).\n\\]\nCompute \\[\nG(x-y) \\equiv \\int_{a}^{x}\\theta(z-y)\\,\\mathrm{d}z,\\qquad y \\in (a, b).\n\\] and show that \\(G(x)\\) is the Green’s function for the second derivative: \\[\n\\frac{\\mathrm{d}^2}{\\mathrm{d}x^2} \\int_{a}^{b} G(x - y) f(y)\\,\\mathrm{d}y = f(x).\n\\]\nProve the inverse relation: \\[\nf(x) = \\int_{a}^{b} G(x - y) \\frac{\\mathrm{d}^2}{\\mathrm{d}y^2}f(y)\\,\\mathrm{d}y.\n\\]\nExpress \\(G(x, y)\\) in terms of \\(\\sigma(x, y) = |x - y|\\).\n\nOne-dimensional Green’s functions. Derive Green’s identities for the operator \\(\\hat{L} = -\\dfrac{\\mathrm{d}^2}{\\mathrm{d}x^2}\\) on the interval \\((a, b)\\), assuming \\(\\phi(a) = \\phi(b) = 0\\).\n\nStart by verifying the product rule: \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}x} \\left(\\phi \\frac{\\mathrm{d}\\psi}{\\mathrm{d}x}\\right) =\n\\frac{\\mathrm{d}\\phi}{\\mathrm{d}x} \\frac{\\mathrm{d}\\psi}{\\mathrm{d}x} +\n\\phi \\frac{\\mathrm{d}^2\\psi}{\\mathrm{d}x^2}.\n\\]\nIntegrate both sides over \\((a, b)\\) and use the boundary condition \\(\\phi(a) = \\phi(b) = 0\\) to show: \\[\n\\int_a^b \\phi(x) \\frac{\\mathrm{d}^2\\psi}{\\mathrm{d}x^2}(x)\\,\\mathrm{d}x =\n- \\int_a^b \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}(x) \\frac{\\mathrm{d}\\psi}{\\mathrm{d}x}(x)\\,\\mathrm{d}x.\n\\] This is Green’s first identity.\nSwap \\(\\phi\\) and \\(\\psi\\), perform the same computation, and subtract the two identities to obtain Green’s second identity: \\[\n\\int_a^b \\left[\\phi(x) \\frac{\\mathrm{d}^2\\psi}{\\mathrm{d}x^2}(x) -\n\\psi(x) \\frac{\\mathrm{d}^2\\phi}{\\mathrm{d}x^2}(x)\\right]\\,\\mathrm{d}x = 0.\n\\]\nNow assume \\(\\psi(x) = G(x, y)\\) is the Green’s function for the operator, and let \\(\\phi(x)\\) be a given function. Use the identity above to obtain the formal solution of the equation \\[\n- \\frac{\\mathrm{d}^2}{\\mathrm{d}x^2} f(x) = s(x)\n\\] in terms of the Green’s function: \\[\nf(x) = \\int_a^b G(x, y) s(y)\\, \\mathrm{d}y.\n\\]\nUsing the previous result, show that the Green’s function is: \\[\nG(x - y) = \\frac{|x - y|}{2} + \\alpha(x - y) + \\beta.\n\\]\nDetermine \\(\\alpha\\) and \\(\\beta\\) by imposing the boundary conditions \\(G(x - y) = 0\\) at \\(x = a\\) and \\(x = b\\) for fixed \\(y \\in (a, b)\\).\nUse the Green’s function to solve \\(-\\dfrac{\\mathrm{d}^2}{\\mathrm{d}x^2}f(x) = 1\\) on \\((a, b)\\) and show that \\[\nf(x) = -\\frac{1}{2}(x - a)(x - b).\n\\]",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Green Functions"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/green-functions.html#greens-functions-in-multiple-dimensions",
    "href": "courses/mathematical-physics/green-functions.html#greens-functions-in-multiple-dimensions",
    "title": "Green Functions",
    "section": "Green’s Functions in Multiple Dimensions",
    "text": "Green’s Functions in Multiple Dimensions\nThe concepts developed in one dimension generalize naturally to higher dimensions, where Green’s functions play a central role in solving elliptic partial differential equations such as the Poisson and Laplace equations. In multiple dimensions, the Green’s function depends on the spatial distance between points and reflects the geometry of the underlying space.\nThis section explores how the Laplacian operator behaves under these generalizations, how to regularize singular Green’s functions, and how Green’s identities lead to integral representations of solutions. These results form the basis for classical potential theory and have applications in fields ranging from electrostatics to quantum mechanics.",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Green Functions"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/green-functions.html#exercises-1",
    "href": "courses/mathematical-physics/green-functions.html#exercises-1",
    "title": "Green Functions",
    "section": "Exercises",
    "text": "Exercises\n\nThe Green’s function for the second derivative in one dimension is proportional to the distance \\(\\sigma(x, y) = |x - y|\\). In three dimensions, define: \\[\n\\sigma(\\vec{x}, \\vec{y}) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (x_3 - y_3)^2},\n\\] where \\(\\vec{x} = (x_1, x_2, x_3)\\) and \\(\\vec{y} = (y_1, y_2, y_3)\\).\n\nShow that the Laplacian in \\(\\mathbb{R}^3\\), \\[\n\\vec{\\nabla}^2 = \\frac{\\partial^2}{\\partial x_1^2} + \\frac{\\partial^2}{\\partial x_2^2} + \\frac{\\partial^2}{\\partial x_3^2},\n\\] satisfies: \\[\n\\vec{\\nabla}^2 \\left(\\frac{1}{\\sigma(\\vec{x}, \\vec{y})}\\right) = 0 \\quad \\text{for } \\vec{x} \\neq \\vec{y}.\n\\]\nRegularize the Green’s function by introducing \\(\\varepsilon &gt; 0\\): \\[\nG_\\varepsilon(\\vec{x}, \\vec{y}) =\n-\\frac{1}{4\\pi} \\frac{1}{\\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (x_3 - y_3)^2 + \\varepsilon^2}}.\n\\] Show that: \\[\n\\lim_{\\varepsilon \\to 0} \\int_{\\mathbb{R}^3} \\vec{\\nabla}^2 G_\\varepsilon(\\vec{x}, \\vec{y})\\,\\mathrm{d}^3\\vec{x} = 1.\n\\]\n\nDerive Green’s identities for the Laplacian.\n\nDerive the following Green’s identities: \\[\n\\oint_{\\partial\\Omega} \\phi\\,\\vec{\\nabla}\\psi \\cdot \\mathrm{d}\\vec{S}\n= \\int_{\\Omega} \\left(\\vec{\\nabla}\\phi \\cdot \\vec{\\nabla}\\psi + \\phi\\,\\vec{\\nabla}^2\\psi\\right)\\,\\mathrm{d}^3\\vec{x},\n\\] \\[\n\\oint_{\\partial\\Omega} (\\phi\\,\\vec{\\nabla}\\psi - \\psi\\,\\vec{\\nabla}\\phi) \\cdot \\mathrm{d}\\vec{S}\n= \\int_{\\Omega} \\left(\\phi\\,\\vec{\\nabla}^2\\psi - \\psi\\,\\vec{\\nabla}^2\\phi\\right)\\,\\mathrm{d}^3\\vec{x},\n\\] where \\(\\Omega\\) is a volume bounded by the surface \\(\\partial\\Omega\\).\nShow how to use the second Green’s identity together with the Green’s function for the Laplacian to obtain the solution to the Poisson equation.",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Green Functions"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/dirac-delta.html",
    "href": "courses/mathematical-physics/dirac-delta.html",
    "title": "Dirac Delta",
    "section": "",
    "text": "The Dirac delta distribution is a central object in mathematical physics. Although not a function in the classical sense, it can be rigorously defined as the limit of a sequence of well-behaved functions. One common construction uses the normal (Gaussian) distribution with vanishing variance. In this section, we explore this limiting process and use it to derive properties of the delta distribution.",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Dirac Delta"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/dirac-delta.html#dirac-delta",
    "href": "courses/mathematical-physics/dirac-delta.html#dirac-delta",
    "title": "Dirac Delta",
    "section": "",
    "text": "The Dirac delta distribution is a central object in mathematical physics. Although not a function in the classical sense, it can be rigorously defined as the limit of a sequence of well-behaved functions. One common construction uses the normal (Gaussian) distribution with vanishing variance. In this section, we explore this limiting process and use it to derive properties of the delta distribution.",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Dirac Delta"
    ]
  },
  {
    "objectID": "courses/mathematical-physics/dirac-delta.html#exercises",
    "href": "courses/mathematical-physics/dirac-delta.html#exercises",
    "title": "Dirac Delta",
    "section": "Exercises",
    "text": "Exercises\n\nConsider the normal distribution: \\[\np_\\sigma(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-x^2 / (2\\sigma^2)},\n\\] where \\(\\sigma &gt; 0\\) is the standard deviation.\n\nShow that \\(p_\\sigma(x)\\) is normalized: \\[\n\\int_{-\\infty}^{\\infty} p_\\sigma(x)\\,\\mathrm{d}x = 1.\n\\]\nCompute the expected values of odd powers of \\(x\\): \\[\n\\left\\langle x^{2n+1} \\right\\rangle = \\int_{-\\infty}^{\\infty} x^{2n+1} p_\\sigma(x)\\,\\mathrm{d}x.\n\\]\nCompute the expected values of even powers of \\(x\\): \\[\n\\left\\langle x^{2n} \\right\\rangle = \\int_{-\\infty}^{\\infty} x^{2n} p_\\sigma(x)\\,\\mathrm{d}x.\n\\]\nUse the results above to show: \\[\n\\int_{-\\infty}^{\\infty} f(x) p_\\sigma(x)\\,\\mathrm{d}x =\nf(0) + \\frac{\\sigma^2}{2} f''(0) + \\frac{\\sigma^4}{8} f^{(4)}(0) + \\cdots,\n\\] where derivatives are with respect to \\(x\\). Conclude that the limit \\(\\sigma \\to 0\\) yields the Dirac delta: \\[\n\\lim_{\\sigma \\to 0} p_\\sigma(x) = \\delta(x).\n\\]",
    "crumbs": [
      "Courses",
      "Mathematical Physics",
      "Dirac Delta"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/index.html",
    "href": "courses/introduction-to-physics/index.html",
    "title": "Introduction to Physics",
    "section": "",
    "text": "These are the lecture notes and exercise lists for the Introduction to Physics (Estrutura da Matéria) course. This course introduces the basics of physics, including classical mechanics, quantum mechanics, electrodynamics and relativity. The content is under development. This course material is in portuguese.\n\n\n\nA Física como Linguagem Matemática\nDeterminismo e Estatística\nLeis macroscópicas emergindo do coletivo; noções de entropia.\nIntrodução ao Princípio da Incerteza de Heisenberg\n\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Overview"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/index.html#course-outline",
    "href": "courses/introduction-to-physics/index.html#course-outline",
    "title": "Introduction to Physics",
    "section": "",
    "text": "A Física como Linguagem Matemática\nDeterminismo e Estatística\nLeis macroscópicas emergindo do coletivo; noções de entropia.\nIntrodução ao Princípio da Incerteza de Heisenberg\n\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Overview"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/intro-stat-mechanics.html",
    "href": "courses/introduction-to-physics/intro-stat-mechanics.html",
    "title": "Leis macroscópicas emergindo do coletivo; noções de entropia.",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Statistical Mechanics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/intro-stat-mechanics.html#o-problema-fundamental-a-impossibilidade-de-acompanhar-tudo",
    "href": "courses/introduction-to-physics/intro-stat-mechanics.html#o-problema-fundamental-a-impossibilidade-de-acompanhar-tudo",
    "title": "Leis macroscópicas emergindo do coletivo; noções de entropia.",
    "section": "O Problema Fundamental: A Impossibilidade de Acompanhar Tudo",
    "text": "O Problema Fundamental: A Impossibilidade de Acompanhar Tudo\nUm sistema macroscópico típico (um gás, um copo d’água) contém da ordem de \\(N_A \\approx 10^{23}\\) partículas. É impossível, e praticamente inútil, resolver as equações de movimento para cada uma delas.\nSurge então a questão fundamental: como leis simples e determinísticas (Leis de Newton) dão origem a comportamentos coletivos igualmente simples, mas qualitativamente diferentes (Leis da Termodinâmica)?\nA resposta está em mudar nossa perspectiva: de uma descrição microscópica (trajetórias individuais) para uma estatística (propriedades médias do coletivo).",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Statistical Mechanics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/intro-stat-mechanics.html#a-ideia-central-o-ensemble-microcanônico",
    "href": "courses/introduction-to-physics/intro-stat-mechanics.html#a-ideia-central-o-ensemble-microcanônico",
    "title": "Leis macroscópicas emergindo do coletivo; noções de entropia.",
    "section": "A Ideia Central: O Ensemble Microcanônico",
    "text": "A Ideia Central: O Ensemble Microcanônico\n\nDefinição Formal\nO Ensemble Microcanônico é a amostra estatisticamente representativa de todos os possíveis estados microscópicos de um sistema que:\n\nTem um número fixo de partículas (\\(N\\))\nEstá confinado em um volume fixo (\\(V\\))\nTem uma energia total isolada e fixa (\\(E\\))\n\n\n\nPostulado Fundamental\nPara um sistema isolado em equilíbrio, todos os microestados compatíveis com \\((N, V, E)\\) são igualmente prováveis.\nEste é um postulado de “ignorância máxima”: na ausência de qualquer informação que privilegie um estado particular, atribuímos probabilidades iguais a todos os estados acessíveis.\nExiste uma outra forma de expressar esse postulado: se não existe qualquer informação que privilegie um estado particular, então todos os microestados compatíveis com \\((N, V, E)\\) são igualmente prováveis.\n\n\nExercícios\n\nExplique em suas palavras por que não é prático resolver as equações de movimento para cada partícula em um sistema macroscópico.\nImagine um gás ideal isolado com \\(N\\) partículas e energia total \\(E\\). Se você conhecer apenas \\(N\\), \\(V\\) e \\(E\\), como calcularia a probabilidade de encontrar um subconjunto de partículas em uma região específica do volume?\nDiscuta a relação entre “ignorância máxima” e a atribuição de probabilidades iguais aos microestados. Por que isso é necessário para a consistência da teoria?",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Statistical Mechanics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/intro-stat-mechanics.html#a-ponte-entre-as-escalas-entropia-e-a-emergência-da-seta-do-tempo",
    "href": "courses/introduction-to-physics/intro-stat-mechanics.html#a-ponte-entre-as-escalas-entropia-e-a-emergência-da-seta-do-tempo",
    "title": "Leis macroscópicas emergindo do coletivo; noções de entropia.",
    "section": "A Ponte entre as Escalas: Entropia e a Emergência da Seta do Tempo",
    "text": "A Ponte entre as Escalas: Entropia e a Emergência da Seta do Tempo\n\nMicroestado vs. Macroestado\n\nMicroestado: Descrição completa do sistema (posições e momentos de todas as partículas).\nExemplo: \\((\\vec{x}_1, \\vec{p}_1, \\vec{x}_2, \\vec{p}_2, \\dots, \\vec{x}_N, \\vec{p}_N)\\)\nMacroestado: Descrição macroscópica (medida por instrumentos).\nExemplo: Pressão (\\(P\\)), Volume (\\(V\\)), Temperatura (\\(T\\)), Energia (\\(E\\))\n\nPonto central: Um único macroestado (ex: “gás à temperatura \\(T\\)”) pode ser realizado por um número astronômico (\\(\\Omega\\)) de microestados diferentes.\n\n\nA Definição de Entropia de Boltzmann\n\\[\nS = k_B \\ln \\Omega.\n\\] onde:\n\n\\(S\\): Entropia do macroestado\n\\(k_B\\): Constante de Boltzmann (\\(1.38 \\times 10^{-23}\\) J/K)\n\\(\\Omega\\): Número de microestados compatíveis com o macroestado \\((N, V, E)\\)\n\nInterpretação física: A entropia mede o número de maneiras pelas quais um sistema pode ser organizado microscopicamente sem alterar sua aparência macroscópica. É uma medida do número de possibilidades de arranjo interno.\n\n\nA Emergência da Segunda Lei\nUm sistema isolado evolui naturalmente para o macroestado de maior entropia porque este é o macroestado com o maior número de microestados.\nÉ estatisticamente extremamente improvável que o sistema espontaneamente abandone um macroestado com \\(\\Omega \\approx 10^{10^{23}}\\) possibilidades e vá para um com muito menos possibilidades. A seta do tempo é, portanto, uma seta probabilística.\n\n\nVisualização: Microestados vs Macroestado\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\nnp.random.seed(42)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n# Macroestado\nax[0].add_patch(Rectangle((0, 0), 1, 1, fill=False, edgecolor=\"black\", lw=2))\nax[0].set_xlim(-0.1, 1.1)\nax[0].set_ylim(-0.1, 1.1)\nax[0].set_title(\"Macroestado\\nSistema: N partículas, Volume V, Energia E\")\nax[0].text(\n    0.5,\n    0.5,\n    \"P, T, E, S\",\n    ha=\"center\",\n    va=\"center\",\n    fontsize=14,\n    bbox=dict(facecolor=\"white\", alpha=0.7),\n)\nax[0].set_xticks([])\nax[0].set_yticks([])\n\n# Microestados\nax[1].add_patch(Rectangle((0, 0), 1, 1, fill=False, edgecolor=\"black\", lw=2))\nax[1].set_xlim(-0.1, 1.1)\nax[1].set_ylim(-0.1, 1.1)\nax[1].set_title(\"Alguns Microestados Possíveis\\n(Todos com mesma Energia E)\")\nax[1].set_xticks([])\nax[1].set_yticks([])\n\n# Gerar diferentes configurações de microestados\nfor i in range(4):\n    for j in range(4):\n        x_pos = np.random.rand(6) * 0.96 + 0.02\n        y_pos = np.random.rand(6) * 0.96 + 0.02\n        ax[1].scatter(x_pos, y_pos, s=30, alpha=0.7)\n\nax[1].text(\n    2.2,\n    0.5,\n    r\"$\\Omega \\approx 10^{23}$ microestados\" + \"\\n\" + r\"$S = \\mathrm{k}_B \\ln\\Omega$\",\n    ha=\"left\",\n    va=\"center\",\n    fontsize=12,\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Ilustração conceitual da conexão entre microestados e macroestados. Um único macroestado (mesma energia E) pode ser realizado por um número enorme (\\(\\Omega\\)) de microestados diferentes.\n\n\n\n\n\n\n\nSimulação da Expansão Livre de um Gás\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n# Configuração inicial\nnp.random.seed(42)\nn_particles = 50\nbox_size = 2.0\ninitial_pos = 0.5  # Metade da caixa inicialmente ocupada\n\n# Posições iniciais (confinadas à metade esquerda)\nx_pos = np.random.rand(n_particles) * initial_pos\ny_pos = np.random.rand(n_particles) * initial_pos\n\n# Velocidades aleatórias\nvx = np.random.normal(0, 0.02, n_particles)\nvy = np.random.normal(0, 0.02, n_particles)\n\n# Configuração do plot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_xlim(0, box_size)\nax.set_ylim(0, box_size)\nax.set_xlabel(\"Posição X\")\nax.set_ylabel(\"Posição Y\")\nax.set_title(\"Expansão Livre de um Gás: Aumento de Entropia\")\n\n# Linha divisória inicial\ndiv_line = ax.axvline(initial_pos, color=\"red\", linestyle=\"--\", alpha=0.7)\nparticles = ax.scatter(x_pos, y_pos, s=30, alpha=0.7)\n\n# Texto para mostrar o tempo\ntime_text = ax.text(0.02, 0.95, \"\", transform=ax.transAxes)\n\n\ndef update(frame):\n    global x_pos, y_pos, vx, vy\n\n    # Atualizar posições\n    x_pos += vx\n    y_pos += vy\n\n    # Colisões com as paredes\n    vx[x_pos &lt;= 0] = np.abs(vx[x_pos &lt;= 0])\n    vx[x_pos &gt;= box_size] = -np.abs(vx[x_pos &gt;= box_size])\n    vy[y_pos &lt;= 0] = np.abs(vy[y_pos &lt;= 0])\n    vy[y_pos &gt;= box_size] = -np.abs(vy[y_pos &gt;= box_size])\n\n    # Atualizar scatter plot\n    particles.set_offsets(np.c_[x_pos, y_pos])\n    time_text.set_text(f\"Tempo: {frame}\")\n\n    return particles, time_text\n\n\n# Criar animação\nani = FuncAnimation(fig, update, frames=200, interval=50, blit=True)\nplt.close(fig)  # Evita mostrar o plot estático\n\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nFigure 2: Simulação da expansão livre de um gás, ilustrando o aumento de entropia.\n\n\n\n\n\n\nConclusão: A Emergência do Coletivo\n\nA Termodinâmica é a Física dos Macroestados: Leis como a Segunda Lei não são leis fundamentais no sentido newtoniano, mas leis estatísticas emergentes.\nPropriedades Macroscópicas como Médias: Temperatura, pressão e outras quantidades termodinâmicas são interpretadas como médias estatísticas sobre o ensemble de microestados.\nA Entropia como Ponte: A grandeza \\(S = k_B \\ln \\Omega\\) quantifica precisamente a transição do microscópico para o macroscópico, conectando a descrição estatística às observáveis termodinâmicas.\n\n\n\nExercícios\n\nDiferencie microestado e macroestado usando exemplos concretos de um gás em uma caixa.\nExplique o significado físico da entropia de Boltzmann \\(S = k_B \\ln \\Omega\\). Por que ela cresce durante a expansão livre de um gás?\nSuponha um sistema isolado com energia total fixa. Por que é extremamente improvável que ele espontaneamente evolua para um macroestado de menor entropia?\nNo exemplo da expansão de um gás, como a distribuição espacial das partículas ilustra a seta do tempo?\nDiscuta a relação entre o número de microestados \\(\\Omega\\) e a previsibilidade de propriedades macroscópicas. Por que a enorme quantidade de microestados não impede previsões precisas de médias como pressão e temperatura?\nConsidere um ensemble microcanônico. Explique como a entropia conecta a estatística de microestados à termodinâmica clássica.\nProponha uma forma de quantificar visualmente o aumento de entropia em uma simulação de partículas em 2D, semelhante ao exemplo dado.",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Statistical Mechanics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/intro-stat-mechanics.html#o-ensemble-microcanônico-em-detalhe",
    "href": "courses/introduction-to-physics/intro-stat-mechanics.html#o-ensemble-microcanônico-em-detalhe",
    "title": "Leis macroscópicas emergindo do coletivo; noções de entropia.",
    "section": "O Ensemble Microcanônico em Detalhe",
    "text": "O Ensemble Microcanônico em Detalhe\n\nDefinição Formal e Postulados\nO ensemble microcanônico descreve um sistema isolado em equilíbrio termodinâmico, caracterizado por:\n\nNúmero de partículas fixo: \\(N = \\text{constante}\\)\nVolume fixo: \\(V = \\text{constante}\\)\n\nEnergia fixa: \\(E = \\text{constante}\\) (com pequena incerteza \\(\\Delta E\\))\n\n\n\nPostulado Fundamental da Mecânica Estatística\nPara um sistema isolado em equilíbrio, todos os microestados acessíveis são igualmente prováveis.\nMatematicamente, se \\(\\Omega(E, V, N)\\) é o número de microestados com energia entre \\(E\\) e \\(E + \\Delta E\\), então a probabilidade de qualquer microestado específico é:\n\\[P_\\mu = \\frac{1}{\\Omega(E, V, N)}\\]\n\n\nA Densidade de Estados\nO conceito central no ensemble microcanônico é a densidade de estados \\(g(E)\\), definida como: \\[\ng(E) = \\frac{d\\Gamma}{dE}\n\\] onde \\(\\Gamma(E)\\) é o volume do espaço de fases com energia menor que \\(E\\).\nPara um sistema com \\(f\\) graus de liberdade: \\[\n\\Gamma(E) = \\int_{H(\\mathbf{q},\\mathbf{p}) \\leq E} \\dd^fq\\, \\dd^fp.\n\\]\nO número de microestados na casca de energia \\([E, E+\\Delta E]\\) é então: \\[\n\\Omega(E, V, N) = g(E) \\Delta E.\n\\]\n\n\nExemplo: Gás Ideal Monoatômico\nPara um gás ideal de \\(N\\) partículas em 3 dimensões: \\[\n\\Omega(E, V, N) = \\frac{V^N}{N! h^{3N}} \\frac{(2\\pi m E)^{3N/2}}{\\Gamma(3N/2 + 1)} \\Delta E.\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gamma\n\n# Parâmetros\nm = 1.0  # massa\nh = 1.0  # constante de Planck (unidades naturais)\nV = 1.0  # volume\nDelta_E = 0.1  # largura da casca de energia\n\n# Energias\nE = np.linspace(0.1, 10, 1000)\n\n# Densidade de estados para diferentes N\nN_values = [1, 3, 10, 100]\n\nplt.figure(figsize=(10, 6))\n\nfor N in N_values:\n    # Fator de contagem de estados\n    prefactor = (V**N) / (gamma(N + 1) * h ** (3 * N))\n    # Parte energética\n    energy_part = (2 * np.pi * m * E) ** (3 * N / 2) / gamma(3 * N / 2 + 1)\n    # Densidade de estados\n    g_E = prefactor * energy_part\n    # Número de estados na casca\n    Omega = g_E * Delta_E\n\n    plt.plot(E, Omega, label=f\"N = {N}\", linewidth=2)\n\nplt.xlabel(\"Energia E\")\nplt.ylabel(\"Número de microestados Ω(E)\")\nplt.title(\"Densidade de Estados para Gás Ideal\")\nplt.yscale(\"log\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Densidade de estados para um gás ideal com diferentes números de partículas\n\n\n\n\n\n\n\nA Conexão com a Termodinâmica\nA ponte entre a descrição microscópica e macroscópica é feita através da entropia de Boltzmann: \\[\nS(E, V, N) = k_B \\ln \\Omega(E, V, N).\n\\]\nA partir desta definição, todas as outras propriedades termodinâmicas podem ser derivadas:\n\nTemperatura: \\[\n\\frac{1}{T} = \\left(\\frac{\\partial S}{\\partial E}\\right)_{V,N}.\n\\]\nPressão: \\[\nP = T \\left(\\frac{\\partial S}{\\partial V}\\right)_{E,N}.\n\\]\nPotencial Químico: \\[\n\\mu = -T \\left(\\frac{\\partial S}{\\partial N}\\right)_{E,V}.\n\\]\n\n\n1. Fator \\(V^N\\): O Volume Acessível no Espaço de Configurações\nEste termo surge das integrais espaciais: \\[\n\\int_V d^3r_1 \\int_V d^3r_2 \\cdots \\int_V d^3r_N = V^N\n\\]\nCada partícula pode estar em qualquer ponto do volume \\(V\\), e como as partículas são independentes (gás ideal), o volume total acessível no espaço de configurações é o produto dos volumes individuais.\n\n\n2. Fator \\(\\frac{1}{N!}\\): A Indistinguibilidade Quântica\nEste é um dos conceitos mais profundos e não-triviais:\n\nNa mecânica clássica, partículas idênticas são distinguíveis em princípio (podemos “pintá-las” e acompanhar suas trajetórias).\nNa mecânica quântica, partículas idênticas são fundamentalmente indistinguíveis.\n\nO fator \\(1/N!\\) corrige a sobrecontagem de estados que são fisicamente indistinguíveis. Se permutamos duas partículas idênticas, não criamos um novo estado físico, apenas uma redistribuição das mesmas partículas.\nSem este fator, a entropia não seria extensiva - violaria uma propriedade fundamental da termodinâmica. Este foi o insight crucial de Gibbs que resolveu o “paradoxo de Gibbs”.\n\n\n3. Fator \\(\\frac{1}{h^{3N}}\\): O Quantum do Espaço de Fases\nEste termo tem origem quântica profunda:\n\n\\(h\\) é a constante de Planck, que define a escala na qual efeitos quânticos se tornam importantes.\nCada par (posição, momento) ocupa um “volume” mínimo no espaço de fases devido ao princípio da incerteza: \\[\n\\Delta x \\Delta p_x \\geq \\frac{\\hbar}{2}\n\\]\nO fator \\(h^{3N}\\) normaliza o volume do espaço de fases por estado quântico. Ele garante que estamos contando estados genuinamente distintos do ponto de vista quântico.\n\n\n\n4. Fator \\((2\\pi m E)^{3N/2}\\): A “Área” da Hiperesfera de Energia\nEste termo vem da integral no espaço de momentos com a restrição \\(H = \\sum \\frac{p_i^2}{2m} \\leq E\\):\nPara \\(N\\) partículas em 3 dimensões, estamos essencialmente calculando o volume de uma 3N-esfera de raio \\(\\sqrt{2mE}\\) no espaço de momentos.\nO volume de uma n-esfera de raio \\(R\\) é: \\[\nV_n(R) = \\frac{\\pi^{n/2}}{\\Gamma(\\frac{n}{2} + 1)} R^n\n\\]\nPara \\(n = 3N\\) e \\(R = \\sqrt{2mE}\\), obtemos: \\[\n\\text{Volume} = \\frac{\\pi^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} (2mE)^{3N/2}\n\\]\n\n\n5. Fator \\(\\frac{1}{\\Gamma(3N/2)}\\): A Correção da Casca de Energia\nEstamos interessados não no volume com energia \\(\\leq E\\), mas no número de estados na casca de energia \\([E, E+\\Delta E]\\).\nA relação entre o volume e a área da superfície de uma n-esfera é: \\[\n\\frac{dV_n(R)}{dR} = \\text{Área da superfície} = \\frac{2\\pi^{n/2}}{\\Gamma(n/2)} R^{n-1}\n\\]\nPara \\(n = 3N\\): \\[\ng(E) = \\frac{d}{dE} \\left[\\text{Volume}\\right] \\propto \\frac{(2\\pi m E)^{3N/2 - 1/2}}{\\Gamma(3N/2)} \\approx \\frac{(2\\pi m E)^{3N/2}}{\\Gamma(3N/2) E}\n\\]\nO fator \\(\\Gamma(3N/2)\\) no denominador vem desta derivação geométrica.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gamma\n\n# Dimensões\nn_values = np.arange(1, 21)\nR = 1.0  # raio unitário\n\n# Volume e área de n-esferas\nvolumes = [np.pi ** (n / 2) / gamma(n / 2 + 1) * R**n for n in n_values]\nareas = [2 * np.pi ** (n / 2) / gamma(n / 2) * R ** (n - 1) for n in n_values]\n\nplt.figure(figsize=(12, 5))\n\n# Volume vs Dimensão\nplt.subplot(1, 2, 1)\nplt.plot(n_values, volumes, \"o-\", label=\"Volume\")\nplt.xlabel(\"Dimensão (n)\")\nplt.ylabel(\"Volume\")\nplt.title(\"Volume de n-esferas de raio 1\")\nplt.yscale(\"log\")\nplt.grid(True, alpha=0.3)\n\n# Área vs Dimensão\nplt.subplot(1, 2, 2)\nplt.plot(n_values, areas, \"s-\", color=\"orange\", label=\"Área de Superfície\")\nplt.xlabel(\"Dimensão (n)\")\nplt.ylabel(\"Área de Superfície\")\nplt.title(\"Área de Superfície de n-esferas de raio 1\")\nplt.yscale(\"log\")\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Mostrar valores para algumas dimensões\nprint(\"Propriedades de n-esferas de raio 1:\")\nfor n in [1, 2, 3, 10, 20]:\n    vol = np.pi ** (n / 2) / gamma(n / 2 + 1)\n    area = 2 * np.pi ** (n / 2) / gamma(n / 2)\n    print(f\"n = {n:2d}: Volume = {vol:8.4f}, Área = {area:8.4f}\")\n\n\n\n\n\n\n\n\nFigure 4: Volume e área de superfície de n-esferas para diferentes dimensões\n\n\n\n\n\nPropriedades de n-esferas de raio 1:\nn =  1: Volume =   2.0000, Área =   2.0000\nn =  2: Volume =   3.1416, Área =   6.2832\nn =  3: Volume =   4.1888, Área =  12.5664\nn = 10: Volume =   2.5502, Área =  25.5016\nn = 20: Volume =   0.0258, Área =   0.5161\n\n\n\n\nSíntese: O Significado de Cada Termo\n\n\n\n\n\n\n\n\nTermo\nOrigem Física\nSignificado\n\n\n\n\n\\(V^N\\)\nIntegrais espaciais\nVolume acessível no espaço de configurações\n\n\n\\(\\frac{1}{N!}\\)\nIndistinguibilidade quântica\nCorreção para evitar sobrecontagem de estados físicos\n\n\n\\(\\frac{1}{h^{3N}}\\)\nPrincípio da incerteza\nNormalização por estado quântico no espaço de fases\n\n\n\\((2\\pi m E)^{3N/2}\\)\nIntegral no espaço de momentos\nVolume da hiperesfera de energia no espaço de momentos\n\n\n\\(\\frac{1}{\\Gamma(3N/2)}\\)\nGeometria da casca de energia\nFator de correção da derivada do volume\n\n\n\nEsta expressão encapsula a transição da mecânica clássica para a quântica, e da descrição microscópica para a termodinâmica.\n\n\n\nExercícios\n\nExplique em palavras o significado físico do ensemble microcanônico. Quais são as variáveis fixas e por quê?\nA partir da expressão \\[\n\\Omega(E, V, N) = \\frac{V^N}{N! h^{3N}} \\frac{(2\\pi m E)^{3N/2}}{\\Gamma(3N/2 + 1)} \\Delta E,\n\\] identifique e explique a origem física de cada fator.\nPara uma única partícula \\((N=1)\\), calcule o volume da hiperesfera de energia no espaço de momentos e a área da superfície, comparando com a aproximação para o número de microestados na casca de energia \\([E, E+\\Delta E]\\).\nFaça um gráfico da densidade de estados \\(\\Omega(E)\\) para \\(N = 1, 2, 5, 10\\) e discuta qualitativamente como ela cresce com \\(N\\) e \\(E\\).",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Statistical Mechanics"
    ]
  },
  {
    "objectID": "courses/introduction-to-physics/intro-stat-mechanics.html#relação-entre-entropia-e-energia",
    "href": "courses/introduction-to-physics/intro-stat-mechanics.html#relação-entre-entropia-e-energia",
    "title": "Leis macroscópicas emergindo do coletivo; noções de entropia.",
    "section": "Relação entre Entropia e Energia",
    "text": "Relação entre Entropia e Energia\nUsando a aproximação de Stirling (\\(\\ln N! \\approx N \\ln N - N\\)): \\[S \\approx Nk_B \\left[\\ln\\left(\\frac{V}{N}\\right) + \\frac{3}{2}\\ln\\left(\\frac{4\\pi m E}{3N h^2}\\right) + \\frac{5}{2}\\right]\\]\nA temperatura é então: \\[\n\\frac{1}{T} = \\left(\\frac{\\partial S}{\\partial E}\\right)_{V,N} = \\frac{3}{2}\\frac{Nk_B}{E}.\n\\] o que nos dá a famosa relação: \\[\nE = \\frac{3}{2}Nk_BT.\n\\]\n\n\nCode\n# Parâmetros\nN = 1000\nkB = 1.0\nV = 1.0\nm = 1.0\nh = 1.0\n\n# Energias\nE = np.linspace(100, 1000, 1000)\n\n# Entropia (usando fórmula aproximada)\nS = N * kB * (np.log(V / N) + 1.5 * np.log(4 * np.pi * m * E / (3 * N * h**2)) + 2.5)\n\n# Temperatura (derivada numérica)\ndE = E[1] - E[0]\ndS = np.gradient(S, dE)\nT = 1 / (dS / (N * kB))\n\nplt.figure(figsize=(12, 5))\n\n# Gráfico 1: Entropia vs Energia\nplt.subplot(1, 2, 1)\nplt.plot(E, S)\nplt.xlabel(\"Energia E\")\nplt.ylabel(\"Entropia S\")\nplt.title(\"Entropia vs Energia\")\nplt.grid(True, alpha=0.3)\n\n# Gráfico 2: Energia vs Temperatura\nplt.subplot(1, 2, 2)\nplt.plot(T, E)\nplt.xlabel(\"Temperatura T\")\nplt.ylabel(\"Energia E\")\nplt.title(\"Energia vs Temperatura\")\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Relação entre entropia e energia para um gás ideal\n\n\n\n\n\n\nAproximação do Continuum e Correções Quânticas\nPara sistemas macroscópicos, \\(\\Omega(E)\\) é tão grande que podemos tratar \\(E\\) como variável contínua. No entanto, para sistemas pequenos ou a baixas temperaturas, efeitos quânticos tornam-se importantes:\n\nEspaçamento entre níveis de energia: \\(\\Delta E_\\text{nível} \\approx \\frac{E}{N}\\)\nCondição de validade do continuum: \\(\\Delta E \\gg \\Delta E_\\text{nível}\\)\n\n\n\nAplicações e Limitações\nAplicações:\n\nSistemas isolados em equilíbrio\nFundamentação da termodinâmica estatística\nEstudo de sistemas com energia bem definida\n\nLimitações:\n\nDifícil de aplicar na prática (energia fixa é rara)\nCálculos tornam-se complexos para sistemas interagentes\nNão adequado para sistemas que trocam energia ou partículas\n\nO ensemble microcanônico serve como base conceitual para os ensembles canônico e grande-canônico, que são mais convenientes para aplicações práticas.\n\n\nExercícios\n\nUsando a aproximação de Stirling, derive a expressão aproximada para a entropia de um gás ideal monoatômico com \\(N\\) partículas, volume \\(V\\) e energia \\(E\\).\nA partir da entropia aproximada \\[\nS \\approx Nk_B \\left[\\ln\\left(\\frac{V}{N}\\right) + \\frac{3}{2}\\ln\\left(\\frac{4\\pi m E}{3N h^2}\\right) + \\frac{5}{2}\\right],\n\\] derive a expressão para a temperatura (T) e mostre que (E = Nk_B T).",
    "crumbs": [
      "Courses",
      "Introduction to Physics (portuguese)",
      "Introduction to Statistical Mechanics"
    ]
  }
]