[
  {
    "objectID": "courses/vector-calculus/vector-spaces.html",
    "href": "courses/vector-calculus/vector-spaces.html",
    "title": "Vector Spaces Products and Maps",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#vector-spaces",
    "href": "courses/vector-calculus/vector-spaces.html#vector-spaces",
    "title": "Vector Spaces Products and Maps",
    "section": "Vector Spaces",
    "text": "Vector Spaces\nVector spaces are used in physics to represent a wide variety of quantities. For example, in Newtonian mechanics, we use vector spaces to represent positions in space, velocities, and displacements. It is worth noting that these three physical quantities have distinct natures, which is reflected in the meaning of the operations we can perform with them. For instance, velocity vectors can be added or subtracted when we want to represent velocities in different inertial reference frames. Similarly, position vectors can be added when we change the origin of a coordinate system. However, there is no natural meaning to the sum of a position vector and a velocity vector, even though such a sum is mathematically well-defined. This highlights that when working with vector analysis, we are typically dealing with several different vector spaces. This becomes more evident when working with curvilinear coordinates, as we will see later.\nTo make our discussion more rigorous, we begin by defining vector spaces. In our treatment, we will restrict ourselves to real vector spaces, meaning our vector spaces are defined over the field of real numbers \\(\\mathbb{R}\\). In practice, this means we can multiply our vectors by real numbers. However, before defining vector spaces, we introduce some basic mathematical concepts that will be useful throughout this exposition. These concepts may seem overly formal and unnecessary for understanding the subject, but it is important for students to have these definitions available for reference to clarify the meaning of other expressions used in this exposition.\n\nDefinition 1 (Cartesian Product) Given two sets \\(A\\) and \\(B\\), the Cartesian product is simply the set of ordered pairs defined as: \\[A \\times B \\equiv \\{(a, b) \\mid a \\in A \\text{ and } b \\in B\\}.\\]\n\n\nDefinition 2 (Function Between Sets) Let \\(A\\) and \\(B\\) be two sets. We denote a function \\(f\\) with domain \\(A\\) and codomain \\(B\\) as: \\[f: A \\to B.\\] We use the notation \\(f(a) \\in B\\) to denote the value of the function \\(f\\) when applied to an element \\(a \\in A\\). It is important to note that some functions are denoted differently. For example, the addition of real numbers, \\(+: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}\\), is represented as \\(a + b \\in \\mathbb{R}\\) for \\(a, b \\in \\mathbb{R}\\). In these cases, we often refer to such functions as operators.\n\nIn Definition 2, we saw that we use different notation for some functions, such as the addition of two real numbers. Furthermore, some functions are denoted even more compactly. For example, the multiplication of two real numbers is typically denoted by the mere juxtaposition of two numbers. That is, when we write \\(ab\\) where \\(a, b \\in \\mathbb{R}\\), we are compactly representing the application of the multiplication function to two real numbers.\nFinally, we can define the concept of a real vector space:\n\nDefinition 3 (Real Vector Space) A vector space over the real numbers \\(\\mathbb{R}\\) is given by a set \\(\\mathbb{V}\\) and two operations: scalar multiplication and vector addition. The elements of \\(\\mathbb{V}\\) are denoted with an arrow above the symbol, i.e., \\(\\vec{v} \\in \\mathbb{V}\\). The operations are defined as: \\[\\cdot: \\mathbb{R} \\times \\mathbb{V} \\to \\mathbb{V}, \\quad +: \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{V}.\\] The addition operation \\(+\\) satisfies the following properties (where \\(\\vec{v}, \\vec{u}, \\vec{w} \\in \\mathbb{V}\\)):\n\nAssociativity: \\((\\vec{v} + \\vec{u}) + \\vec{w} = \\vec{v} + (\\vec{u} + \\vec{w})\\).\nCommutativity: \\(\\vec{v} + \\vec{u} = \\vec{u} + \\vec{v}\\).\nExistence of the additive identity: \\(\\vec{v} + \\vec{0} = \\vec{v}\\).\nExistence of the additive inverse: \\(\\vec{v} + (-\\vec{v}) = \\vec{0}\\), where \\(-\\vec{v} \\in \\mathbb{V}\\).\n\nThe scalar multiplication operation satisfies (where \\(\\vec{v}, \\vec{u} \\in \\mathbb{V}\\) and \\(a, b \\in \\mathbb{R}\\)):\n\nCompatibility with real multiplication: \\(a(b\\vec{v}) = (ab)\\vec{v}\\).\nExistence of the multiplicative identity: \\(1\\vec{v} = \\vec{v}\\).\nDistributivity over vector addition: \\(a(\\vec{v} + \\vec{u}) = a\\vec{v} + a\\vec{u}\\).\nDistributivity over scalar addition: \\((a + b)\\vec{v} = a\\vec{v} + b\\vec{v}\\).\n\n\nAn obvious first example of a real vector space is the set of real numbers \\(\\mathbb{R}\\) itself, whose elements we call scalars. It is easy to verify that this set satisfies all the properties described above. We can use this fact to construct more complex vector spaces. Consider the Cartesian product of two real spaces, \\(\\mathbb{R}^2 \\equiv \\mathbb{R} \\times \\mathbb{R}\\). Now, we can define the addition of two elements of \\(\\mathbb{R}^2\\) and scalar multiplication by a real number as: \\[\\begin{align}\n(a, b) + (c, d) &\\equiv (a + c, b + d), \\quad a, b, c, d \\in \\mathbb{R}, \\\\\na(b, c) &\\equiv (ab, ac), \\quad a, b, c \\in \\mathbb{R}.\n\\end{align}\\] It is straightforward to verify that all properties of Definition 3 are automatically satisfied. It is interesting to note exactly what we are doing: by defining addition and scalar multiplication as above, we are using the concepts of addition and multiplication of real numbers to define addition and multiplication in more complex sets, in this case \\(\\mathbb{R}^2\\). In other words, the set of ordered pairs of real numbers forms a vector space once we define the operations above. Naturally, we can make the same definitions for \\(\\mathbb{R}^3 \\equiv \\mathbb{R} \\times \\mathbb{R} \\times \\mathbb{R}\\). Henceforth, we will always use these definitions of addition and scalar multiplication when dealing with spaces \\(\\mathbb{R}^n \\equiv \\mathbb{R} \\times \\mathbb{R} \\times \\dots \\times \\mathbb{R}\\).",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#linear-independence",
    "href": "courses/vector-calculus/vector-spaces.html#linear-independence",
    "title": "Vector Spaces Products and Maps",
    "section": "Linear Independence",
    "text": "Linear Independence\nTo highlight the difference between \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\), we introduce a new definition:\nDefinition (Linear Independence): A set of vectors \\(\\vec{v}_i\\), \\(i \\in \\{1, 2, \\dots, n\\}\\), is said to be linearly independent if their linear combination is zero if and only if all coefficients are zero, i.e., \\[\\sum_{i=1}^n a^i \\vec{v}_i = 0 \\text{ if and only if } a^i = 0 \\text{ for all } i \\in \\{1, 2, \\dots, n\\}.\\]\nIn practical terms, a set of vectors is linearly independent if none of the vectors can be written as a linear combination of the others. That is, when they are not linearly independent, their combination may be zero even if some coefficients are non-zero (say \\(a^1 \\neq 0\\)), so: \\[\\begin{equation}\n\\vec{v}_1 = -\\frac{1}{a^1} \\sum_{i=2}^n a^i \\vec{v}_i.\n\\end{equation}\\]\nDefinition (Basis): A set of vectors forms a basis if they are linearly independent and any element of \\(\\mathbb{V}\\) can be written as a linear combination of them. The number of vectors in the basis denotes the dimension of \\(\\mathbb{V}\\).\nIt is easy to verify that the vectors \\(\\vec{e}_1 \\equiv (1, 0, 0)\\), \\(\\vec{e}_2 \\equiv (0, 1, 0)\\), and \\(\\vec{e}_3 \\equiv (0, 0, 1)\\) form a basis for \\(\\mathbb{R}^3\\). Thus, any vector in this space can be written as a linear combination of these vectors, i.e., \\[\\begin{equation}\n\\vec{v} = \\sum_{i=1}^n v^i \\vec{e}_i.\n\\end{equation}\\]\nFrom a physics perspective, we are interested in using mathematical concepts to represent concrete elements found in nature. Our real-world experience shows that we need three coordinates to specify a point in space. If we calculate the three coordinates of an object relative to an origin and then the coordinates of the origin relative to another reference point, the position of the object relative to the other reference point is given by the simple sum of the coordinates. This example shows that we can map the concept of position in space to elements of \\(\\mathbb{R}^3\\), and moreover, the operations defined in this space have physical meaning. In this sense, it is important to understand what we do when we apply physical modeling: we are seeking mathematical objects that can be mapped to physical quantities and whose operations correspond to elements of the physical world. In the example above, points in \\(\\mathbb{R}^3\\) are mapped to coordinate positions in the real world, and the addition of two vectors corresponds to a change in the coordinate origin.\nIn this chapter, we will focus on three-dimensional vector spaces, which is the arena where electromagnetism will be developed in this course.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#exercise-vector-space",
    "href": "courses/vector-calculus/vector-spaces.html#exercise-vector-space",
    "title": "Vector Spaces Products and Maps",
    "section": "Exercise Vector Space",
    "text": "Exercise Vector Space\n\nShow that \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) are vector spaces of dimension two and three, respectively.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#inner-product",
    "href": "courses/vector-calculus/vector-spaces.html#inner-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Inner Product",
    "text": "Inner Product\nThe inner product is defined as a function on a vector space that is used to calculate magnitudes and angles between vectors. The mathematical definition is as follows:\n\nDefinition 4 (Inner Product) Given a vector space \\(\\mathbb{V}\\), we define the inner product as the map: \\[\\cdot: \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}.\\] It satisfies the following properties (where \\(\\vec{v}, \\vec{u}, \\vec{w} \\in \\mathbb{V}\\)):\n\nSymmetry: \\(\\vec{v} \\cdot \\vec{w} = \\vec{w} \\cdot \\vec{v}\\).\nLinearity on the left: \\((\\vec{v} + \\vec{u}) \\cdot \\vec{w} = \\vec{v} \\cdot \\vec{w} + \\vec{u} \\cdot \\vec{w}\\).\nPositivity: \\(\\vec{v} \\cdot \\vec{v} \\geq 0\\), and equality holds if and only if \\(\\vec{v} = \\vec{0}\\).\n\nIt is worth noting that left linearity, combined with symmetry, implies right linearity. In other words, this product is bilinear. Finally, given an inner product, we define the norm or magnitude of a vector as: \\[\\begin{equation}\n|\\vec{v}| \\equiv \\sqrt{\\vec{v} \\cdot \\vec{v}},\n\\end{equation}\\] where the positivity property ensures that the square root is always well-defined.\n\nIn general, there are numerous possible choices for inner products. To see this, note that linearity implies that the inner product of any two vectors is: \\[\\vec{v} \\cdot \\vec{u} = \\left( \\sum_{i=1}^3 v^i \\vec{e}_i \\right) \\cdot \\left( \\sum_{j=1}^3 u^j \\vec{e}_j \\right) = \\sum_{i,j=1}^3 v^i u^j \\vec{e}_i \\cdot \\vec{e}_j.\\] Thus, we can compute the inner product between any two vectors if we know the inner products between the basis elements. To determine this, we need to define the six quantities \\(\\vec{e}_i \\cdot \\vec{e}_j\\). Note that, in general, there would be nine combinations of \\(i\\) and \\(j\\), but symmetry provides three equations: \\[\\begin{equation}\n\\vec{e}_2 \\cdot \\vec{e}_1 = \\vec{e}_1 \\cdot \\vec{e}_2, \\quad \\vec{e}_3 \\cdot \\vec{e}_1 = \\vec{e}_1 \\cdot \\vec{e}_3, \\quad \\vec{e}_3 \\cdot \\vec{e}_2 = \\vec{e}_2 \\cdot \\vec{e}_3.\n\\end{equation}\\] Apart from the positivity condition,1 we have complete freedom to choose these six quantities.\n1 This condition imposes constraints on the signs and inequalities between the terms but does not introduce additional equations and thus does not reduce the dimensionality of the problem.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#kronecker-delta",
    "href": "courses/vector-calculus/vector-spaces.html#kronecker-delta",
    "title": "Vector Spaces Products and Maps",
    "section": "Kronecker Delta",
    "text": "Kronecker Delta\nIn our treatment, we assume the space is flat, and in this case, the inner product that reproduces the familiar Euclidean geometry is given by \\(\\vec{e}_i \\cdot \\vec{e}_j = \\delta_{ij}\\), where we introduce the Kronecker delta: \\[\\begin{equation}\n\\delta_{ij} \\equiv \\begin{cases}\n1 & \\text{if } i = j, \\\\\n0 & \\text{if } i \\neq j.\n\\end{cases}\n\\end{equation}\\] In these notes, the inner product will always be the Euclidean inner product defined above.\nTo understand the geometric meaning of this product, we start by calculating the magnitude of a vector: \\[\\begin{equation}\n|\\vec{v}| = \\sqrt{(v^1)^2 + (v^2)^2 + (v^3)^2},\n\\end{equation}\\] which is simply the Euclidean norm we are familiar with. We can now implicitly define the angle \\(\\theta\\) between two vectors as: \\[\\begin{equation}\n\\vec{v} \\cdot \\vec{u} = |\\vec{v}| |\\vec{u}| \\cos \\theta.\n\\end{equation}\\] To verify that this definition aligns with the usual Euclidean geometry, consider the inner product between the vector \\(\\vec{v} = a^1 \\vec{e}_1 + a^2 \\vec{e}_2\\) and the vector \\(\\vec{u} = \\vec{e}_1\\), i.e., \\[\\begin{equation}\n\\vec{v} \\cdot \\vec{u} = \\left( a^1 \\vec{e}_1 + a^2 \\vec{e}_2 \\right) \\cdot \\vec{e}_1 = a^1 = |\\vec{v}| |\\vec{u}| \\frac{a^1}{\\sqrt{(a^1)^2 + (a^2)^2}} \\implies \\cos \\theta = \\frac{a^1}{\\sqrt{(a^1)^2 + (a^2)^2}}.\n\\end{equation}\\] In this example, the cosine of the angle between the vectors is exactly the adjacent side over the hypotenuse.\nIn general, the inner product defined above can be written as: \\[\n\\vec{v} \\cdot \\vec{u} = \\left( \\sum_{i=1}^3 v^i \\vec{e}_i \\right) \\cdot \\left( \\sum_{j=1}^3 u^j \\vec{e}_j \\right) = \\sum_{i,j=1}^3 v^i u^j \\vec{e}_i \\cdot \\vec{e}_j = \\sum_{i,j=1}^3 v^i u^j \\delta_{ij} = \\sum_{i=1}^3 v^i u^i.\n\\tag{1}\\] Note that in the last equality, we used the fact that \\(\\delta_{ij} = 0\\) for \\(i \\neq j\\) to eliminate one summation. Finally, we say that two vectors \\(\\vec{v}\\) and \\(\\vec{u}\\) are orthogonal if \\(\\vec{v} \\cdot \\vec{u} = 0\\).\nUsing the norm of a vector, we define the associated unit vector as: \\[\\begin{equation}\n\\hat{v} \\equiv \\frac{\\vec{v}}{|\\vec{v}|}.\n\\end{equation}\\]",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#exercises-inner-product",
    "href": "courses/vector-calculus/vector-spaces.html#exercises-inner-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Exercises Inner Product",
    "text": "Exercises Inner Product\n\nShow that left linearity and symmetry imply right linearity.\nWrite out the summations in Equation 1 explicitly and show that the presence of the Kronecker delta can be used to eliminate one summation.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#sec-eprod",
    "href": "courses/vector-calculus/vector-spaces.html#sec-eprod",
    "title": "Vector Spaces Products and Maps",
    "section": "Exterior Product",
    "text": "Exterior Product\nThe vector analysis developed so far pertains to objects representing points and directions in space. However, other concepts may be necessary to describe physical phenomena, namely planes and volumes. To describe a plane, we need two directions in space, meaning there is only one plane defined by two vectors that are not collinear.2 A volume, on the other hand, requires three vectors that are not coplanar to be defined. The concept needed to define planes and volumes is the tensor product. In this exposition, we provide a simplified discussion to build the reader’s intuition. Loosely speaking, the tensor product of a space \\(\\mathbb{V}\\) with itself, \\(\\mathbb{V} \\otimes \\mathbb{V}\\), is also a vector space (in the sense of Definition 3) formed by the basis vectors \\(\\vec{e}_i \\otimes \\vec{e}_j\\), \\(i, j \\in \\{1, 2, 3\\}\\), meaning it is a vector space of dimension nine. However, to describe planes and volumes, we want to exclude cases with collinear vectors. To do so, we restrict ourselves to the antisymmetric subspace of \\(\\mathbb{V} \\otimes \\mathbb{V}\\), as the antisymmetric product of two collinear vectors is always zero.\n2 Collinear vectors are those that correspond to a simple scalar multiple, i.e., if \\(\\vec{v}\\) and \\(\\vec{u}\\) are collinear, there exists a real number \\(a \\neq 0\\) such that \\(\\vec{v} = a \\vec{u}\\).\nDefinition 5 (Exterior Product) The basis of this antisymmetric second-order tensor space is defined as: \\[\\vec{e}_i \\wedge \\vec{e}_j \\in \\mathbb{V} \\wedge \\mathbb{V}, \\quad \\vec{e}_i \\wedge \\vec{e}_j = \\left( \\vec{e}_i \\otimes \\vec{e}_j - \\vec{e}_j \\otimes \\vec{e}_i \\right). \\tag{2}\\]\n\nWe denote vectors in the space \\(\\mathbb{V} \\wedge \\mathbb{V}\\) as bivectors. In this definition, we use the exterior product, but we will not delve into the mathematical details required for its formal definition. The important properties in our context are: \\[\\begin{aligned}\n\\vec{v} \\wedge \\vec{u} &= -\\vec{u} \\wedge \\vec{v}, \\quad (\\vec{v} + \\vec{u}) \\wedge \\vec{w} = \\vec{v} \\wedge \\vec{w} + \\vec{u} \\wedge \\vec{w}, \\quad (a \\vec{v}) \\wedge \\vec{u} = a (\\vec{v} \\wedge \\vec{u}), \\\\\n\\left( \\vec{v} \\wedge \\vec{u} \\right) \\wedge \\vec{w} &= \\vec{v} \\wedge \\left( \\vec{u} \\wedge \\vec{w} \\right), \\quad \\vec{v}, \\vec{u}, \\vec{w} \\in \\mathbb{V}, \\ a \\in \\mathbb{R}.\n\\end{aligned} \\tag{3}\\] It is easy to verify that this space is three-dimensional, meaning there are only three non-zero basis vectors: \\(\\vec{e}_1 \\wedge \\vec{e}_2\\), \\(\\vec{e}_1 \\wedge \\vec{e}_3\\), and \\(\\vec{e}_2 \\wedge \\vec{e}_3\\). Due to this coincidence—that the dimension of the antisymmetric tensor space in three dimensions is also three—J. Willard Gibbs introduced the concept of the vector product in his vector analysis textbook. The idea is to create a linear, bijective map (an isomorphism) between the space \\(\\mathbb{V} \\wedge \\mathbb{V}\\) and the original vector space \\(\\mathbb{V}\\). To do this, note that there is only one vector orthogonal to the plane \\(\\vec{e}_1 \\wedge \\vec{e}_2\\) (i.e., a vector orthogonal to both vectors used to construct the plane), namely \\(\\vec{e}_3\\).",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#vector-product",
    "href": "courses/vector-calculus/vector-spaces.html#vector-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Vector Product",
    "text": "Vector Product\nTo define our map, we can set \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\to \\vec{e}_3\\). However, note that planes can have different signs (e.g., \\(\\vec{e}_1 \\wedge \\vec{e}_2 = -\\vec{e}_2 \\wedge \\vec{e}_1\\)). Thus, to determine the map, we need to choose the signs of the planes mapped to vectors. Here, we follow the standard “right-hand rule” convention.3 We choose the sign according to the position of the thumb of the right hand when the first vector of the product is aligned with the palm and the second with the fingertips, i.e., we define the linear map \\(V: \\mathbb{V} \\wedge \\mathbb{V} \\to \\mathbb{V}\\) as: \\[V(\\vec{e}_1 \\wedge \\vec{e}_2) = \\vec{e}_3, \\quad V(\\vec{e}_3 \\wedge \\vec{e}_1) = \\vec{e}_2, \\quad V(\\vec{e}_2 \\wedge \\vec{e}_3) = \\vec{e}_1. \\tag{4}\\] Let us now explicitly calculate the exterior product of two vectors and compute its map to the space \\(\\mathbb{V}\\): \\[\\begin{aligned}\n\\vec{v} \\wedge \\vec{u} &= \\left( \\sum_{i=1}^3 v^i \\vec{e}_i \\right) \\wedge \\left( \\sum_{j=1}^3 u^j \\vec{e}_j \\right), \\\\\n&= \\left( v^1 u^2 - v^2 u^1 \\right) \\vec{e}_1 \\wedge \\vec{e}_2 - \\left( v^1 u^3 - v^3 u^1 \\right) \\vec{e}_3 \\wedge \\vec{e}_1 + \\left( v^2 u^3 - v^3 u^2 \\right) \\vec{e}_2 \\wedge \\vec{e}_3.\n\\end{aligned} \\tag{5}\\] Finally, we define the vector product by applying our isomorphism \\(V\\) to this product: \\[\\vec{v} \\times \\vec{u} \\equiv V(\\vec{v} \\wedge \\vec{u}) = \\left( v^2 u^3 - v^3 u^2 \\right) \\vec{e}_1 - \\left( v^1 u^3 - v^3 u^1 \\right) \\vec{e}_2 + \\left( v^1 u^2 - v^2 u^1 \\right) \\vec{e}_3. \\tag{6}\\] To simplify vector product calculations, we can compute the product of basis vectors and establish a general rule: \\[\\vec{e}_i \\times \\vec{e}_j = \\sum_{k=1}^3 \\epsilon_{ijk} \\vec{e}_k. \\tag{7}\\] The antisymmetry of the vector product shows that the term \\(\\epsilon_{ijk}\\), called the Levi-Civita symbol, must be antisymmetric in its first two indices: \\(\\epsilon_{ijk} = -\\epsilon_{jik}\\). Moreover, examining Equation 4, we see that this symbol is also antisymmetric in the last two indices, i.e., \\(\\epsilon_{ijk} = -\\epsilon_{ikj}\\), and that \\(\\epsilon_{123} = 1\\). Using the Levi-Civita symbol, we can write Equation 6 more compactly: \\[\\vec{v} \\times \\vec{u} = \\left( \\sum_{i=1}^3 v^i \\vec{e}_i \\right) \\times \\left( \\sum_{j=1}^3 u^j \\vec{e}_j \\right) = \\sum_{i,j,k=1}^3 \\epsilon_{ijk} v^i u^j \\vec{e}_k. \\tag{8}\\] Not only is this form more compact, but it is also much more convenient for computing more complex products, as we will see later.\n3 Collinear vectors are those that correspond to a simple scalar multiple, i.e., if \\(\\vec{v}\\) and \\(\\vec{u}\\) are collinear, there exists a real number \\(a \\neq 0\\) such that \\(\\vec{v} = a \\vec{u}\\).For completeness we provide the following definition:\n\nDefinition 6 (Levi-Civita Symbol) The Levi-Civita symbol is defined the complete antisymmetric tensor \\(\\epsilon_{ijk}\\) such that \\(\\epsilon_{123} = 1\\).\n\nFrom a geometric perspective, we can understand the meaning of this product by calculating the vector product between the vector \\(\\vec{v} = \\vec{e}_1\\) and the vector \\(\\vec{u} = a^1 \\vec{e}_1 + a^2 \\vec{e}_2\\): \\[\\vec{v} \\times \\vec{u} = a^2 \\vec{e}_3 = |\\vec{v}| |\\vec{u}| \\frac{a^2}{\\sqrt{(a^1)^2 + (a^2)^2}} \\vec{e}_3 = |\\vec{v}| |\\vec{u}| \\sin \\theta \\vec{e}_3. \\tag{9}\\] In other words, the vector product of two vectors results in a vector orthogonal to the plane formed by the two vectors, with a magnitude equal to the area of the parallelogram they form. This makes it clear that the result of a vector product does not have the same physical meaning as an ordinary vector. In our construction, the vector product is actually an antisymmetric tensor product used to represent planes (hence, its magnitude is associated with areas, not lengths). However, in three dimensions, it has the same dimensionality as the original vector space, allowing us to map between these spaces.\nThe case of volumes is even simpler. Following the previous logic, we want the composition of three non-coplanar vectors, which corresponds to the antisymmetric tensor product \\(\\mathbb{V} \\wedge \\mathbb{V} \\wedge \\mathbb{V}\\), called the space of trivectors. The only basis vector in this space is \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3\\); any other combination is either zero or proportional to this vector due to antisymmetry. This shows that this space is one-dimensional. For this reason, we can map this space to \\(\\mathbb{R}\\) using the map \\(\\Upsilon: \\mathbb{V} \\wedge \\mathbb{V} \\wedge \\mathbb{V} \\to \\mathbb{R}\\), defined by \\(\\Upsilon(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3) = 1\\). Like the linear map \\(V\\) used for the vector product, this map also depends on an arbitrary choice of sign. Moreover, the map \\(\\Upsilon\\) can be understood as computing the “magnitude” of this trivector, which represents the volume of the parallelepiped formed by the vectors.\nIn Gibbs’ vector analysis, instead of using the exterior product, the vector product is directly defined as a map \\(\\times: \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{V}\\). However, these vectors do not behave the same as vectors in \\(\\mathbb{V}\\) under spatial inversions, i.e., \\(\\vec{e}_i \\to -\\vec{e}_i\\). It is easy to see that the vector resulting from a vector product does not change sign in this case. For this reason, such vectors are called pseudovectors. This reveals a disadvantage of Gibbs’ analysis: it avoids introducing tensors and the exterior product but requires distinguishing between different types of vectors, which can lead to confusion in more extensive calculations. Similarly, instead of introducing the space of trivectors, we use the map \\(\\Upsilon\\) to map these elements to scalars (i.e., \\(\\mathbb{R}\\)). Here, we encounter the same complication as with the vector product: the scalars resulting from \\(\\Upsilon\\) do not behave like ordinary elements of \\(\\mathbb{R}\\) under spatial inversion, as they change sign and are thus called pseudoscalars.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#exercises-vector-product",
    "href": "courses/vector-calculus/vector-spaces.html#exercises-vector-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Exercises Vector Product",
    "text": "Exercises Vector Product\n\nShow that the exterior product of two vectors \\(\\vec{v}\\) and \\(a \\vec{v}\\) is zero.\nExplicitly compute Equation 5.\nExplicitly compute Equation 8.\nShow that the exterior product of three vectors \\(\\vec{v} \\wedge \\vec{u} \\wedge \\vec{w}\\) is proportional to \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3\\).",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#sec-trip",
    "href": "courses/vector-calculus/vector-spaces.html#sec-trip",
    "title": "Vector Spaces Products and Maps",
    "section": "Relation to the Exterior Product",
    "text": "Relation to the Exterior Product\nAs discussed in the Definition 5, the exterior product can be used to define mathematical objects that describe planes and volumes. Naturally, \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3\\) is a first example of a triple product. However, we need to relate this quantity to the tools available, namely the vector product and inner product. Using the definition of the map \\(V\\) given in Equation 4, it is straightforward to see that there is a relation between this map and \\(\\Upsilon\\), i.e., \\[\\begin{aligned}\n\\vec{e}_1 \\cdot V(\\vec{e}_2 \\wedge \\vec{e}_3) &= \\Upsilon(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3), \\\\\n\\vec{e}_2 \\cdot V(\\vec{e}_3 \\wedge \\vec{e}_1) &= \\Upsilon(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3), \\\\\n\\vec{e}_3 \\cdot V(\\vec{e}_1 \\wedge \\vec{e}_2) &= \\Upsilon(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3).\n\\end{aligned} \\tag{10}\\] In fact, it is not difficult to show that, in general, \\[\\vec{e}_i \\cdot V(\\vec{e}_j \\wedge \\vec{e}_k) = \\vec{e}_i \\cdot (\\vec{e}_j \\times \\vec{e}_k) = \\Upsilon(\\vec{e}_i \\wedge \\vec{e}_j \\wedge \\vec{e}_k) = \\epsilon_{ijk}. \\tag{11}\\] With this result, it is also easy to see that \\[\\vec{e}_i \\wedge \\vec{e}_j \\wedge \\vec{e}_k = \\epsilon_{ijk} \\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3. \\tag{12}\\] Using the linearity of the maps \\(V\\) and \\(\\Upsilon\\) and the definition of the vector product in Equation 6, we have \\[\\vec{v} \\cdot (\\vec{u} \\times \\vec{w}) = \\Upsilon(\\vec{v} \\wedge \\vec{u} \\wedge \\vec{w}). \\tag{13}\\] This shows that the triple product, where we first compute the vector product of \\(\\vec{u}\\) and \\(\\vec{w}\\) and then take the inner product of the result with \\(\\vec{v}\\), is equivalent to computing the trivector formed by these vectors and then taking its magnitude, resulting in the volume of the parallelepiped they form.\nThe second triple product we can consider is the composition of two vector products, i.e., \\[(\\vec{e}_i \\times \\vec{e}_j) \\times \\vec{e}_k = V\\left( V\\left( \\vec{e}_i \\wedge \\vec{e}_j \\right) \\wedge \\vec{e}_k \\right),\\] where we rewrite the vector product in terms of the exterior product on the right-hand side. Using the map in Equation 4, it is straightforward to compute this product for a specific choice of basis elements, for example, \\[\\begin{aligned}\nV\\left( V\\left( \\vec{e}_1 \\wedge \\vec{e}_2 \\right) \\wedge \\vec{e}_1 \\right) &= V\\left( \\vec{e}_3 \\wedge \\vec{e}_1 \\right) = +\\vec{e}_2, \\\\\nV\\left( V\\left( \\vec{e}_1 \\wedge \\vec{e}_2 \\right) \\wedge \\vec{e}_2 \\right) &= V\\left( \\vec{e}_3 \\wedge \\vec{e}_2 \\right) = -\\vec{e}_1, \\\\\nV\\left( V\\left( \\vec{e}_1 \\wedge \\vec{e}_2 \\right) \\wedge \\vec{e}_3 \\right) &= V\\left( \\vec{e}_3 \\wedge \\vec{e}_3 \\right) = \\vec{0}.\n\\end{aligned} \\tag{14}\\] We can also use the result in Equation 7 in terms of the Levi-Civita symbol to compute the same product. After a tedious process, we can show that all such products can be represented by \\[(\\vec{e}_i \\times \\vec{e}_j) \\times \\vec{e}_k = V\\left( V\\left( \\vec{e}_i \\wedge \\vec{e}_j \\right) \\wedge \\vec{e}_k \\right) = \\left( \\vec{e}_k \\cdot \\vec{e}_i \\right) \\vec{e}_j - \\left( \\vec{e}_k \\cdot \\vec{e}_j \\right) \\vec{e}_i. \\tag{15}\\] Using the linearity of all products involved, it follows naturally that \\[\\left( \\vec{v} \\times \\vec{u} \\right) \\times \\vec{w} = V\\left( V\\left( \\vec{v} \\wedge \\vec{u} \\right) \\wedge \\vec{w} \\right) = \\left( \\vec{w} \\cdot \\vec{v} \\right) \\vec{u} - \\left( \\vec{w} \\cdot \\vec{u} \\right) \\vec{v}. \\tag{16}\\] With these rules, we can compute any product involving three or more terms.\nAnother useful rule for the upcoming sections comes from applying the inverse of \\(V\\), defined in Equation 4, i.e., \\[V^{-1}(\\vec{e}_3) = \\vec{e}_1 \\wedge \\vec{e}_2, \\quad V^{-1}(\\vec{e}_2) = \\vec{e}_3 \\wedge \\vec{e}_1, \\quad V^{-1}(\\vec{e}_1) = \\vec{e}_2 \\wedge \\vec{e}_3. \\tag{17}\\] It is evident that the exterior product of \\(V^{-1}(\\vec{e}_i)\\) with \\(\\vec{e}_j\\) is zero if \\(i \\neq j\\), and it is straightforward to show that it equals \\(\\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3\\) if \\(i = j\\). Thus, we have \\[V^{-1}(\\vec{e}_i) \\wedge \\vec{e}_j = \\vec{e}_1 \\wedge \\vec{e}_2 \\wedge \\vec{e}_3 \\ \\delta_{ij}, \\quad \\Upsilon\\left[ V^{-1}(\\vec{e}_i) \\wedge \\vec{e}_j \\right] = \\delta_{ij}. \\tag{18}\\] In terms of vectors, we can easily compute that \\[\\Upsilon\\left[ V^{-1}(\\vec{v}) \\wedge \\vec{u} \\right] = \\vec{v} \\cdot \\vec{u}. \\tag{19}\\]\nWhen working in a vector space with both an exterior product and an inner product, it is natural to extend the inner product to compute not only the product between two vectors but also the product between a vector and a bivector or trivector. A possible definition, compatible with the antisymmetry of the exterior product, is \\[\\begin{aligned}\n\\vec{w} \\cdot (\\vec{v} \\wedge \\vec{u}) &= \\left( \\vec{w} \\cdot \\vec{v} \\right) \\vec{u} - \\left( \\vec{w} \\cdot \\vec{u} \\right) \\vec{v}, \\\\\n\\vec{w} \\cdot (\\vec{v} \\wedge \\vec{u} \\wedge \\vec{k}) &= \\left( \\vec{w} \\cdot \\vec{v} \\right) \\vec{u} \\wedge \\vec{k} - \\left( \\vec{w} \\cdot \\vec{u} \\right) \\vec{v} \\wedge \\vec{k} + \\left( \\vec{w} \\cdot \\vec{k} \\right) \\vec{v} \\wedge \\vec{u}.\n\\end{aligned} \\tag{20}\\] In words, the inner product of a vector with a bivector or trivector is the sum of the inner products of the vector with each constituent vector of the bivector or trivector, with the sign given by \\((-1)^n\\), where \\(n\\) is the number of vectors to the left of the vector being multiplied. Since any exterior product of more than three vectors in a three-dimensional vector space is zero, we can write \\[\\vec{k} \\wedge \\vec{v} \\wedge \\vec{u} \\wedge \\vec{w} = 0. \\tag{21}\\] Taking the inner product of the above expression with the vector \\(\\vec{l}\\), we obtain \\[\\left( \\vec{l} \\cdot \\vec{k} \\right) \\vec{v} \\wedge \\vec{u} \\wedge \\vec{w} - \\left( \\vec{l} \\cdot \\vec{v} \\right) \\vec{k} \\wedge \\vec{u} \\wedge \\vec{w} + \\left( \\vec{l} \\cdot \\vec{u} \\right) \\vec{k} \\wedge \\vec{v} \\wedge \\vec{w} - \\left( \\vec{l} \\cdot \\vec{w} \\right) \\vec{k} \\wedge \\vec{v} \\wedge \\vec{u} = 0. \\tag{22}\\] The result above can also be obtained directly through a long and tedious process by explicitly writing all vectors in terms of the basis \\(\\vec{e}_i\\).\nNote that Equation 16 also shows the relation between the map \\(V\\) and the inner product between vectors and bivectors or trivectors, i.e., \\[V\\left( V\\left( \\vec{v} \\wedge \\vec{u} \\right) \\wedge \\vec{w} \\right) = \\vec{w} \\cdot (\\vec{v} \\wedge \\vec{u}). \\tag{23}\\] Thus, similar to Equation 11, the exterior product has an intimate relation with the map \\(V\\). This arises because there exists a transformation between vectors, bivectors, and trivectors called the Hodge star map, and both \\(V\\) and \\(\\Upsilon\\) are related to the action of this map when applied to bivectors and trivectors, respectively.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#exercises-triple-product",
    "href": "courses/vector-calculus/vector-spaces.html#exercises-triple-product",
    "title": "Vector Spaces Products and Maps",
    "section": "Exercises Triple Product",
    "text": "Exercises Triple Product\n\nShow that Equation 11 and Equation 15 are true.\nStarting from Equation 11, prove that Equation 13 is valid.\nUsing Equation 15, prove that Equation 16 is valid.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#sec-euclid",
    "href": "courses/vector-calculus/vector-spaces.html#sec-euclid",
    "title": "Vector Spaces Products and Maps",
    "section": "Euclidean Space",
    "text": "Euclidean Space\n\nCartesian Coordinates\nWith the tools for calculations in vector spaces, we now establish a correspondence between mathematical definitions and the geometric notions of the space we aim to describe. Our definition of \\(\\mathbb{R}^3\\) does not specify the meaning of each component of the ordered triple. Assuming the space we inhabit is well-described by Euclidean geometry, we can construct a coordinate system to describe points as follows. First, choose an origin point and three orthogonal directions, \\(\\vec{e}_1\\), \\(\\vec{e}_2\\), and \\(\\vec{e}_3\\) (using the right-hand rule to determine \\(\\vec{e}_3\\)’s direction). For any point \\(p\\), compute its coordinates by:\n\nMove along \\(\\vec{e}_1\\) until aligned with \\(p\\), measure the distance from the origin, assign a positive sign if moving in the direction of \\(\\vec{e}_1\\) or negative if opposite, and call this value \\(x^1\\).\nMove along \\(\\vec{e}_2\\) until aligned with \\(p\\), record the distance and sign, obtaining \\(x^2\\).\nRepeat along \\(\\vec{e}_3\\) to reach \\(p\\), computing \\(x^3\\).\n\n\nDefinition 7 (Cartesian Coordinates) The Cartesian coordinates of a point \\(p\\) are the ordered triple of signed distances traveled in three perpendicular directions from the origin to \\(p\\). The point is represented by the vector: \\[\\vec{x} = \\sum_{i=1}^3 x^i \\vec{e}_i \\equiv (x^1, x^2, x^3), \\quad \\text{where} \\quad \\begin{array}{c}\n\\vec{e}_1 = (1, 0, 0), \\\\ \\vec{e}_2 = (0, 1, 0), \\\\ \\vec{e}_3 = (0, 0, 1).\n\\end{array} \\tag{24}\\] The equality after the summation uses the representation of vectors as ordered triples in \\(\\mathbb{R}^3\\). We call \\(\\vec{x}\\) the position vector.\n\nCartesian coordinates are special because operations on position vectors have geometric meaning. For example, the magnitude \\(|\\vec{x}|\\) represents the distance from the origin to the point at the vector’s tip. The difference between two position vectors, \\(\\vec{x} - \\vec{y}\\), represents the displacement between their points.4 The magnitude \\(|\\vec{x} - \\vec{y}|\\) is the Euclidean distance between the points. Thus, the inner product, as defined in Equation 1, acts as a metric for calculating distances in Euclidean space when applied to position vectors.\n4 We implicitly assume displacements are straight lines, equivalent to geodesics in Euclidean space.In contrast, other coordinate systems can label points differently. For example, using spherical coordinates with the same origin and directions, we compute: - The distance \\(r\\) from the origin to \\(p\\). - The angle \\(\\theta\\) between the line from the origin to \\(p\\) and \\(\\vec{e}_3\\). - The angle \\(\\varphi\\) between the projection of this line onto the \\(\\vec{e}_1\\)-\\(\\vec{e}_2\\) plane and \\(\\vec{e}_1\\).\nThe triple \\((r, \\theta, \\varphi)\\) is a point in \\(\\mathbb{R}^3\\). While \\(\\mathbb{R}^3\\) has a natural vector space structure, allowing addition, subtraction, and magnitude calculations, these operations lack immediate geometric meaning in spherical coordinates. For instance, computing the distance between \\((r_1, \\theta_1, \\varphi_1)\\) and \\((r_2, \\theta_2, \\varphi_2)\\) requires parameterizing the straight line between them and evaluating a line integral.5\n5 The line integral, as discussed later, depends on a norm in the tangent vector space, requiring Euclidean geometry to define it.Spherical coordinates highlight how Cartesian coordinates and position vectors simplify geometric calculations. This approach, using Cartesian coordinates as the starting point, will be adopted throughout these notes for geometric information. However, this is a matter of convenience, not necessity; a full differential geometry approach is more complex but possible.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/vector-calculus/vector-spaces.html#sec-vectan",
    "href": "courses/vector-calculus/vector-spaces.html#sec-vectan",
    "title": "Vector Spaces Products and Maps",
    "section": "Tangent Vectors",
    "text": "Tangent Vectors\nUsing geometric concepts represented by position vectors, we define the space of tangent vectors. Given a curve described by the position vector \\(\\vec{x}(t)\\), the displacement due to a parameter change \\(t \\to t + \\delta t\\) is: \\[\\Delta \\vec{x}(t) \\equiv \\vec{x}(t + \\delta t) - \\vec{x}(t). \\tag{25}\\] As \\(\\delta t \\to 0\\), \\(\\Delta \\vec{x}(t)\\) approximates the true displacement along the curve: \\[\\Delta \\vec{x}(t) \\approx \\dot{\\vec{x}}(t) \\delta t, \\quad \\dot{\\vec{x}}(t) \\equiv \\frac{\\partial \\vec{x}(t)}{\\partial t}. \\tag{26}\\] We call \\(\\dot{\\vec{x}}(t)\\) the tangent vector to the curve \\(\\vec{x}(t)\\).\n\nDefinition 8 (Tangent Space) The space of all tangent vectors to curves passing through a point \\(\\vec{x}\\) is called the tangent space at \\(\\vec{x}\\), denoted \\(\\mathbb{V}_{\\vec{x}}\\).\n\nThe displacement’s magnitude is: \\[|\\Delta \\vec{x}(t)| \\approx |\\dot{\\vec{x}}| |\\delta t|, \\quad \\text{where} \\quad |\\dot{\\vec{x}}| = \\sqrt{\\dot{\\vec{x}} \\cdot \\dot{\\vec{x}}}. \\tag{27}\\] The inner product, used to measure distances in the position space, is “inherited” by the tangent space \\(\\mathbb{V}_{\\vec{x}}\\). This allows us to measure infinitesimal displacements from a point by computing the length of tangent vectors. Thus, the inner product defines angles and lengths in both the position space and \\(\\mathbb{V}_{\\vec{x}}\\).\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Vector Spaces Products and Maps"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html",
    "href": "courses/special-relativity/galilean-relativity.html",
    "title": "Galilean Relativity",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#galilean-relativity",
    "href": "courses/special-relativity/galilean-relativity.html#galilean-relativity",
    "title": "Galilean Relativity",
    "section": "Galilean Relativity",
    "text": "Galilean Relativity\nIn Galilean relativity, space is modeled as a three-dimensional Euclidean vector space1:\n1 Euclidean space\\[\n\\mathbb{E}^3 = (\\mathbb{R}^3, +, \\cdot)\n\\]\nThis structure defines:\n\nA vector space: closed under vector addition and scalar multiplication.\nAn inner product \\((\\cdot)\\): allows us to define angles and lengths.\n\nLet \\(\\vec{x}, \\vec{y} \\in \\mathbb{E}^3\\). Then:\n\nAddition: \\(\\vec{x} + \\vec{y} \\in \\mathbb{E}^3\\)\nScalar multiplication: \\(a \\vec{x} \\in \\mathbb{E}^3\\), for \\(a \\in \\mathbb{R}\\)\nDistance between points is given by:\n\\[\n|\\vec{x} - \\vec{y}| = \\sqrt{(\\vec{x} - \\vec{y}) \\cdot (\\vec{x} - \\vec{y})}\n\\]\nwhich defines a norm, and hence a metric.\n\n\nCoordinates in \\(\\mathbb{E}^3\\)\nWe represent vectors in Cartesian coordinates:\n\\[\n\\vec{x} = (x^1, x^2, x^3)\n\\]\nEach component \\(x^i\\) corresponds to the projection of \\(\\vec{x}\\) onto the corresponding basis vector \\(\\vec{e}^i\\). These components carry physical meaning: they represent distances measured along fixed spatial directions from the origin.\nTo simplify visualization, we consider a 2-dimensional slice, \\(\\mathbb{E}^2\\), where:\n\\[\n\\vec{x} = (x^1, x^2)\n\\]\nBelow, we illustrate a vector \\(\\vec{x}\\) in \\(\\mathbb{E}^2\\), decomposed into its components.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 4))\n\ndraw_euclidean_2d(ax, title=r\"$\\mathbb{E}^2$\")\ndraw_vector_2d(ax, (1.0, 0.2), label=\"x\", color=\"blue\", components=True)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: In this diagram, \\(\\vec{x}\\) is shown along with its projections onto the \\(x^1\\) and \\(x^2\\) axes. The dashed lines and gray arrows indicate the coordinate decomposition that defines the Cartesian components.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#time",
    "href": "courses/special-relativity/galilean-relativity.html#time",
    "title": "Galilean Relativity",
    "section": "Time",
    "text": "Time\nSo far, we’ve discussed a single static space, like a snapshot of the world frozen at a particular instant. In such a snapshot, distances and directions are described by a Euclidean space, \\(\\mathbb{E}^2\\) or \\(\\mathbb{E}^3\\).\nBut time enters physics through measurement: We don’t just measure where things are, we also measure when.\n\nTwo Roles of Time\nTime appears in two distinct but related ways:\n\nSimultaneity: We want to compare the positions of multiple objects at the same instant. For example: “Two cars are 10 meters apart at 12:00.”\nChange: We want to track how the position of something evolves over time. For example: “This car was at \\(\\vec{x}_0\\) at 12:00 and at \\(\\vec{x}_1\\) at 12:05.”\n\nThis second case introduces a subtle question:",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#what-does-it-mean-for-a-point-to-stay-in-the-same-place",
    "href": "courses/special-relativity/galilean-relativity.html#what-does-it-mean-for-a-point-to-stay-in-the-same-place",
    "title": "Galilean Relativity",
    "section": "What Does It Mean for a Point to Stay in the Same Place?",
    "text": "What Does It Mean for a Point to Stay in the Same Place?\nTo say something “moved” from \\(\\vec{x}_0\\) to \\(\\vec{x}_1\\), we must assume:\n\n“The point that was at \\(\\vec{x}_0\\) at time \\(t_0\\) is the same place as the one at \\(\\vec{x}_1\\) at time \\(t_1\\).”\n\nBut how do we decide that?\nEach snapshot in time is a separate space, like \\(\\mathbb{E}^2_{t_0}\\) and \\(\\mathbb{E}^2_{t_1}\\). To describe motion, we must define a rule that identifies points across these spaces.\nLet’s visualize this idea:\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\ndraw_euclidean_2d(ax1, title=r\"$\\mathbb{E}^2_{t_0}$\")\ndraw_euclidean_2d(ax2, title=r\"$\\mathbb{E}^2_{t_1}$\")\n\ndraw_vector_2d(ax1, (0.5, 0.2), label=r\"x_0\", color=\"blue\", components=True)\ndraw_vector_2d(ax2, (0.5, 0.4), label=r\"x_1\", color=\"blue\", components=True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nEach figure is a full space at a fixed time: a spatial snapshot. But motion requires more than snapshots: it requires a way to track points across time.\n\nThe Identity Map (Trivial Frame)\nTo describe motion, we need to track spatial points across time: we must decide what it means for a point at time \\(t_0\\) to be “the same place” as a point at time \\(t_1\\).\nLet’s consider a simple scenario:\n\nAn observer is in a room and chooses a particular corner to serve as the origin of their coordinate system. From this origin, they align three fixed axes, say, along the edges of the room, and assign coordinates to every point based on measured distances along these axes.\n\nNow suppose this same observer makes two measurements, at times \\(t_0\\) and \\(t_1\\), using the same reference system: the same corner, the same directions, and the same notion of distance. Then, the observer naturally identifies the “same point in space” at both times by matching coordinates: \\[\n\\vec{x}_1 = \\vec{x}_0\n\\]\nThis defines the so-called identity map (or trivial identification): \\[\n\\phi: \\mathbb{E}^2_{t_0} \\to \\mathbb{E}^2_{t_1}, \\qquad \\phi(\\vec{x}_0) = \\vec{x}_0\n\\]\nUnder this rule, an object is said to remain at rest if its coordinates do not change with time. However, this identification is not absolute, it is tied to the specific reference system used by the observer.\nThis distinction is key:\n\nThe first part of our discussion (assigning coordinates) requires physical references: like walls, corners, rulers; to define positions at a single instant.\nThe second part (relating points across time) defines the reference frame: the rule we use to track positions through time.\n\nIn practice, when we draw diagrams in \\(\\mathbb{E}^2\\), we often implicitly assume this trivial map, we imagine the reference objects do not move over time. This allows us to focus on how other objects evolve relative to the fixed background.\nBut this is only one possibility. More general mappings reflect different frames of reference, for example, frames moving with constant velocity (Galilean boosts). We’ll return to this idea soon.\n\n\nMaps Must Reflect Physical Expectations\nOnce we allow more general observers or motion, we must impose some physical conditions on the maps we use between spatial snapshots.\nThese come from our experience with the physical world, and how we expect objects to move and interact:\n\nPoints must not collide: If two distinct points at time \\(t_0\\) map to the same point at \\(t_1\\), we cannot distinguish their identities anymore. This contradicts the idea that particles have individual, continuous paths.\nNo points should appear or vanish: Every point at \\(t_1\\) must correspond to a point at \\(t_0\\). Otherwise, matter could appear from nothing or disappear entirely.\n\nFrom these we conclude:\n\nThe map \\(\\phi: \\mathbb{E}^2_{t_0} \\to \\mathbb{E}^2_{t_1}\\) must be bijective (one-to-one and onto).\n\nFurthermore, based on how real objects move, without teleportation or discontinuous jumps, we also expect:\n\nThe map should be smooth: small changes in position at \\(t_0\\) lead to small changes at \\(t_1\\).\n\nThis leads us to model \\(\\phi\\) as a diffeomorphism, a smooth, invertible map with a smooth inverse.\nBut again: This isn’t a mathematical requirement: it’s a physical modeling assumption based on the continuity of motion observed in nature.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#describing-motion-the-law-of-dynamics",
    "href": "courses/special-relativity/galilean-relativity.html#describing-motion-the-law-of-dynamics",
    "title": "Galilean Relativity",
    "section": "Describing Motion: The Law of Dynamics",
    "text": "Describing Motion: The Law of Dynamics\nBefore we can speak of forces or equations of motion, we must clarify what it means for an object to move, and how we can measure that change.\nIn practice, what we have is a sequence of observations: the position of a particle at different instants of time. Each of these measurements belongs to a copy of space, say \\(\\mathbb{E}^3_t\\), at time \\(t\\). But to compare them, to say how much the object moved, or how fast it changed direction, we must first decide how to identify points across time.\nThis is not automatic. It involves a modeling choice, which we made earlier by introducing a map between spatial snapshots. This map tells us which points at different times are considered “the same” from the observer’s perspective. Only then does the notion of displacement make sense.\nOnce we can compare positions across time, we can take differences, and eventually define the acceleration: \\[\n\\ddot{\\vec{x}}(t) = \\lim_{\\delta t \\to 0} \\frac{\\vec{x}(t + \\delta t) + \\vec{x}(t - \\delta t) - 2\\vec{x}(t)}{\\delta t^2},\n\\] where the dot indicates derivatives with respect to time.\nOnly after setting up this structure, coordinates, maps, and vector space operations, does it become meaningful to introduce Newton’s second law: \\[\n\\vec{F} = m \\ddot{\\vec{x}}\n\\] This equation is not the starting point, but rather the culmination of a chain of ideas rooted in observation: we measure position, define displacement and acceleration, and from there, assign a force. Historically, this came from empirical regularities, but here we are unpacking what it means to write such an equation and what assumptions it relies on.\nUltimately, Newton’s law reflects not just an observed pattern, but a whole framework of measurement, a way of tracking objects through space and time using a coherent mathematical model.\n\nConsistency of Vector Operations\nTo define acceleration, we use the second derivative: \\[\n\\ddot{\\vec{x}}(t) = \\lim_{\\delta t \\to 0} \\frac{ \\vec{x}(t+\\delta t) + \\vec{x}(t-\\delta t) - 2\\vec{x}(t) }{\\delta t^2}\n\\] At first glance, this seems like a purely mathematical operation. But for this expression to make sense, we must be able to add and subtract vectors that come from different spaces: \\(\\vec{x}(t+\\delta t) \\in \\mathbb{E}^3_{t+\\delta t}\\), \\(\\vec{x}(t-\\delta t) \\in \\mathbb{E}^3_{t-\\delta t}\\), and \\(\\vec{x}(t) \\in \\mathbb{E}^3_t\\).\nTo do this, we need a rule that tells us how to bring all these vectors into the same vector space so that arithmetic operations are well-defined. This is precisely what our map \\(\\phi\\) does, it tells us how to identify points and vectors between spaces at different times.\nWhen we adopt the trivial map (identity), we are assuming that: \\[\n\\vec{x}(t+\\delta t) \\in \\mathbb{E}^3_{t+\\delta t} \\quad \\text{is mapped to} \\quad \\mathbb{E}^3_t \\quad \\text{via} \\quad \\phi^{-1},\n\\] and likewise for \\(\\vec{x}(t-\\delta t)\\). With this identification, all three vectors are interpreted as elements of \\(\\mathbb{E}^3_t\\), and the derivative is now well defined.\nThis idea, that to differentiate vectors we must decide how to compare them at different points, is not just a technicality. It is the seed of a deep mathematical concept known as parallel transport, a central idea in differential geometry. There, we study more general rules for transporting vectors across curved spaces, where no global trivial identification exists.\nIn our case, we are working in flat Euclidean space with a chosen frame, so the identity map suffices. But it’s important to recognize that even here, the notion of transporting vectors is already present, though implicitly. And it is this structure that allows us to go from a sequence of positions to the definition of motion and dynamics.\n\n\nWhat if We Could Not Adopt the Identity Map?\nSo far, we have implicitly assumed that vectors at different times can be directly compared using the identity map. But what if this were not the case? What if comparing vectors at different times required a more general rule?\nLet’s define a map \\(T_{t_1, t_2}\\) that takes a vector from the space at time \\(t_1\\), denoted \\(\\mathbb{E}^3_{t_1}\\), to the space at time \\(t_2\\), \\(\\mathbb{E}^3_{t_2}\\). A simple example of such a map is: \\[\nT_{t_1, t_2} \\vec{x} = \\vec{x} + \\vec{v}(t_2) - \\vec{v}(t_1),\n\\] where \\(\\vec{v}(t)\\) is some vector-valued function of time. This is actually a special case of a more general concept called a diffeomorphism, but this simple form is enough for our discussion.\nNow, let’s see how this affects how we define the second derivative. We can write: \\[\n\\frac{\\dD^2 \\vec{x}}{\\dD t^2} = \\lim_{\\delta t \\to 0} \\frac{T_{t+\\delta t, t} \\vec{x}(t + \\delta t) + T_{t-\\delta t, t} \\vec{x}(t - \\delta t) - 2 \\vec{x}(t)}{\\delta t^2}.\n\\]\nTo evaluate this, we expand both transported terms using a Taylor series. For small \\(\\delta t\\), we get: \\[\n\\begin{aligned}\nT_{t+\\delta t, t} \\vec{x}(t + \\delta t) &= \\vec{x}(t + \\delta t) + \\vec{v}(t) - \\vec{v}(t + \\delta t) \\\\\n&= \\vec{x}(t + \\delta t) - \\dot{\\vec{v}}(t)\\delta t - \\tfrac{1}{2} \\ddot{\\vec{v}}(t)\\delta t^2 + O(\\delta t^3), \\\\\nT_{t-\\delta t, t} \\vec{x}(t - \\delta t) &= \\vec{x}(t - \\delta t) + \\vec{v}(t) - \\vec{v}(t - \\delta t) \\\\\n&= \\vec{x}(t - \\delta t) + \\dot{\\vec{v}}(t)\\delta t - \\tfrac{1}{2} \\ddot{\\vec{v}}(t)\\delta t^2 + O(\\delta t^3).\n\\end{aligned}\n\\]\nNow plugging these into the expression for the second derivative: \\[\n\\frac{\\dD^2 \\vec{x}}{\\dD t^2} = \\lim_{\\delta t \\to 0} \\frac{\\vec{x}(t + \\delta t) + \\vec{x}(t - \\delta t) - 2\\vec{x}(t) - \\ddot{\\vec{v}}(t)\\delta t^2}{\\delta t^2}.\n\\]\nThe limit gives: \\[\n\\frac{\\dD^2 \\vec{x}}{\\dD t^2} = \\ddot{\\vec{x}}(t) - \\ddot{\\vec{v}}(t).\n\\]\nThis result shows that under the transport map, the second derivative of the position depends not only on the usual acceleration \\(\\ddot{\\vec{x}}\\), but also on how the map varies with time — specifically, on \\(\\ddot{\\vec{v}}\\). Thus, even if the particle remains at the same coordinates, the computed derivative may differ under a different map.\nThis implies that, unless \\(\\ddot{\\vec{v}} = 0\\), the measured acceleration and force will depend on the choice of transport map. In practice, classical mechanics traditionally adopts a physical coordinate system — typically the lab frame — and uses the identity map, which corresponds to \\(\\ddot{\\vec{v}} = 0\\). The empirical laws of motion are formulated in this context. However, the calculation above shows that this choice is not unique: we can equally use any other transport map with \\(\\ddot{\\vec{v}} = 0\\), and the resulting forces and accelerations will be unchanged. In other words, if a set of physical laws holds for one such map, it will hold for all of them. This invariance under transformations with constant relative velocity is the content of Galilean relativity.\nNow here’s the key physical point: classical mechanics is grounded on the empirical observation that the motion of a system is fully determined by its initial position and velocity. If we prepare a system twice with the same position and velocity, we observe the same trajectory. If we change the velocity while keeping the position fixed, the resulting motion changes. This is not a modeling assumption — it is a physical fact. Therefore, the equations of motion must involve second-order time derivatives: the state of the system is defined by both position and velocity.\nThis second-order structure has an important implication. In our expression for the transformed second derivative, only the term \\(\\ddot{\\vec{v}}(t)\\) from the transport map affects the result. Hence, if we restrict ourselves to maps for which \\(\\ddot{\\vec{v}}(t) = 0\\), all such maps yield the same equations of motion. The physical laws are thus invariant under transformations between frames in relative uniform motion — that is, Galilean relativity arises as a direct consequence of the second-order nature of dynamics.\nMoreover, this structure reveals a further symmetry: even if we begin in a non-inertial frame (i.e., one with \\(\\ddot{\\vec{v}}(t) \\ne 0\\)), we can still transform to any other frame that differs by a constant velocity (\\(\\ddot{\\vec{v}} = 0\\)) without changing the form of the equations governing that non-inertial frame. So even in accelerated frames, there remains an internal symmetry among all frames connected by uniform motion.\nIn summary:\n\nThe second-order nature of dynamics implies that all frames related by constant-velocity transformations describe the same physics.\nThis defines a symmetry of the laws of motion.\nThat symmetry persists even in non-inertial frames: we can still shift to another frame with constant relative velocity without altering our description of the observed phenomena.\n\nHere is an improved and expanded version of your section, with clearer structure, smoother language, and more precise statements — while preserving your original intent:",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#other-symmetries",
    "href": "courses/special-relativity/galilean-relativity.html#other-symmetries",
    "title": "Galilean Relativity",
    "section": "Other Symmetries",
    "text": "Other Symmetries\nChanging the reference frame is not the only symmetry present in classical mechanics. The equations of motion exhibit additional symmetries that are both empirically observed and reflected in the mathematical structure of the theory.\nFirst, we observe that the outcome of a mechanical experiment does not depend on where it is performed. That is, if we repeat the same setup at a different location in space, we obtain the same results. This is spatial translation symmetry. Similarly, the results of the experiment do not depend on the orientation of the coordinate system: rotating the entire setup in space leaves the physics unchanged. This is rotational symmetry. Both of these are geometric symmetries of the Euclidean space we assume, they are built into the invariance of the spatial metric under translations and rotations.\nSecond, we observe that repeating an experiment at a different time, today, tomorrow, or next year, yields the same results, provided external conditions remain the same. This is time translation symmetry.\nThese symmetries, spatial translations, spatial rotations, and time translations, are empirical facts that are encoded in the structure of Newtonian mechanics. When combined with the invariance under changes of inertial reference frame (i.e., transformations with constant relative velocity), they form the full symmetry group of Galilean relativity.\nIn summary:\n\nRotational symmetry: physics does not depend on orientation.\nSpatial translation symmetry: physics does not depend on position.\nTime translation symmetry: physics does not depend on absolute time.\nInertial frame symmetry: physics is the same in all frames moving at constant velocity relative to one another.\n\nThese symmetries not only reflect experimental observations but also constrain the form of the allowed physical laws. They play a central role in the formulation of conservation laws and the structure of classical dynamics.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/galilean-relativity.html#field-transformations-and-the-role-of-the-map",
    "href": "courses/special-relativity/galilean-relativity.html#field-transformations-and-the-role-of-the-map",
    "title": "Galilean Relativity",
    "section": "Field Transformations and the Role of the Map",
    "text": "Field Transformations and the Role of the Map\nLet us now examine how the structures we have built so far apply to fields and their dynamics.\nOperationally, a field is something we measure in space at a given instant of time. That is, at each fixed time \\(t\\), we measure a quantity defined over the corresponding three-dimensional Euclidean space \\(\\mathbb{E}^3_t\\). Therefore, we do not have a single function \\(\\phi(t, \\vec{x})\\) defined on \\(\\mathbb{R} \\times \\mathbb{E}^3_{t_0}\\), but rather a family of functions: \\[\n\\phi: t \\in \\mathbb{R} \\mapsto \\phi_t \\in \\text{Func}(\\mathbb{E}^3_t, \\mathbb{R}),\n\\] where each \\(\\phi_t\\) assigns values to points in the corresponding spatial slice \\(\\mathbb{E}^3_t\\). In this formulation, the field is a map from time to spatial functions, not a function of both time and space in a fixed space.\nTo write a global expression like \\(\\phi(t, \\vec{x})\\), we must first introduce a rule that identifies “the same point” across different spatial slices \\(\\mathbb{E}^3_t\\). This is the role of the maps \\[\nT_{t_0,t}: \\mathbb{E}^3_{t_0} \\to \\mathbb{E}^3_t,\n\\] which define how we track a fixed spatial point through time. Once such maps are chosen, we can represent the field as a function in the fixed space \\(\\mathbb{E}^3_{t_0}\\) via: \\[\n\\phi(t, \\vec{x}) \\equiv \\phi_t(T_{t_0,t}(\\vec{x})),\n\\quad \\text{with } \\vec{x} \\in \\mathbb{E}^3_{t_0}.\n\\] This global description depends on the chosen identification map \\(T_{t_0,t}\\), reflecting a modeling assumption about how space is related across time.\nThis structure becomes essential when describing field evolution, as it determines how we compare field values at different times. It encodes our implicit definition of “the same spatial point at different times”: a notion that becomes nontrivial when changing reference frames.\nFor instance, consider a frame moving with constant velocity \\(\\vec{v}_0\\). In this case, we define the map: \\[\nT_{t_0,t}(\\vec{x}) = \\vec{x} + \\vec{v}_0(t - t_0).\n\\] This corresponds to tracking a point that moves rigidly with velocity \\(\\vec{v}_0\\). Then, assuming \\(t_0 = 0\\) for convenience, the field seen from this moving frame is expressed as: \\[\n\\phi(t, \\vec{x}) = \\phi_t(\\vec{x} + \\vec{v}_0 t).\n\\]\n\nWhat Happens to the Equations of Motion?\nOnce we choose a reference frame, i.e., a coordinate system and an identification map \\(T_{t_0,t}\\), we can design experiments that probe the dynamics of a field. For instance, we may repeatedly measure the field \\(\\phi\\) at different points and times, in such a way that we can compare values across spatial slices. Within this operational setting, we may discover that the field obeys a certain dynamical law.\nSuppose, after performing such measurements in a given frame \\(S\\), we determine that the field satisfies the wave equation: \\[\n\\left( -\\frac{1}{c^2} \\frac{\\partial^2}{\\partial t^2} + \\nabla^2 \\right) \\phi(t, \\vec{x}) = 0.\n\\] This equation, which appears in the context of Maxwell’s equations in Lorenz gauge, is taken as a model for the field’s dynamics in that frame.\nWe could now repeat the entire set of experiments in a different frame \\(S'\\), defined by a different choice of coordinates and identification maps, for instance, one related to the original frame by a constant relative velocity \\(\\vec{v}_0\\). From our previous analysis of particle mechanics, we know that Newton’s laws retain their form under such Galilean changes of frame. This leads us to expect that the dynamical laws for fields, being part of physical reality, should also be invariant under such transformations.\nLet us test this expectation explicitly.\nIn the new frame \\(S'\\), we identify points using the map: \\[\nT_{0,t}(\\vec{x}) = \\vec{x} + \\vec{v}_0 t.\n\\] So the field as seen in \\(S'\\) is given by: \\[\n\\phi'(t, \\vec{x}) \\equiv \\phi(t, \\vec{x} + \\vec{v}_0 t),\n\\] which corresponds to observing the same field using the new coordinates.\nComputing the time derivative in this frame: \\[\n\\frac{\\partial \\phi'}{\\partial t}\n= \\frac{\\partial}{\\partial t} \\phi(t, \\vec{x} + \\vec{v}_0 t)\n= \\frac{\\partial \\phi}{\\partial t} + \\vec{v}_0 \\cdot \\nabla \\phi,\n\\] and \\[\n\\frac{\\partial^2 \\phi'}{\\partial t^2}\n= \\frac{\\partial^2 \\phi}{\\partial t^2}\n+ 2 (\\vec{v}_0 \\cdot \\nabla) \\frac{\\partial \\phi}{\\partial t}\n+ (\\vec{v}_0 \\cdot \\nabla)^2 \\phi.\n\\]\nSubstituting into the wave equation yields: \\[\n\\left[ -\\frac{1}{c^2} \\left( \\frac{\\partial}{\\partial t} + \\vec{v}_0 \\cdot \\nabla \\right)^2 + \\nabla^2 \\right] \\phi'(t, \\vec{x}) = 0.\n\\] This is not the same operator as in the original frame, and hence the field no longer satisfies the same wave equation in the transformed coordinates unless \\(\\vec{v}_0 = 0\\). The transformation deforms the structure of the equation.\nTherefore:\n\nThe form of Maxwell’s equations is not preserved under Galilean transformations.\n\nThis result was historically problematic: unlike Newtonian mechanics, electromagnetism did not exhibit Galilean invariance. The resolution came with Einstein’s formulation of special relativity, in which the maps between inertial frames are given by Lorentz transformations. These maps preserve the wave operator: \\[\n\\square = -\\frac{1}{c^2} \\frac{\\partial^2}{\\partial t^2} + \\nabla^2,\n\\] ensuring that Maxwell’s equations hold in all inertial frames. Thus, special relativity restores the principle of relativity at the level of field dynamics.\n\nThis concludes our first module. We’ve built from physical intuition a precise mathematical structure: spaces, maps, vectors, derivatives, that allows us to formulate and interpret physical laws. We’ve seen that consistency of operations across time depends on how we relate points in space, and how this structure is already implicit in the description of dynamics. Finally, we’ve learned that even the transformation properties of space and time are constrained by the physical laws we wish to preserve.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Galilean Relativity"
    ]
  },
  {
    "objectID": "courses/special-relativity/index.html",
    "href": "courses/special-relativity/index.html",
    "title": "Special Relativity",
    "section": "",
    "text": "Welcome to the Special Relativity course lecture notes by Sandro Vitenti.\n\n\n\nGalilean Relativity\nSpacetime Algebra\n\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Overview"
    ]
  },
  {
    "objectID": "courses/special-relativity/index.html#course-outline",
    "href": "courses/special-relativity/index.html#course-outline",
    "title": "Special Relativity",
    "section": "",
    "text": "Galilean Relativity\nSpacetime Algebra\n\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "",
    "text": "This website hosts lecture notes for undergraduate physics courses taught by Sandro Vitenti. These notes are designed to support students in understanding key concepts in physics through clear explanations, mathematical rigor, and practical examples. The content is open source, and contributions from the community are encouraged."
  },
  {
    "objectID": "index.html#welcome-to-undergraduate-physics-lecture-notes",
    "href": "index.html#welcome-to-undergraduate-physics-lecture-notes",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "",
    "text": "This website hosts lecture notes for undergraduate physics courses taught by Sandro Vitenti. These notes are designed to support students in understanding key concepts in physics through clear explanations, mathematical rigor, and practical examples. The content is open source, and contributions from the community are encouraged."
  },
  {
    "objectID": "index.html#available-courses",
    "href": "index.html#available-courses",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "Available Courses",
    "text": "Available Courses\nExplore the following courses, each with notes and exercises:\n\nVector Calculus: Covers vector spaces, inner products, vector products, and triple products, with applications to physics.\nSpecial Relativity: Introduces the principles of special relativity, including Lorentz transformations, spacetime, and relativistic kinematics."
  },
  {
    "objectID": "index.html#about-the-project",
    "href": "index.html#about-the-project",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "About the Project",
    "text": "About the Project\nThese notes are developed as an open-source project to provide accessible, high-quality resources for physics students. The content is hosted on GitHub, where you can:\n\nAccess the source files (written in Quarto).\nReport errors or ask questions.\nSuggest improvements or new topics.\nContribute directly by submitting pull requests.\n\nThe notes are released under the CC BY-NC-SA 4.0 license, allowing non-commercial use, remixing, and distribution with attribution to the original author by linking back to this site."
  },
  {
    "objectID": "index.html#contribute",
    "href": "index.html#contribute",
    "title": "Undergraduate Physics Lecture Notes",
    "section": "How to Contribute",
    "text": "How to Contribute\nWe welcome feedback, questions, and contributions from students, educators, and physics enthusiasts. Whether you spot a typo, have a question, or want to add a new section, you can get involved through our GitHub repository. Below are quick tips to get started.\n\nCreating a GitHub Issue\nIf you find an error, have a question, or want to suggest an improvement, create a GitHub issue:\n\nGo to the Repository: Visit github.com/vitenti-physics/undergrad.\nNavigate to Issues: Click the “Issues” tab near the top of the page.\nCreate a New Issue: Click the green “New issue” button.\nFill in Details:\n\nTitle: Write a clear, concise title (e.g., “Typo in Vector Product Equation”).\nDescription: Provide details, such as the section (e.g., Vector Product), the issue, and any suggestions. For questions, include context (e.g., “I don’t understand why the Levi-Civita symbol is antisymmetric”).\nLabels (optional): Add labels like “bug,” “question,” or “enhancement” if prompted.\n\nSubmit: Click “Submit new issue” to post it. You’ll be notified of responses.\n\n\n\nSuggesting Improvements or Contributing Content\nTo propose changes (e.g., fixing equations, adding examples), you can submit a pull request:\n\nFork the Repository: On the GitHub repo, click “Fork” to create a copy under your account.\nClone Your Fork: Download your fork to your computer using:\n\n   git clone https://github.com/your-username/undergrad.git\n\nEdit Files: Modify the Quarto files (e.g., vector_calculus.qmd) using a text editor.\n\nFor example, fix a typo in Vector Product.\n\nCommit Changes: Save your changes and commit:\n\n    git add .\n    git commit -m \"Fix typo in vector product section\"\n\n    Push to GitHub: Upload your changes:\n    bash\n\n    git push origin main\n\nCreate a Pull Request: Go to your fork on GitHub, click “Contribute,” and select “Open pull request.”\n\nDescribe your changes and submit."
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html",
    "href": "courses/special-relativity/spacetime-algebra.html",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "",
    "text": "\\[\n\\newcommand{\\vec}[1]{\\mathbf{#1}}\n\\newcommand{\\dd}{\\mathrm{d}}\n\\newcommand{\\dD}{\\mathrm{D}}\n\\newcommand{\\e}{\\mathsf{e}}\n\\]\nThis document summarizes key aspects of the Clifford algebra \\(\\mathrm{Cl}(1,3)\\), which encodes the geometric structure of Minkowski spacetime. We outline its basis, multivector structure, products, and important identities. The geometric algebra approach provides an algebraic description of spacetime, encompassing Lorentz transformations and reference frames.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#clifford-algebra-of-spacetime",
    "href": "courses/special-relativity/spacetime-algebra.html#clifford-algebra-of-spacetime",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Clifford Algebra of Spacetime",
    "text": "Clifford Algebra of Spacetime\nThe algebra \\(\\mathrm{Cl}(1,3)\\) is generated by a set of 1-vectors \\({\\e^\\mu}\\) satisfying the Minkowski metric \\(\\eta\\_{\\mu\\nu} = \\mathrm{diag}(-1,1,1,1)\\). Its elements are constructed from wedge products of these generators: \\[\n\\mathrm{Cl}(1,3) = \\mathrm{span}\\left\\{1, \\e^\\mu, \\e^\\mu \\wedge \\e^\\nu, \\e^\\mu \\wedge \\e^\\nu \\wedge \\e^\\rho, \\e^0 \\wedge \\e^1 \\wedge \\e^2 \\wedge \\e^3\\right\\}\n\\] The algebra has 16 independent elements: one scalar, four vectors, six bivectors, four trivectors, and one pseudoscalar. These are classified by grade: a scalar is grade-0, a vector is grade-1, a bivector is grade-2, a trivector is grade-3, and the pseudoscalar is grade-4. The full algebra includes linear combinations of such elements, known collectively as multivectors.\nEach \\(k\\)-vector represents a specific geometric object: for instance, \\(\\e^\\mu\\) is a 1-vector (associated with directions), \\(\\e^\\mu \\wedge \\e^\\nu\\) is a bivector (representing oriented planes), and so on. This graded structure mirrors the decomposition of the algebra and plays a central role in defining its products and geometric interpretation.\nFor grade-1 elements \\(\\omega, u \\in \\mathrm{Cl}(1,3)\\), the symmetric (inner) and antisymmetric (exterior) products are defined as: \\[\n\\omega \\cdot u = \\frac{1}{2}(\\omega u + u \\omega), \\qquad\n\\omega \\wedge u = \\frac{1}{2}(\\omega u - u \\omega)\n\\] These operations extract the grade-0 and grade-2 parts of the Clifford product, respectively. The algebra is defined by the inner products of the basis elements: \\[\n\\e^\\mu \\cdot \\e^\\nu = \\eta^{\\mu\\nu}\n\\] where \\(\\eta_{\\mu\\nu}\\) is the Minkowski metric, used to raise and lower indices, including elements of the algebra and components of (multi)vectors.\nGiven vectors written in component form as \\(\\omega = \\omega^\\mu \\e_\\mu\\) and \\(u = u^\\nu \\e_\\nu\\), the inner product becomes: \\[\n\\omega \\cdot u = \\omega^\\mu u^\\nu \\eta_{\\mu\\nu}\n\\] while the exterior product is: \\[\n\\omega \\wedge u = (\\omega^\\mu u^\\nu - \\omega^\\nu u^\\mu) \\, \\e_\\mu \\wedge \\e_\\nu\n\\] The exterior product is antisymmetric and defines a bivector, corresponding to an oriented plane segment spanned by \\(\\omega\\) and \\(u\\).\nThe Clifford algebra encodes the geometric structure of spacetime, including Lorentz transformations and the behavior of reference frames. By defining the inner product as the symmetric part of the Clifford product, the Minkowski metric emerges directly from the algebraic relations among basis vectors. This allows calculations in special relativity to be carried out through purely algebraic manipulations.\nThe basis vectors satisfy: \\[\n(\\e^0)^2 = \\e^0 \\cdot \\e^0 = -1, \\qquad (\\e^i)^2 = \\e^i \\cdot \\e^i = 1 \\quad (i = 1, 2, 3)\n\\] which reflects the signature of the Minkowski metric \\(\\eta_{\\mu\\nu} = \\mathrm{diag}(-1, 1, 1, 1)\\). For distinct indices, the generators anticommute: \\[\n\\e^\\mu \\e^\\nu = -\\e^\\nu \\e^\\mu \\qquad (\\mu \\neq \\nu)\n\\] These relations define the algebraic structure that underlies spacetime geometry in the context of special relativity.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#bivectors-and-their-algebra",
    "href": "courses/special-relativity/spacetime-algebra.html#bivectors-and-their-algebra",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Bivectors and Their Algebra",
    "text": "Bivectors and Their Algebra\nBivectors are grade-2 elements constructed as wedge products of vectors. They span a 6-dimensional subspace of \\(\\mathrm{Cl}(1,3)\\), with basis elements of the form \\(\\e^\\mu \\wedge \\e^\\nu\\) for \\(\\mu &lt; \\nu\\). By construction, they are antisymmetric: \\[\n\\e^\\mu \\wedge \\e^\\nu = -\\e^\\nu \\wedge \\e^\\mu\n\\]\nThe square of a bivector depends on the causal character of the plane it spans. Bivectors involving the timelike basis vector \\(\\e^0\\) (e.g., \\(\\e^0 \\wedge \\e^1\\)) are called timelike and satisfy: \\[\n(\\e^0 \\wedge \\e^1)^2 = 1\n\\] In contrast, bivectors composed of spatial vectors only (e.g., \\(\\e^1 \\wedge \\e^2\\)) are spacelike and satisfy: \\[\n(\\e^1 \\wedge \\e^2)^2 = -1\n\\]\nWhen manipulating bivectors, it is useful to note that for orthogonal vectors \\(\\e^\\mu\\) and \\(\\e^\\nu\\), the wedge and Clifford products coincide: \\[\n\\e^\\mu \\wedge \\e^\\nu = \\e^\\mu \\e^\\nu \\quad (\\mu \\neq \\nu).\n\\] For example, \\[\n(\\e^0 \\wedge \\e^1)^2 = (\\e^0 \\e^1)^2 = \\e^0 \\e^1 \\e^0 \\e^1.\n\\] Using anticommutation, \\(\\e^1 \\e^0 = -\\e^0 \\e^1\\), we have: \\[\n\\e^0 \\e^1 \\e^0 \\e^1 = -\\e^0 \\e^0 \\e^1 \\e^1 = - (\\e^0)^2 (\\e^1)^2.\n\\] Since \\((\\e^0)^2 = -1\\) and \\((\\e^1)^2 = 1\\), it follows that: \\[\n(\\e^0 \\wedge \\e^1)^2 = 1.\n\\] This illustrates the general procedure for computing with multivectors: expand the product, apply anticommutation relations, and use the metric to evaluate contractions. This approach extends naturally to more complex expressions involving elements of arbitrary grade.\nThis distinction reflects the signature of the Minkowski metric and divides the six basis bivectors into two classes: three with positive square (timelike) and three with negative square (spacelike). Bivectors play a central role in representing infinitesimal Lorentz transformations, with spacelike bivectors generating spatial rotations and timelike bivectors generating boosts.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#trivectors",
    "href": "courses/special-relativity/spacetime-algebra.html#trivectors",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Trivectors",
    "text": "Trivectors\nTrivectors are grade-3 elements formed by wedge products of three distinct vectors. They span a 4-dimensional subspace of \\(\\mathrm{Cl}(1,3)\\) and are fully antisymmetric. A basis consists of one purely spatial trivector, \\[\n\\e^1 \\wedge \\e^2 \\wedge \\e^3,\n\\] and three mixed (temporal) trivectors, \\[\n\\e^0 \\wedge \\e^1 \\wedge \\e^2, \\qquad \\e^0 \\wedge \\e^2 \\wedge \\e^3, \\qquad \\e^0 \\wedge \\e^1 \\wedge \\e^3.\n\\]\nTheir squared norms reflect the causal nature of the planes they span. The purely spatial trivector satisfies: \\[\n(\\e^1 \\wedge \\e^2 \\wedge \\e^3)^2 = -1,\n\\] while temporal trivectors satisfy: \\[\n(\\e^0 \\wedge \\e^1 \\wedge \\e^2)^2 = 1.\n\\]\nTrivectors arise naturally in Clifford products involving three vectors, such as \\(\\e^0 \\e^1 \\e^2\\), and require careful attention to sign conventions due to their antisymmetry, e.g., \\[\n\\e^0 \\e^1 \\e^2 = -\\e^1 \\e^0 \\e^2.\n\\]\nAlthough trivectors have fewer direct physical interpretations than bivectors, they are useful in defining duals and in describing oriented volumes in spacetime.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#pseudoscalar",
    "href": "courses/special-relativity/spacetime-algebra.html#pseudoscalar",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Pseudoscalar",
    "text": "Pseudoscalar\nThe pseudoscalar is the unique grade-4 element of \\(\\mathrm{Cl}(1,3)\\): \\[\nI = \\e^0 \\wedge \\e^1 \\wedge \\e^2 \\wedge \\e^3 = \\e^0 \\e^1 \\e^2 \\e^3, \\qquad I^2 = 1.\n\\] It represents the oriented 4-volume element in spacetime—not to be confused with the usual 3-dimensional spatial volume, and plays a central role in the geometric structure of the algebra.\nThe pseudoscalar anticommutes with all 1-vectors: \\[\nI \\e^\\mu = -\\e^\\mu I,\n\\] and acts as the generator of Hodge duality via multiplication. For example: - \\(I \\e^\\mu\\) is a trivector, - \\(I (\\e^\\mu \\wedge \\e^\\nu)\\) yields a bivector, - \\(I \\cdot 1 = I\\) maps scalars to pseudoscalars.\nAs the oriented volume element, \\(I\\) performs the same operation as the Hodge star map, which is traditionally defined on components via contraction with the Levi-Civita symbol. For instance, \\[\n\\e^1 I = \\e^1 (\\e^0 \\e^1 \\e^2 \\e^3) = -\\e^0 \\e^2 \\e^3\n\\] maps the vector \\(\\e^1\\) to its dual trivector. This mirrors the way a cross product maps two vectors to a third in 3D, or more generally, how a \\(k\\)-vector is mapped to an \\((n{-}k)\\)-vector in \\(n\\)-dimensional Hodge duality.\nIn \\(\\mathrm{Cl}(1,3)\\), \\(I\\) implements the following dualities: - Scalars \\(\\leftrightarrow\\) pseudoscalars, - Vectors \\(\\leftrightarrow\\) trivectors, - Bivectors \\(\\to\\) bivectors (interchanging timelike and spacelike types).\nThis operation will be shown explicitly in later sections and is especially useful in formulating Lorentz-covariant quantities such as the electromagnetic field.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#additional-notes",
    "href": "courses/special-relativity/spacetime-algebra.html#additional-notes",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Additional Notes",
    "text": "Additional Notes\nThe Clifford algebra \\(\\mathrm{Cl}(1,3)\\) consists of 16 basis elements: 1 scalar, 4 vectors, 6 bivectors, 4 trivectors, and 1 pseudoscalar. This structure is isomorphic to the \\(4 \\times 4\\) matrix algebra used in Dirac’s formulation of quantum mechanics, providing a natural framework for relativistic physics. Scalars are Lorentz-invariant, vectors transform as four-vectors under Lorentz transformations, and bivectors correspond to antisymmetric rank-2 tensors, such as the electromagnetic field tensor \\(F^{\\mu\\nu}\\). The pseudoscalar \\(I = \\e^0 \\wedge \\e^1 \\wedge \\e^2 \\wedge \\e^3\\) represents the oriented 4-volume element of Minkowski spacetime and remains invariant under proper Lorentz transformations (determinant 1).\nFor computations in \\(\\mathrm{Cl}(1,3)\\), the following algebraic rules are essential:\n\nThe squares of basis vectors reflect the Minkowski metric: \\((\\e^0)^2 = -1\\), \\((\\e^i)^2 = 1\\) for spatial indices \\(i = 1, 2, 3\\).\nBasis vectors anticommute for distinct indices: \\(\\e^\\mu \\e^\\nu = -\\e^\\nu \\e^\\mu\\) when \\(\\mu \\neq \\nu\\).\nPermutations of generators in multivector products introduce sign changes due to antisymmetry, e.g., \\(\\e^0 \\e^1 \\e^2 \\e^3 = -\\e^1 \\e^0 \\e^2 \\e^3\\).\n\nThese properties enable the algebraic derivation of key results in special relativity, such as Lorentz transformations, invariant intervals, and spacetime rotations, by manipulating multivectors within the \\(\\mathrm{Cl}(1,3)\\) framework. This approach unifies geometric and algebraic operations, simplifying calculations in relativistic physics.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#duality-relations",
    "href": "courses/special-relativity/spacetime-algebra.html#duality-relations",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Duality Relations",
    "text": "Duality Relations\nThe pseudoscalar \\[\nI = \\e^0 \\wedge \\e^1 \\wedge \\e^2 \\wedge \\e^3 = \\e^0 \\e^1 \\e^2 \\e^3\n\\] implements the Hodge duality operation in \\(\\mathrm{Cl}(1,3)\\), mapping \\(k\\)-vectors to \\((4-k)\\)-vectors. In particular, multiplication by \\(I\\) maps bivectors to their dual bivectors. For the basis bivectors, the duality relations are: \\[\n\\begin{aligned}\nI (\\e^0 \\e^1) &= \\e^2 \\e^3, \\\\\nI (\\e^0 \\e^2) &= -\\e^1 \\e^3, \\\\\nI (\\e^0 \\e^3) &= \\e^1 \\e^2, \\\\\nI (\\e^1 \\e^2) &= -\\e^0 \\e^3, \\\\\nI (\\e^1 \\e^3) &= \\e^0 \\e^2, \\\\\nI (\\e^2 \\e^3) &= -\\e^0 \\e^1.\n\\end{aligned}\n\\] These follow from anticommutation relations and the property \\(I \\e^\\mu = -\\e^\\mu I\\). For example, \\[\nI (\\e^0 \\e^1) = (\\e^0 \\e^1 \\e^2 \\e^3)(\\e^0 \\e^1) = \\e^2 \\e^3,\n\\] where anticommutation and metric contractions are used.\n\nDuality in 3D\nWithin the 3D spatial subalgebra of \\(\\mathrm{Cl}(1,3)\\) generated by \\[\n\\sigma^i \\equiv \\e^0 \\e^i, \\quad i=1,2,3,\n\\] the spatial pseudoscalar \\[\nI_3 = \\sigma^1 \\sigma^2 \\sigma^3 = I\n\\] defines a 3D duality operation analogous to the cross product. This subalgebra uses timelike-spatial bivectors \\(\\sigma^i\\), which satisfy \\[\n\\sigma^i \\sigma^j = -\\sigma^j \\sigma^i, \\quad (\\sigma^i)^2 = 1,\n\\] and their symmetric product reproduces the standard Euclidean inner product: \\[\n\\sigma^i \\cdot \\sigma^j \\equiv \\frac{\\sigma^i \\sigma^j + \\sigma^j \\sigma^i}{2} = \\delta^{ij}.\n\\]\nThe 3D cross product emerges naturally as \\[\nu \\times v \\equiv -I (u \\wedge v),\n\\] for vectors \\(u,v \\in \\mathrm{span}\\{\\sigma^1, \\sigma^2, \\sigma^3\\}\\). For example, \\[\n\\begin{aligned}\n\\sigma^1 \\times \\sigma^2 &= -I (\\sigma^1 \\wedge \\sigma^2) = \\sigma^3, \\\\\n\\sigma^1 \\times \\sigma^3 &= -I (\\sigma^1 \\wedge \\sigma^3) = -\\sigma^2, \\\\\n\\sigma^2 \\times \\sigma^3 &= -I (\\sigma^2 \\wedge \\sigma^3) = \\sigma^1.\n\\end{aligned}\n\\] Here, the wedge product \\(\\sigma^i \\wedge \\sigma^j\\) is a bivector, and multiplication by \\(I\\) maps it back to a vector, revealing the bivector nature of the cross product.\nThis framework provides a natural algebraic interpretation of the magnetic field in electromagnetism as a bivector transforming under spatial rotations consistent with the duality \\(I_3\\), unifying the cross product with spacetime geometry.\n\n\nContraction with the Pseudoscalar\nContraction with the pseudoscalar \\(I\\) in \\(\\mathrm{Cl}(1,3)\\) implements the Hodge dual operation algebraically. For a \\(k\\)-vector \\(A\\), the product \\(A I\\) produces a \\((4-k)\\)-vector, preserving the graded structure of the algebra.\nSpecific examples include: - Scalars \\(c \\in \\mathbb{R}\\) map to pseudoscalars via \\(c I = c\\, \\e^0 \\e^1 \\e^2 \\e^3\\). - Vectors transform into trivectors, e.g., \\[\n  I \\e^\\mu = (\\e^0 \\e^1 \\e^2 \\e^3) \\e^\\mu, \\quad \\text{with} \\quad I \\e^1 = \\e^0 \\e^2 \\e^3.\n  \\] - Bivectors map to bivectors with timelike and spacelike components interchanged, as discussed previously. - Trivectors map to vectors: \\[\n  I (\\e^\\mu \\e^\\nu \\e^\\rho) = \\pm \\e^\\sigma,\n  \\] where \\(\\e^\\sigma\\) is the complementary basis vector completing the 4-vector basis.\nThis multiplication by \\(I\\) corresponds to the Hodge star operator on differential forms, but here it is realized entirely through algebraic multiplication, eliminating the need for component-wise contractions with the Levi-Civita tensor.\nFor example, the electromagnetic field tensor \\[\nF = \\frac{1}{2} F_{\\mu\\nu} \\e^\\mu \\e^\\nu\n\\] is mapped to its dual \\[\n\\star F = I F,\n\\] which exchanges electric and magnetic components. This duality plays a central role in expressing Maxwell’s equations covariantly in spacetime.\nThe algebraic simplicity of contraction with \\(I\\) exemplifies the unifying power of \\(\\mathrm{Cl}(1,3)\\) in linking geometric and physical structures.\n\n\nContraction with the Pseudoscalar\nFor a vector \\(v = v^\\mu \\e_\\mu\\), contraction with the pseudoscalar \\(I\\) produces its dual trivector: \\[\nI v = I (v^\\mu \\e_\\mu) = -\\frac{v^\\mu}{3!} \\epsilon_{\\mu\\nu\\rho\\sigma} \\, \\e^\\nu \\e^\\rho \\e^\\sigma,\n\\] where \\(\\epsilon_{0123} = 1\\) is the totally antisymmetric Levi-Civita symbol. This expresses the Hodge dual of \\(v\\) as a trivector in \\(\\mathrm{Cl}(1,3)\\).\n\n\nVector Notation and Tensor Relations\nGeometric objects in \\(\\mathrm{Cl}(1,3)\\) can be represented with tensor components as follows: - Trivector: \\[\n  T = \\frac{1}{3!} T^{\\mu\\nu\\alpha} \\e_\\mu \\wedge \\e_\\nu \\wedge \\e_\\alpha,\n  \\] - Bivector: \\[\n  B = \\frac{1}{2!} B^{\\mu\\nu} \\e_\\mu \\wedge \\e_\\nu,\n  \\] - Pseudoscalar: \\[\n  \\alpha I, \\quad \\alpha \\in \\mathbb{R}.\n  \\]\nThe geometric product of vectors \\(v = v^\\mu \\e_\\mu\\) and \\(w = w^\\nu \\e_\\nu\\) decomposes into scalar (inner) and bivector (exterior) parts: \\[\nv w = v^\\mu w^\\nu \\left(\\e_\\mu \\cdot \\e_\\nu + \\e_\\mu \\wedge \\e_\\nu\\right).\n\\]\nThe Faraday tensor \\(F\\), encoding the electromagnetic field, is naturally a bivector in \\(\\mathrm{Cl}(1,3)\\): \\[\nF = \\frac{1}{2} F_{\\mu\\nu} \\e^\\mu \\wedge \\e^\\nu.\n\\] Its components combine electric fields (associated with bivectors \\(\\e^0 \\wedge \\e^i\\)) and magnetic fields (associated with spatial bivectors \\(\\e^i \\wedge \\e^j\\)), totaling six independent components. Under Lorentz transformations, electric and magnetic parts mix, but \\(F\\) as a bivector remains invariant, unifying electric and magnetic fields within the algebraic framework of spacetime geometry.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#algebra-of-bivectors",
    "href": "courses/special-relativity/spacetime-algebra.html#algebra-of-bivectors",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Algebra of Bivectors",
    "text": "Algebra of Bivectors\nIf two bivectors \\(\\omega\\) and \\(u\\) satisfy \\(\\omega \\wedge u = 0\\), they commute under the Clifford product: \\[\n\\omega u = u \\omega.\n\\] Conversely, if they are orthogonal in the sense that their inner product vanishes, \\(\\omega \\cdot u = 0\\), they anticommute: \\[\n\\omega u = -u \\omega.\n\\] In this case, their product squares as \\[\n(\\omega u)^2 = -\\omega^2 u^2,\n\\] with the sign depending on the signatures of \\(\\omega^2\\) and \\(u^2\\): - If \\(\\omega^2 &lt; 0\\) and \\(u^2 &gt; 0\\), then \\(-\\omega^2 u^2 &gt; 0\\). - If both \\(\\omega^2 &lt; 0\\) and \\(u^2 &lt; 0\\), then \\(-\\omega^2 u^2 &lt; 0\\).\nThe exponential of a bivector generates Lorentz transformations: \\[\ne^{\\omega \\wedge u} = \\cosh\\left(\\sqrt{-\\omega^2 u^2}\\right) + \\frac{\\omega \\wedge u}{\\sqrt{-\\omega^2 u^2}} \\sinh\\left(\\sqrt{-\\omega^2 u^2}\\right).\n\\]\nBivectors generate boosts or rotations depending on their causal character. For instance, the timelike bivector \\(\\e^0 \\wedge \\e^1\\), with \\((\\e^0 \\wedge \\e^1)^2 = 1\\), generates boosts: \\[\n\\e^{\\lambda (\\e^0 \\wedge \\e^1)} = \\cosh \\lambda + (\\e^0 \\wedge \\e^1) \\sinh \\lambda.\n\\] Applying this boost to the vector \\(w = \\e^0\\) gives \\[\n\\e^{-\\lambda (\\e^0 \\wedge \\e^1)} \\e^0 \\e^{\\lambda (\\e^0 \\wedge \\e^1)} = (\\cosh \\lambda) \\e^0 + (\\sinh \\lambda) \\e^1,\n\\] corresponding to a Lorentz boost along \\(\\e^1\\) with \\(\\cosh \\lambda = \\gamma\\), \\(\\sinh \\lambda = \\gamma \\beta\\).\nIn contrast, a spacelike bivector such as \\(\\e^1 \\wedge \\e^2\\), with \\((\\e^1 \\wedge \\e^2)^2 = -1\\), generates spatial rotations: \\[\n\\e^{\\lambda (\\e^1 \\wedge \\e^2)} = \\cos \\lambda + (\\e^1 \\wedge \\e^2) \\sin \\lambda.\n\\]",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#electromagnetism-in-clifford-algebra",
    "href": "courses/special-relativity/spacetime-algebra.html#electromagnetism-in-clifford-algebra",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Electromagnetism in Clifford Algebra",
    "text": "Electromagnetism in Clifford Algebra\nThe electromagnetic field is naturally represented by the Faraday bivector \\[\nF = \\frac{1}{2} F_{\\mu\\nu} \\e^\\mu \\wedge \\e^\\nu,\n\\] which contains six components: three electric (associated with bivectors \\(\\e^0 \\wedge \\e^i\\)) and three magnetic (associated with \\(\\e^i \\wedge \\e^j\\)). Under Lorentz transformations, electric and magnetic parts mix, but the bivector \\(F\\) itself remains invariant. This formalism unifies electric and magnetic fields as frame-dependent projections of the electromagnetic field tensor, simplifying the formulation of electromagnetism and related calculations such as the Lorentz force. Further applications will be developed in the course.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#exercises",
    "href": "courses/special-relativity/spacetime-algebra.html#exercises",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Exercises",
    "text": "Exercises\nTo improve your understanding of \\(\\mathrm{Cl}(1,3)\\), work through these exercises:\n\nSquares of Basis Bivectors\nCompute the square of the following bivectors and classify each as timelike, spacelike, or lightlike:\n\n\\((\\e^0 \\wedge \\e^1)^2\\)\n\n\\((\\e^1 \\wedge \\e^2)^2\\)\n\n\\((\\e^2 \\wedge \\e^3)^2\\)\n\n\\((\\e^0 \\wedge \\e^2)^2\\)\n\nDuals with the Pseudoscalar\nCalculate the duals of bivectors by multiplying with the pseudoscalar \\(I\\):\n\n\\(I (\\e^0 \\e^1)\\)\n\n\\(I (\\e^1 \\e^2)\\)\n\n\\(I (\\e^0 \\e^2)\\)\n\n\\(I (\\e^2 \\e^3)\\)\nVerify that these results are consistent with the duality relations in \\(\\mathrm{Cl}(1,3)\\).\n\nSquares of Trivectors\nCompute the squares of these trivectors and determine their causal character:\n\n\\((\\e^0 \\wedge \\e^1 \\wedge \\e^2)^2\\)\n\n\\((\\e^1 \\wedge \\e^2 \\wedge \\e^3)^2\\)\n\nPseudoscalar Commutation\nVerify the anticommutation relation with the pseudoscalar for each basis vector:\n\\[\nI \\e^\\mu = - \\e^\\mu I, \\quad \\mu=0,1,2,3.\n\\]\nIdentify the grade and type of the multivector \\(I \\e^\\mu\\).\nExponential of a Bivector\nFor \\(\\omega = \\e^0\\), \\(u = \\e^1\\):\n\nCompute the wedge product \\(\\omega \\wedge u\\).\n\nCalculate \\((\\omega \\wedge u)^2\\).\n\nWrite down the exponential \\(\\e^{\\omega \\wedge u}\\) and interpret whether it corresponds to a boost or a rotation.\n\nAnticommutators and Commutators\nCalculate and interpret:\n\nThe anticommutator \\(\\e^0 \\e^1 + \\e^1 \\e^0\\).\n\nThe commutator \\(\\e^1 \\e^2 - \\e^2 \\e^1\\).\nRelate these to the inner (symmetric) and exterior (antisymmetric) products of vectors.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#inversion-and-hermitian-conjugates",
    "href": "courses/special-relativity/spacetime-algebra.html#inversion-and-hermitian-conjugates",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Inversion and Hermitian Conjugates",
    "text": "Inversion and Hermitian Conjugates\nIn \\(\\mathrm{Cl}(1,3)\\), the reversion (also called transpose) is an involutive operation defined on multivectors by: \\[\n(ab)^\\mathrm{T} = b^\\mathrm{T} a^\\mathrm{T},\n\\] for any multivectors \\(a\\) and \\(b\\). Reversion reverses the order of the product but leaves individual vectors and scalars unchanged: \\[\n(\\e^\\mu)^\\mathrm{T} = \\e^\\mu, \\quad (\\alpha)^\\mathrm{T} = \\alpha, \\quad \\text{for any } \\alpha \\in \\mathbb{R}.\n\\]\nFor the pseudoscalar \\(I = \\e^0 \\e^1 \\e^2 \\e^3\\): \\[\nI^\\mathrm{T} = (\\e^0 \\e^1 \\e^2 \\e^3)^\\mathrm{T} = \\e^3 \\e^2 \\e^1 \\e^0 = I,\n\\] since reversing all four basis vectors introduces an even number of sign changes.\nFor a bivector \\(B = \\frac{1}{2} B_{\\mu\\nu} \\e^\\mu \\wedge \\e^\\nu\\), the reversion changes the sign: \\[\nB^\\mathrm{T} = \\frac{1}{2} B_{\\mu\\nu} (\\e^\\nu \\wedge \\e^\\mu) = -\\frac{1}{2} B_{\\mu\\nu} \\e^\\mu \\wedge \\e^\\nu = -B,\n\\] reflecting the antisymmetry of bivectors under reversion.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#exponentials-of-bivectors",
    "href": "courses/special-relativity/spacetime-algebra.html#exponentials-of-bivectors",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Exponentials of Bivectors",
    "text": "Exponentials of Bivectors\nThe exponential of a bivector \\(B\\) is defined through the usual power series: \\[\ne^{\\alpha B} = \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} B^n,\n\\] with \\(\\alpha \\in \\mathbb{R}\\). The closed form depends on the square of \\(B\\), and can be derived by separating the even and odd terms:\n\nFor \\(B^2 = +1\\), (e.g., \\(B = \\e^0 \\wedge \\e^1\\)) the exponential is given by: \\[\n\\begin{aligned}\ne^{\\alpha B} &= \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} B^n \\\\\n             &= \\sum_{n=0}^{\\infty} \\left( \\frac{\\alpha^{2n}}{(2n)!} B^{2n} + \\frac{\\alpha^{2n+1}}{(2n+1)!} B^{2n+1} \\right) \\\\\n             &= \\sum_{n=0}^{\\infty} \\frac{\\alpha^{2n}}{(2n)!} + B \\sum_{n=0}^{\\infty} \\frac{\\alpha^{2n+1}}{(2n+1)!} \\\\\n             &= \\cosh(\\alpha) + B \\sinh(\\alpha).\n\\end{aligned}\n\\]\nFor \\(B^2 = -1\\), (e.g., \\(B = \\e^2 \\wedge \\e^3\\)) the exponential is given by: \\[\n\\begin{aligned}\ne^{\\alpha B} &= \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} B^n \\\\\n             &= \\sum_{n=0}^{\\infty} \\left( \\frac{\\alpha^{2n}}{(2n)!} B^{2n} + \\frac{\\alpha^{2n+1}}{(2n+1)!} B^{2n+1} \\right) \\\\\n             &= \\sum_{n=0}^{\\infty} \\frac{(-1)^n \\alpha^{2n}}{(2n)!} + B \\sum_{n=0}^{\\infty} \\frac{(-1)^n \\alpha^{2n+1}}{(2n+1)!} \\\\\n             &= \\cos(\\alpha) + B \\sin(\\alpha).\n\\end{aligned}\n\\]\n\n\nLorentz Transformations with Rotors\nLorentz transformations are implemented using rotors: \\[\nR = e^{\\alpha B / 2},\n\\] where \\(B\\) is a bivector (e.g., \\(\\e^0 \\wedge \\e^1\\) for boosts or \\(\\e^1 \\wedge \\e^2\\) for rotations), and \\(\\alpha \\in \\mathbb{R}\\) is a transformation parameter. The rotor acts on a vector \\(u\\) via the sandwich product: \\[\nu \\mapsto R u R^\\mathrm{T}, \\quad \\text{with} \\quad R^\\mathrm{T} = e^{-\\alpha B / 2}.\n\\]\nThis ensures that the result is a vector, since: \\[\n(R u R^\\mathrm{T})^\\mathrm{T} = R u R^\\mathrm{T}.\n\\] In contrast, a simple left multiplication like \\(R u\\) is not a vector: \\[\n(R u)^\\mathrm{T} = u R^\\mathrm{T} \\neq R u.\n\\]\nThe use of both \\(R\\) and \\(R^\\mathrm{T}\\) in the transformation law necessitates the factor \\(\\alpha/2\\) in the exponent, ensuring the correct group composition law: \\[\nR(\\alpha_1) R(\\alpha_2) = R(\\alpha_1 + \\alpha_2).\n\\]\n\n\nSpinors and Lorentz Rotations\nRotors exhibit spinorial behavior. For example, a \\(2\\pi\\) rotation gives: \\[\nR(2\\pi) = \\cos(\\pi) + B \\sin(\\pi) = -1 = -R(0),\n\\] indicating that a full \\(4\\pi\\) rotation is needed to return to the identity: \\[\nR(4\\pi) = 1.\n\\]\nThis is the hallmark of spin-1/2 behavior. Spinors arise naturally in the even subalgebra of \\(\\mathrm{Cl}(1,3)\\), forming the group \\(\\mathrm{Spin}(1,3)\\), which is a double cover of the Lorentz group \\(\\mathrm{SO}(1,3).\\) The same rotor \\(R\\) that acts on vectors as: \\[\nu \\mapsto R u R^\\mathrm{T},\n\\] acts on spinors \\(\\psi\\) (e.g., four-component complex vectors) as: \\[\n\\psi \\mapsto R \\psi.\n\\] Note, however, that this approach would require introducing a matrix representation of the algebra. While this is a common route, it is not strictly necessary. Instead, we can represent the “vector” \\(\\psi\\) directly as an element of \\(\\mathrm{Cl}(1,3)\\), more precisely, as an even multivector.\nThe pseudoscalar \\(I = \\e_0 \\e_1 \\e_2 \\e_3\\) anticommutes with all vectors and satisfies \\(I^2 = -1\\), playing a role analogous to the complex unit \\(i\\) in the Dirac algebra. In the spinor representation, it appears in constructions involving \\(\\gamma^5\\) and chirality.",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#reference-frames-and-4-velocity",
    "href": "courses/special-relativity/spacetime-algebra.html#reference-frames-and-4-velocity",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Reference Frames and 4-Velocity",
    "text": "Reference Frames and 4-Velocity\nAn observer’s 4-velocity is \\(w^\\mu = (1, 0, 0, 0)\\), or \\(w = \\e_0\\), with \\(w^2 = -1\\). A particle’s 4-velocity is: \\[\nu^\\mu = \\left( \\gamma, \\gamma \\frac{v^1}{c}, \\gamma \\frac{v^2}{c}, \\gamma \\frac{v^3}{c} \\right), \\quad \\gamma = (1 - v^2/c^2)^{-1/2},\n\\] so \\(u = u^\\mu \\e_\\mu\\), with: \\[\nu^2 = -\\gamma^2 (1 - v^2/c^2) = -1, \\quad u \\cdot w = -\\gamma.\n\\]\n\nTrajectories and Decomposition\nA spacetime trajectory is: \\[\nx = x^\\mu \\e_\\mu = x^0 \\e_0 + x^i \\e_i, \\quad x \\cdot w = -x^0.\n\\] The spatial projection orthogonal to \\(w = \\e_0\\): \\[\nx_\\perp = \\frac{x + w x w}{2} = x^i \\e_i,\n\\] since: \\[\nw x w = -x^0 \\e_0 + x^i \\e_i, \\quad x + w x w = 2 x^i \\e_i.\n\\]",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/special-relativity/spacetime-algebra.html#exercises-1",
    "href": "courses/special-relativity/spacetime-algebra.html#exercises-1",
    "title": "Clifford Algebra and Spacetime Geometry",
    "section": "Exercises",
    "text": "Exercises\n\nBasic Comprehension\n\nShow that for \\(u = u^\\mu \\e_\\mu\\), we have \\(u^2 = -1\\) and \\(u \\cdot w = -\\gamma\\) for \\(w = \\e_0\\).\nCompute the spatial projection \\(x_\\perp = \\frac{x + w x w}{2}\\) for \\(x = x^0 \\e_0 + x^1 \\e_1\\), and check that \\(x_\\perp \\cdot w = 0\\).\nLet \\(B = \\e^2 \\wedge \\e^3\\). Compute \\(B^2\\) and verify the closed form of \\(e^{\\theta B}\\).\nProve that for any bivector \\(B\\) and scalar \\(\\alpha\\), we have: \\[\n\\left(e^{\\alpha B}\\right)^\\mathrm{T} = e^{-\\alpha B}.\n\\]\nLet \\(R = e^{\\theta B/2}\\) for a bivector \\(B\\) with \\(B^2 = -1\\). Show that \\(R^\\mathrm{T} = R^{-1}\\).\n\n\n\nApplications\n\nShow that the action \\(u \\mapsto R u R^\\mathrm{T}\\) preserves the norm: \\((R u R^\\mathrm{T})^2 = u^2\\).\nLet \\(B = \\e^0 \\wedge \\e^1\\) and \\(R = e^{\\eta B / 2}\\) with rapidity \\(\\eta\\). Compute \\(R \\e^0 R^\\mathrm{T}\\) and interpret the result as a Lorentz boost along the \\(x^1\\) direction.\nVerify explicitly that a full \\(2\\pi\\) rotation gives \\(R = -1\\) for \\(B = \\e^1 \\wedge \\e^2\\).\nLet \\(x = x^0 \\e_0 + x^1 \\e_1\\) and \\(w = \\e_0\\). Show that the time component of \\(x\\) is recovered by: \\[\nt = -x \\cdot w.\n\\]\nLet \\(B = \\e^1 \\wedge \\e^2\\). Compute \\(R = \\cos(\\theta/2) + B \\sin(\\theta/2)\\) and then compute \\(R^2\\). What is the result? Why does this make sense?\nShow that the pseudoscalar \\(I = \\e_0 \\e_1 \\e_2 \\e_3\\) anticommutes with all vectors: \\(I \\e^\\mu = -\\e^\\mu I\\), and deduce that it also anticommutes with all bivectors.\nLet \\(x = x^\\mu \\e_\\mu\\) and \\(w = \\e_0\\). Show that \\(x = -t w + x_\\perp\\), where \\(t = -x \\cdot w\\) and \\(x_\\perp = \\frac{x + w x w}{2}\\).",
    "crumbs": [
      "Courses",
      "Special Relativity",
      "Spacetime Algebra"
    ]
  },
  {
    "objectID": "courses/vector-calculus/index.html",
    "href": "courses/vector-calculus/index.html",
    "title": "Vector Calculus Overview",
    "section": "",
    "text": "This module introduces the mathematical framework of vector calculus, essential for understanding physics in three or more dimensions. It covers fundamental concepts, operations, and applications frequently used across physics courses, especially electromagnetism and mechanics.\n\n\n\nVector Spaces: Definitions, linear independence, and the structure of spaces where vectors live.\nInner Product: Properties of the dot product, including the Kronecker delta notation.\nVector Product: Cross product, exterior product, and their properties.\nTriple Products: Relations involving scalar and vector triple products.\nEuclidean Space: Cartesian coordinates and tangent vectors.\nDifferential Calculus: Gradient, divergence, curl, and higher-order derivatives.\nIntegral Calculus: Line, surface, and volume integrals.\nCurvilinear Coordinates: Basis vectors, differential operators, and orthogonal coordinate systems.\nDirac Delta: Introduction and properties of the delta distribution.\nVector Field Decomposition: Techniques to break down vector fields into simpler components.\n\nThis course aims to build a solid foundation for applying these mathematical tools throughout your physics studies.\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Overview"
    ]
  },
  {
    "objectID": "courses/vector-calculus/index.html#contents",
    "href": "courses/vector-calculus/index.html#contents",
    "title": "Vector Calculus Overview",
    "section": "",
    "text": "Vector Spaces: Definitions, linear independence, and the structure of spaces where vectors live.\nInner Product: Properties of the dot product, including the Kronecker delta notation.\nVector Product: Cross product, exterior product, and their properties.\nTriple Products: Relations involving scalar and vector triple products.\nEuclidean Space: Cartesian coordinates and tangent vectors.\nDifferential Calculus: Gradient, divergence, curl, and higher-order derivatives.\nIntegral Calculus: Line, surface, and volume integrals.\nCurvilinear Coordinates: Basis vectors, differential operators, and orthogonal coordinate systems.\nDirac Delta: Introduction and properties of the delta distribution.\nVector Field Decomposition: Techniques to break down vector fields into simpler components.\n\nThis course aims to build a solid foundation for applying these mathematical tools throughout your physics studies.\n\nFeel free to create issues, ask questions, or suggest improvements in the GitHub repository.",
    "crumbs": [
      "Courses",
      "Vector Calculus",
      "Overview"
    ]
  }
]